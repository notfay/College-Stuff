A cognitive bias is a systematic pattern of deviation from norm or rationality in judgment. Individuals create their own "subjective reality" from their perception of the input. An individual's construction of reality, not the objective input, may dictate their behavior in the world. Thus, cognitive biases may sometimes lead to perceptual distortion, inaccurate judgment, illogical interpretation, and irrationality.
While cognitive biases may initially appear to be negative, some are adaptive. They may lead to more effective actions in a given context. Furthermore, allowing cognitive biases enables faster decisions which can be desirable when timeliness is more valuable than accuracy, as illustrated in heuristics. Other cognitive biases are a "by-product" of human processing limitations, resulting from a lack of appropriate mental mechanisms (bounded rationality), the impact of an individual's constitution and biological state (see embodied cognition), or simply from a limited capacity for information processing. Cognitive biases can make individuals more inclined to endorsing pseudoscientific beliefs by requiring less evidence for claims that confirm their preconceptions. This can potentially distort their perceptions and lead to inaccurate judgments.
A continually evolving list of cognitive biases has been identified over the last six decades of research on human judgment and decision-making in cognitive science, social psychology, and behavioral economics. The study of cognitive biases has practical implications for areas including clinical judgment, entrepreneurship, finance, and management.

When making judgments under uncertainty, people rely on mental shortcuts or heuristics, which provide swift estimates about the possibility of uncertain occurrences. For example, 
the representativeness heuristic is defined as the tendency to judge the frequency or likelihood of an occurrence by the extent of which the event resembles the typical case. Similarly the availability heuristic is that individuals estimate the likelihood of events by how easy they are to recall, and the anchoring heuristic prefers the initial reference points that are recalled. While these heuristics are efficient and simple for the brain to compute, they sometimes introduce predictable and systematic cognitive errors, or biases.
The "Linda Problem" illustrates the representativeness heuristic and corresponding bias. Participants were given a description of "Linda" that suggests Linda might well be a feminist (e.g., she is said to be concerned about discrimination and social justice issues). They were then asked whether they thought Linda was more likely to be (a) a "bank teller" or (b) a "bank teller and active in the feminist movement." A majority chose answer (b). Independent of the information given about Linda, though, the more restrictive answer (b) is under any circumstance statistically less likely than answer (a). This is an example of the conjunction fallacy: respondents chose (b) because it seemed more "representative" or typical of persons who might fit the description of Linda. The representativeness heuristic may lead to errors such as activating stereotypes and inaccurate judgments of others.
Gerd Gigerenzer argues that heuristics should not lead us to conceive of human thinking as riddled with irrational cognitive biases. They should rather conceive rationality as an adaptive tool, not identical to the rules of formal logic or the probability calculus.  Gigerenzer believes that cognitive biases are not biases, but rules of thumb, or as he would put it "gut feelings" that can actually help us make accurate decisions in our lives. There is not clear evidence that these behaviors are genuinely, severely biased once the actual problems people face are understood. Advances in economics and cognitive neuroscience now suggest that many behaviors previously labeled as biases might instead represent optimal decision-making strategies.

The notion of cognitive biases was introduced by Amos Tversky and Daniel Kahneman in 1972 and grew out of their experience of people's innumeracy, or inability to reason intuitively with the greater orders of magnitude. Tversky, Kahneman, and colleagues demonstrated several replicable ways in which human judgments and decisions differ from rational choice theory. Their 1974 paper, Judgment under Uncertainty: Heuristics and Biases, outlined how people rely on mental shortcuts when making judgments under uncertainty. Experiments such as the "Linda problem" grew into heuristics and biases research programs, which spread beyond academic psychology into other disciplines including medicine and political science.
The list of cognitive biases has long been a topic of critique. In psychology a "rationality war" unfolded between Gerd Gigerenzer and the Kahneman and Tversky school, which pivoted on whether biases are primarily defects of human cognition or the result of behavioural patterns that are actually adaptive or "ecologically rational". Gerd Gigerenzer has historically been one of the main opponents to cognitive biases and heuristics. This debate has recently reignited, with critiques arguing there has been an overemphasis on biases in human cognition.

Biases can be distinguished on a number of dimensions. Examples of cognitive biases include -
Biases specific to groups (such as the risky shift) versus biases at the individual level.
Biases that affect decision-making, where the desirability of options has to be considered (e.g., sunk costs fallacy).
Biases, such as illusory correlation, that affect judgment of how likely something is or whether one thing is the cause of another.
Biases that affect memory, such as consistency bias (remembering one's past attitudes and behavior as more similar to one's present attitudes).
Biases that reflect a subject's motivation, for example, the desire for a positive self-image leading to egocentric bias and the avoidance of unpleasant cognitive dissonance.
Other biases are due to the particular way the brain perceives, forms memories and makes judgments. This distinction is sometimes described as "hot cognition" versus "cold cognition", as motivated reasoning can involve a state of arousal. Among the "cold" biases,
some are due to ignoring relevant information (e.g., neglect of probability),
some involve a decision or judgment being affected by irrelevant information (for example the framing effect where the same problem receives different responses depending on how it is described; or the distinction bias where choices presented together have different outcomes than those presented separately), and
others give excessive weight to an unimportant but salient feature of the problem (e.g., anchoring).
As some biases reflect motivation specifically the motivation to have positive attitudes to oneself. It accounts for the fact that many biases are self-motivated or self-directed (e.g., illusion of asymmetric insight, self-serving bias). There are also biases in how subjects evaluate in-groups or out-groups; evaluating in-groups as more diverse and "better" in many respects, even when those groups are arbitrarily defined (ingroup bias, outgroup homogeneity bias).
Some cognitive biases belong to the subgroup of attentional biases, which refers to paying increased attention to certain stimuli. It has been shown, for example, that people addicted to alcohol and other drugs pay more attention to drug-related stimuli. Common psychological tests to measure those biases are the Stroop task and the dot probe task.
Individuals' susceptibility to some types of cognitive biases can be measured by the Cognitive Reflection Test (CRT) developed by Shane Frederick (2005).

The following is a list of the more commonly studied cognitive biases:

Many social institutions rely on individuals to make rational judgments. Across management, finance, medicine, and law, the most recurrent bias is overconfidence, though anchoring and framing also play substantial roles. While research in finance often uses large-scale data, studies in medicine and law frequently rely on vignette-based designs. Berthet highlights the lack of ecological validity in many studies and the need for deeper exploration of individual differences in susceptibility to bias. The securities regulation regime largely assumes that all investors act as perfectly rational persons. In truth, actual investors face cognitive limitations from biases, heuristics, and framing effects. In some academic disciplines, the study of bias is very popular. For instance, bias is a wide spread and well studied phenomenon because most decisions that concern the minds and hearts of entrepreneurs are computationally intractable.
In law enforcement and legal decision-making, confirmation bias and related errors frequently influence investigative decisions and evidence evaluation. Structured intervention strategies, such as accountability measures and checklists, show some promise in reducing bias during case evaluations. A fair jury trial, for example, requires that the jury ignore irrelevant features of the case, weigh the relevant features appropriately, consider different possibilities open-mindedly and resist fallacies such as appeal to emotion. The various biases demonstrated in these psychological experiments suggest that people will frequently fail to do all these things. However, they fail to do so in systematic, directional ways that are predictable.
Cognitive biases can create other issues that arise in everyday life. Study participants who ate more unhealthy snack food tended to have less inhibitory control and more reliance on approach bias. Cognitive biases could be linked to various eating disorders and how people view their bodies and their body image.
Cognitive biases can be used in destructive ways. Some believe that there are people in authority who use cognitive biases and heuristics in order to manipulate others so that they can reach their end goals. Some medications and other health care treatments rely on cognitive biases in order to persuade others who are susceptible to cognitive biases to use their products. Many see this as taking advantage of one's natural struggle of judgement and decision-making. They also believe that it is the government's responsibility to regulate these misleading ads.
Cognitive biases also seem to play a role in property sale price and value. Participants in the experiment were shown a residential property. Afterwards, they were shown another property that was completely unrelated to the first property. They were asked to say what they believed the value and the sale price of the second property would be. They found that showing the participants an unrelated property did have an effect on how they valued the second property.
Cognitive biases can be used in non-destructive ways. In team science and collective problem-solving, the superiority bias can be beneficial. It leads to a diversity of solutions within a group, especially in complex problems, by preventing premature consensus on suboptimal solutions. This example demonstrates how a cognitive bias, typically seen as a hindrance, can enhance collective decision-making by encouraging a wider exploration of possibilities.
Cognitive biases are interlinked with collective illusions, a phenomenon where a group of people mistakenly believe that their views and preferences are shared by the majority, when in reality, they are not. These illusions often arise from various cognitive biases that misrepresent our perception of social norms and influence how we assess the beliefs of others.
Cognitive biases also influence the spread of misinformation, particularly in digital environments. Lazer, Baum, and Grinberg (2018) analyzed over 16,000 false news stories shared by millions of Twitter users during the 2016 U.S. election and found that false information spread significantly faster than accurate news. This occurs partly because misinformation aligns with existing beliefs and triggers emotional reactions, both of which are linked to confirmation and availability biases. These findings illustrate how cognitive biases can distort public understanding and contribute to the rapid dissemination of false narratives.

The content and direction of cognitive biases are not "arbitrary". Debiasing is the reduction of biases in judgment and decision-making through incentives, nudges, and training. Cognitive bias mitigation and cognitive bias modification are forms of debiasing specifically applicable to cognitive biases and their effects. One debiasing technique aims to decrease biases by encouraging individuals to use controlled processing compared to automatic processing. Because they cause systematic errors, cognitive biases cannot be compensated for using a wisdom of the crowd technique of averaging answers from several people. Reference class forecasting is a method for systematically debiasing estimates and decisions, based on what Daniel Kahneman has dubbed the outside view.
Cognitive bias modification (CBM) refers to the process of modifying cognitive biases in healthy people and also refers to a growing area of psychological (non-pharmaceutical) therapies for anxiety, depression and addiction called cognitive bias modification therapy (CBMT). CBMT is sub-group of therapies within a growing area of psychological therapies based on modifying cognitive processes with or without accompanying medication and talk therapy, sometimes referred to as applied cognitive processing therapies (ACPT). Although cognitive bias modification can refer to modifying cognitive processes in healthy individuals, CBMT is a growing area of evidence-based psychological therapy, in which cognitive processes are modified to relieve suffering from serious depression, anxiety, and addiction. CBMT techniques are technology-assisted therapies that are delivered via a computer with or without clinician support. CBM combines evidence and theory from the cognitive model of anxiety, cognitive neuroscience, and attentional models. Even one-shot training interventions, such as educational videos and debiasing games that taught mitigating strategies, significantly reduced the commission of several cognitive biases.
Cognitive bias modification has also been used to help those with obsessive-compulsive beliefs and obsessive-compulsive disorder. This therapy has shown that it decreases the obsessive-compulsive beliefs and behaviors.
In relation to reducing the fundamental attribution error, monetary incentives and informing participants they will be held accountable for their attributions have been linked to the increase of accurate attributions.

Bias arises from various processes that are sometimes difficult to distinguish. These include:
Bounded rationality — limits on optimization and rationality
Prospect theory
Evolutionary psychology  — Remnants from evolutionary adaptive mental functions.
Mental accounting
Adaptive bias — basing decisions on limited information and biasing them based on the costs of being wrong
Attribute substitution — making a complex, difficult judgment by unconsciously replacing it with an easier judgment
Attribution theory
Salience
Naïve realism
Cognitive dissonance, and related:
Impression management
Self-perception theory
Information-processing shortcuts (heuristics), including:
Availability heuristic — estimating what is more likely by what is more available in memory, which is biased toward vivid, unusual, or emotionally charged examples
Representativeness heuristic — judging probabilities based on resemblance
Affect heuristic — basing a decision on an emotional reaction rather than a calculation of risks and benefits
Emotional and moral motivations deriving, for example, from:
The two-factor theory of emotion
The somatic markers hypothesis
Introspection illusion
Misinterpretations or misuse of statistics; innumeracy.
Social influence
The brain's limited information processing capacity
Noisy information processing (distortions during storage in and retrieval from memory). For example, a 2012 Psychological Bulletin article suggests that at least eight seemingly unrelated biases can be produced by the same information-theoretic generative mechanism. The article shows that noisy deviations in the memory-based information processes that convert objective evidence (observations) into subjective estimates (decisions) can produce regressive conservatism, the belief revision (Bayesian conservatism), illusory correlations, illusory superiority (better-than-average effect) and worse-than-average effect, subadditivity effect, exaggerated expectation, overconfidence, and the hard–easy effect.

People do appear to have stable individual differences in their susceptibility to decision biases such as overconfidence, temporal discounting, and bias blind spot. That said, these stable levels of bias within individuals are possible to change. Participants in experiments who watched training videos and played debiasing games showed medium to large reductions both immediately and up to three months later in the extent to which they exhibited susceptibility to six cognitive biases: anchoring, bias blind spot, confirmation bias, fundamental attribution error, projection bias, and representativeness.
Individual differences in cognitive bias have also been linked to varying levels of cognitive abilities and functions. The Cognitive Reflection Test (CRT) has been used to help understand the connection between cognitive biases and cognitive ability. There have been inconclusive results when using the Cognitive Reflection Test to understand ability. However, there does seem to be a correlation; those who gain a higher score on the Cognitive Reflection Test, have higher cognitive ability and rational-thinking skills. This in turn helps predict the performance on cognitive bias and heuristic tests. Those with higher CRT scores tend to be able to answer more correctly on different heuristic and cognitive bias tests and tasks.
Age is another individual difference that has an effect on one's ability to be susceptible to cognitive bias. Older individuals tend to be more susceptible to cognitive biases and have less cognitive flexibility. However, older individuals were able to decrease their susceptibility to cognitive biases throughout ongoing trials. These experiments had both young and older adults complete a framing task. Younger adults had more cognitive flexibility than older adults. Cognitive flexibility is linked to helping overcome pre-existing biases.

In psychology and cognitive science, cognitive biases are systematic patterns of deviation from norm and/or rationality in judgment.  They are often studied in psychology, sociology and behavioral economics. A memory bias is a cognitive bias that either enhances or impairs the recall of a memory (either the chances that the memory will be recalled at all, or the amount of time it takes for it to be recalled, or both), or that alters the content of a reported memory. 
Explanations include information-processing rules (i.e., mental shortcuts), called heuristics, that the brain uses to produce decisions or judgments. Biases have a variety of forms and appear as cognitive ("cold") bias, such as mental noise, or motivational ("hot") bias, such as when beliefs are distorted by wishful thinking. Both effects can be present at the same time.
There are also controversies over some of these biases as to whether they count as useless or irrational, or whether they result in useful attitudes or behavior. For example, when getting to know others, people tend to ask leading questions which seem biased towards confirming their assumptions about the person. However, this kind of confirmation bias has also been argued to be an example of social skill; a way to establish a connection with the other person.
Although this research overwhelmingly involves human subjects, some studies have found bias in non-human animals as well. For example, loss aversion has been shown in monkeys and hyperbolic discounting has been observed in rats, pigeons, and monkeys.

Although the reality of these biases is confirmed by reproducible research, there are often controversies about how to classify these biases or how to explain them. Several theoretical causes are known for some cognitive biases, which provides a classification of biases by their common generative mechanism (such as noisy information-processing). Gerd Gigerenzer has criticized the framing of cognitive biases as errors in judgment, and favors interpreting them as arising from rational deviations from logical thought. This list is organized based on the task-based classification proposed by Dimara et al. (2020). This classification defines 6 tasks, namely estimation, decision, hypothesis assessment, causal attribution, recall, and opinion reporting. The biases are further loosely classified into 5 sub-categories or "flavors":
Association: a connection between different pieces of information
Baseline: comparing something to a perceived standard or starting point
Inertia: the reluctance to change something that is already in place
Outcome: how well something aligns with an expected or hoped-for result
Self-perspective: influenced by one's own personal point of view

In estimation or judgement tasks, people are asked to assess the value of a quantity.
Association:
Aesthetic–usability effect: A tendency for people to perceive attractive things as more usable.
Attribute substitution: Occurs when a judgment has to be made (of a target attribute) that is computationally complex, and instead a more easily calculated heuristic attribute is substituted. This substitution is thought of as taking place in the automatic intuitive judgment system, rather than the more self-aware reflective system.
The availability heuristic (also known as the availability bias) is the tendency to overestimate the likelihood of events with greater "availability" in memory, which can be influenced by how recent the memories are or how unusual or emotionally charged they may be. There is a greater likelihood of recalling recent, nearby, or otherwise immediately available examples, and the imputation of importance to those examples over others.
Conjunction fallacy, the tendency to assume that specific conditions are more probable than a more general version of those same conditions.
Hot-cold empathy gap, the tendency to underestimate the influence of visceral drives on one's attitudes, preferences, and behaviors.
Tachypsychia: When time perceived by the individual either lengthens, making events appear to slow down, or contracts.
Time-saving bias, a tendency to underestimate the time that could be saved (or lost) when increasing (or decreasing) from a relatively low speed, and to overestimate the time that could be saved (or lost) when increasing (or decreasing) from a relatively high speed.
Travis syndrome: Overestimating the significance of the present. It is related to chronological snobbery with possibly an appeal to novelty logical fallacy being part of the bias.
Baseline:
The anchoring bias, or focalism, is the tendency to rely too heavily—to "anchor"—on one trait or piece of information when making decisions (usually the first piece of information acquired on that subject).
Base rate fallacy or base rate neglect, the tendency to ignore general information and focus on information only pertaining to the specific case, even when the general information is more important.
Dunning–Kruger effect, the tendency for unskilled individuals to overestimate their own ability and the tendency for experts to underestimate their own ability.
Gambler's fallacy, the tendency to think that future probabilities are altered by past events, when in reality they are unchanged. The fallacy arises from an erroneous conceptualization of the law of large numbers. For example, "I've flipped heads with this coin five times consecutively, so the chance of tails coming out on the sixth flip is much greater than heads."
Hard–easy effect, the tendency to overestimate one's ability to accomplish hard tasks, and underestimate one's ability to accomplish easy tasks.
Hot-hand fallacy (also known as "hot hand phenomenon" or "hot hand"), the belief that a person who has experienced success with a random event has a greater chance of further success in additional attempts.
Insensitivity to sample size, the tendency to under-expect variation in small samples.
Interoceptive bias or Hungry judge effect: The tendency for sensory input about the body itself to affect one's judgement about external, unrelated circumstances. (As for example, in parole judges who are more lenient when fed and rested.)
Conservatism or Regressive bias: Tendency to remember high values and high likelihoods/probabilities/frequencies as lower than they actually were and low ones as higher than they actually were. Based on the evidence, memories are not extreme enough.
Subadditivity effect: The tendency to estimate that the likelihood of a remembered event is less than the sum of its (more than two) mutually exclusive components.
Systematic bias: Judgement that arises when targets of differentiating judgement become subject to effects of regression that are not equivalent.
Unit bias: The standard suggested amount of consumption (e.g., food serving size) is perceived to be appropriate, and a person would consume it all even if it is too much for this particular person.
Weber–Fechner law: Difficulty in perceiving and comparing small differences in large quantities.
Inertia:
Conservatism bias, the tendency to insufficiently revise one's belief when presented with new evidence.
Outcome:
Exaggerated expectation: The tendency to expect or predict more extreme outcomes than those outcomes that actually happen.
Hedonic recall bias: The tendency for people who are satisfied with their wage to overestimate how much they earn, and conversely, for people who are unsatisfied with their wage to underestimate it.
Illusion of validity, the tendency to overestimate the accuracy of one's judgments, especially when available information is consistent or inter-correlated.
Impact bias: The tendency to overestimate the length or the intensity of the impact of future feeling states.
Outcome bias: The tendency to judge a decision by its eventual outcome instead of the quality of the decision at the time it was made.
Planning fallacy, the tendency for people to underestimate the time it will take them to complete a given task.
Restraint bias, the tendency to overestimate one's ability to show restraint in the face of temptation.
Sexual overperception bias, the tendency to overestimate sexual interest of another person in oneself, and sexual underperception bias, the tendency to underestimate it.
Self-perspective:
Curse of knowledge: When better-informed people find it extremely difficult to think about problems from the perspective of lesser-informed people.
Extrinsic incentives bias, an exception to the fundamental attribution error, where people view others as having (situational) extrinsic motivations, while viewing themselves as having (dispositional) intrinsic motivations
False consensus effect, the tendency for people to overestimate the degree to which others agree with them.
Illusion of transparency, the tendency for people to overestimate the degree to which their personal mental state is known by others, and to overestimate how well they understand others' personal mental states.
Naïve cynicism, expecting more egocentric bias in others than in oneself.
Optimism bias: The tendency to be over-optimistic, underestimating greatly the probability of undesirable outcomes and overestimating favorable and pleasing outcomes (see also wishful thinking, valence effect, positive outcome bias, and compare pessimism bias).
Outgroup homogeneity bias, where individuals see members of other groups as being relatively less varied than members of their own group.
Pessimism bias: The tendency to overestimate the likelihood that bad things will happen. (compare optimism bias)
Spotlight effect: The tendency to overestimate the amount that other people notice one's appearance or behavior.
Worse-than-average effect: A tendency to believe ourselves to be worse than others at tasks which are difficult.

In decision or choice tasks, people select one option out of several.
Association:
Ambiguity effect, the tendency to avoid options for which the probability of a favorable outcome is unknown.
Authority bias, the tendency to attribute greater accuracy to the opinion of an authority figure (unrelated to its content) and be more influenced by that opinion.
Automation bias, the tendency to depend excessively on automated systems which can lead to erroneous automated information overriding correct decisions.
Default effect, the tendency to favor the default option when given a choice between several options.
Dread aversion, just as losses yield double the emotional impact of gains, dread yields double the emotional impact of savouring.
The framing effect is the tendency to draw different conclusions from the same information, depending on how that information is presented.
Hyperbolic discounting, where discounting is the tendency for people to have a stronger preference for more immediate payoffs relative to later payoffs. Hyperbolic discounting leads to choices that are inconsistent over time—people make choices today that their future selves would prefer not to have made, despite using the same reasoning. Also known as current moment bias or present bias, and related to Dynamic inconsistency. A good example of this is a study showed that when making food choices for the coming week, 74% of participants chose fruit, whereas when the food choice was for the current day, 70% chose chocolate.
Compassion fade, the tendency to behave more compassionately towards a small number of identifiable victims than to a large number of anonymous ones.
Loss aversion, where the perceived disutility of giving up an object is greater than the utility associated with acquiring it. (see also Sunk cost fallacy)
Neglect of probability, the tendency to completely disregard probability when making a decision under uncertainty.
Non-adaptive choice switching: After experiencing a bad outcome with a decision problem, the tendency to avoid the choice previously made when faced with the same decision problem again, even though the choice was optimal. Also known as "once bitten, twice shy" or "hot stove effect".
Prevention bias: When investing money to protect against risks, decision makers perceive that a dollar spent on prevention buys more security than a dollar spent on timely detection and response, even when investing in either option is equally effective.
Pseudocertainty effect, the tendency to make risk-averse choices if the expected outcome is good but risk-seeking choices if it is bad.
Risk compensation or Peltzman effect: The tendency to take greater risks when perceived safety increases.
Zero-risk bias, the preference for reducing a small risk to zero over a greater reduction in a larger risk.
Baseline:
Action bias: The tendency for someone to act when faced with a problem even when inaction would be more effective, or to act when no evident problem exists.
Additive bias: The tendency to solve problems through addition, even when subtraction is a better approach.
Decoy effect, where preferences for either option A or B change in favor of option B when option C is presented, which is completely dominated by option B (inferior in all respects) and partially dominated by option A.
Ballot order effect, where candidates who are listed first often receive a small but statistically significant increase in votes compared to those listed in lower positions.
Cheerleader effect, the tendency for people to appear more attractive in a group than in isolation.
Compromise effect, choices affected if presented as extreme or average
Denomination effect, the tendency to spend more money when it is denominated in small amounts (e.g., coins) rather than large amounts (e.g., bills).
Disposition effect, the tendency to sell an asset that has accumulated in value and resist selling an asset that has declined in value.
Distinction bias, the tendency to view two options as more dissimilar when evaluating them simultaneously than when evaluating them separately.
Less-is-better effect, the tendency to prefer a smaller set to a larger set judged separately, but not jointly.
Money illusion: The tendency to concentrate on the nominal value (face value) of money rather than its value in terms of purchasing power.
Phantom effect: choices affected by dominant but unavailable options
Normalcy bias, a form of cognitive dissonance, is the refusal to plan for, or react to, a disaster which has never happened before.
Projection bias: The tendency to overestimate how much one's future selves will share one's current preferences, thoughts and values, thus leading to sub-optimal choices.
Scope neglect or scope insensitivity, the tendency to be insensitive to the size of a problem when evaluating it. For example, being willing to pay as much to save 2,000 children or 20,000 children.
Inertia:
Doubling-back aversion, the tendency for people to avoid retracing their steps or restarting a task, even when doing so would clearly save time or effort, because it feels like undoing past progress rather than making future gains.
Endowment effect, the tendency for people to demand much more to give up an object than they would be willing to pay to acquire it.
Escalation of commitment, irrational escalation, or sunk cost fallacy, where people justify increased investment in a decision, based on the cumulative prior investment, despite new evidence suggesting that the decision was probably wrong.
Functional fixedness, a tendency limiting a person to using an object only in the way it is traditionally used.
Mere exposure effect or familiarity principle (in social psychology): The tendency to express undue liking for things merely because of familiarity with them.
Plan continuation bias, failure to recognize that the original plan of action is no longer appropriate for a changing situation or for a situation that is different from anticipated.
Semmelweis reflex, the tendency to reject new evidence that contradicts a paradigm.
Shared information bias: The tendency for group members to spend more time and energy discussing information that all members are already familiar with (i.e., shared information), and less time and energy discussing information that only some members are aware of (i.e., unshared information).
Status quo bias, the tendency to prefer things to stay relatively the same.
Well travelled road effect, the tendency to underestimate the duration taken to traverse oft-travelled routes and overestimate the duration taken to traverse less familiar routes.
Outcome:
Present bias: The tendency of people to give stronger weight to payoffs that are closer to the present time when considering trade-offs between two future moments.
Reactance: The urge to do the opposite of what someone wants one to do out of a need to resist a perceived attempt to constrain one's freedom of choice (see also Reverse psychology).
Self-perspective:
Effort justification is a person's tendency to attribute greater value to an outcome if they had to put effort into achieving it. This can result in more value being applied to an outcome than it actually has. An example of this is the IKEA effect, the tendency for people to place a disproportionately high value on objects that they partially assembled themselves, such as furniture from IKEA, regardless of the quality of the end product.
Law of the instrument, an over-reliance on a familiar tool or methods, ignoring or under-valuing alternative approaches. "If all you have is a hammer, everything looks like a nail."
Not invented here, an aversion to contact with or use of products, research, standards, or knowledge developed outside a group.
Reactive devaluation: Devaluing proposals only because they purportedly originated with an adversary.
Social comparison bias: The tendency, when making decisions, to favour potential candidates who do not compete with one's own particular strengths.

In hypothesis assessment, people determine whether a statement is true or false.
Association:
Agent detection bias, the inclination to presume the purposeful intervention of a sentient or intelligent agent.
Availability cascade, a self-reinforcing process in which a collective belief gains more and more plausibility through its increasing repetition in public discourse (or "repeat something long enough and it will become true"). See also availability heuristic.
Cognitive dissonance is the perception of contradictory information and the mental toll of it.
Common source bias, the tendency to combine or compare research studies from the same source, or from sources that use the same methodologies or data.
False priors are initial beliefs and knowledge which interfere with the unbiased evaluation of factual evidence and lead to incorrect conclusions.
Fluency heuristic. If one object is processed more fluently, faster, or more smoothly than another, the mind infers that this object has the higher value with respect to the question being considered. In other words, the more skillfully or elegantly an idea is communicated, the more likely it is to be considered seriously, whether or not it is logical
Groupthink, the psychological phenomenon that occurs within a group of people in which the desire for harmony or conformity in the group results in an irrational or dysfunctional decision-making outcome. Group members try to minimize conflict and reach a consensus decision without critical evaluation of alternative viewpoints by actively suppressing dissenting viewpoints, and by isolating themselves from outside influences.
Groupshift, the tendency for decisions to be more risk-seeking or risk-averse than the group as a whole, if the group is already biased in that direction
Illusion of explanatory depth, the tendency to believe that one understands a topic much better than one actually does. The effect is strongest for explanatory knowledge, whereas people tend to be better at self-assessments for procedural, narrative, or factual knowledge.
Illusory truth effect, the tendency to believe that a statement is true if it is easier to process, or if it has been stated multiple times, regardless of its actual veracity. People are more likely to identify as true statements those they have previously heard (even if they cannot consciously remember having heard them), regardless of the actual validity of the statement. In other words, a person is more likely to believe a familiar statement than an unfamiliar one.
Probability matching: Sub-optimal matching of the probability of choices with the probability of reward in a stochastic context.
Rhyme as reason effect, where rhyming statements are perceived as more truthful.
Quantification bias, the tendency to ascribe more weight to measured/quantified metrics than to unquantifiable values. See also: McNamara fallacy.
Salience bias, the tendency to focus on items that are more prominent or emotionally striking and ignore those that are unremarkable, even though this difference is often irrelevant by objective standards. See also von Restorff effect.
Saying is believing effect: Communicating a socially tuned message to an audience can lead to a bias of identifying the tuned message as one's own thoughts.
Selection bias, which happens when the members of a statistical sample are not chosen completely at random, which leads to the sample not being representative of the population.
Subadditivity effect, the tendency to judge the probability of the whole to be less than the probabilities of the parts.
Truth bias is people's inclination towards believing, to some degree, the communication of another person, regardless of whether or not that person is actually lying or being untruthful.
Outcome:
Barnum effect or Forer effect, the tendency for individuals to give high accuracy ratings to descriptions of their personality that supposedly are tailored specifically for them, but are in fact vague and general enough to apply to a wide range of people. This effect can provide a partial explanation for the widespread acceptance of some beliefs and practices, such as astrology, fortune telling, graphology, and some types of personality tests.
Belief bias, an effect where someone's evaluation of the logical strength of an argument is biased by the believability of the conclusion.
Berkson's paradox, the tendency to misinterpret statistical experiments involving conditional probabilities.
Clustering illusion, the tendency to overestimate the importance of small runs, streaks, or clusters in large samples of random data (that is, seeing phantom patterns).
Confirmation bias is the tendency to search for, interpret, focus on and remember information in a way that confirms one's preconceptions.
Congruence bias, the tendency to test hypotheses exclusively through direct testing, instead of testing possible alternative hypotheses.
Extension neglect occurs where the quantity of the sample size is not sufficiently taken into consideration when assessing the outcome, relevance or judgement.
Gender bias, a widespread set of implicit biases that discriminate against a gender. For example, the assumption that women are less suited to jobs requiring high intellectual ability. Or the assumption that people or animals are male in the absence of any indicators of gender.
Illusory correlation: Inaccurately seeing a relationship between two events related by coincidence.
Information bias: The tendency to seek information even when it cannot affect action.
Observer-expectancy effect, when a researcher expects a given result and therefore unconsciously manipulates an experiment or misinterprets data in order to find it (see also subject-expectancy effect).
Overconfidence effect, a tendency to have excessive confidence in one's own answers to questions. For example, for certain types of questions, answers that people rate as "99% certain" turn out to be wrong 40% of the time.
Pareidolia, a tendency to perceive a vague and random stimulus (often an image or sound) as significant, e.g., seeing images of animals or faces in clouds, the man in the Moon, and hearing non-existent hidden messages on records played in reverse.
Subjective validation, where statements are perceived as true if a subject's belief demands it to be true. Also assigns perceived connections between coincidences. (Compare confirmation bias.)
Survivorship bias, which is concentrating on the people or things that "survived" some process and inadvertently overlooking those that did not because of their lack of visibility.
Unconscious bias or implicit bias: The underlying attitudes and stereotypes that people unconsciously attribute to another person or group of people that affect how they understand and engage with them. Many researchers suggest that unconscious bias occurs automatically as the brain makes quick judgments based on past experiences and background.
Value selection bias: The tendency to rely on existing numerical data when reasoning in an unfamiliar context, even if calculation or numerical manipulation is required.

In a causal attribution task, people are asked to explain the causes of behavior and events.
Outcome:
Apophenia: The tendency to perceive meaningful connections between unrelated things.
Assumed similarity bias: Where an individual assumes that others have more traits in common with them than those others actually do.
Context neglect bias, the tendency to neglect the human context of technological challenges.
Domain neglect bias, the tendency to neglect relevant domain knowledge while solving interdisciplinary problems.
Embodiment bias: Biases in attribution of meaning and perceived properties to objects or events based on the physical capacities and properties of the body, such as sex  and temperament
Form function attribution bias: In human–robot interaction, the tendency of people to make systematic errors when interacting with a robot. People may base their expectations and perceptions of a robot on its appearance (form) and attribute functions which do not necessarily mirror the true functions of the robot.
G. I. Joe fallacy, the tendency to think that knowing about cognitive bias is enough to overcome it.
Group attribution error, the biased belief that the characteristics of an individual group member are reflective of the group as a whole or the tendency to assume that group decision outcomes reflect the preferences of group members, even when information is available that clearly suggests otherwise.
Hostile attribution bias, the tendency to interpret others' behaviors as having hostile intent, even when the behavior is ambiguous or benign.
Illusory correlation, a tendency to inaccurately perceive a relationship between two unrelated events.
Illusion of control, the tendency to overestimate one's degree of influence over other external events.
Intentionality bias, the tendency to judge human action to be intentional rather than accidental.
Just-world fallacy, the tendency for people to want to believe that the world is fundamentally just, causing them to rationalize an otherwise inexplicable injustice as deserved by the victim(s).
Motonormativity: Also known as windshield bias, car blindness or car brain. The assumption that motor vehicle use is an unremarkable social norm, causing people to discount harms caused by motor vehicle use compared to similar harms caused by other behaviors.
Plant blindness: The tendency to ignore plants in their environment and a failure to recognize and appreciate the utility of plants to life on earth.
Pro-innovation bias: The tendency to have an excessive optimism towards an invention or innovation's usefulness throughout society, while often failing to identify its limitations and weaknesses.
Proportionality bias: Our innate tendency to assume that big events have big causes, may also explain our tendency to accept conspiracy theories.
Puritanical bias, the tendency to attribute cause of an undesirable outcome or wrongdoing by an individual to a moral deficiency or lack of self-control rather than taking into account the impact of broader societal determinants .
Surrogation: Losing sight of the strategic construct that a measure is intended to represent, and subsequently acting as though the measure is the construct of interest.
System justification, the tendency to defend and bolster the status quo. Existing social, economic, and political arrangements tend to be preferred, and alternatives disparaged, sometimes even at the expense of individual and collective self-interest.
Teleological Bias: The tendency to engage in overgeneralized ascriptions of purpose to entities and events that did not arise from goal-directed action, design, or selection based on functional effects.
Turkey illusion: Absence of expectation of sudden trend breaks in continuous developments
Self-perspective:
Actor-observer bias, the tendency for explanations of other individuals' behaviors to overemphasize the influence of their personality and underemphasize the influence of their situation (see also Fundamental attribution error), and for explanations of one's own behaviors to do the opposite (that is, to overemphasize the influence of our situation and underemphasize the influence of our own personality).
Defensive attribution hypothesis, a tendency to attribute more blame to a harm-doer as the outcome becomes more severe or as personal or situational similarity to the victim decreases.
Egocentric bias: Recalling the past in a self-serving manner, e.g., remembering one's exam grades as being better than they were, or remembering a caught fish as bigger than it really was. Also the tendency to rely too heavily on one's own perspective and/or have a different perception of oneself relative to others.
Experimenter's or expectation bias, the tendency for experimenters to believe, certify, and publish data that agree with their expectations for the outcome of an experiment, and to disbelieve, discard, or downgrade the corresponding weightings for data that appear to conflict with those expectations.
False uniqueness bias, the tendency of people to see their projects and themselves as more singular than they actually are.
Fundamental attribution error, the tendency for people to overemphasize personality-based explanations for behaviors observed in others while under-emphasizing the role and power of situational influences on the same behavior (see also actor-observer bias, group attribution error, positivity effect, and negativity effect).
Ingroup bias: the tendency for people to give preferential treatment to others they perceive to be members of their own groups.
Objectivity illusion, the phenomena where people tend to believe that they are more objective and unbiased than others. This bias can apply to itself – where people are able to see when others are affected by the objectivity illusion, but unable to see it in themselves. See also bias blind spot.
Ostrich effect: The tendency to avoid acknowledgment of an obviously bad situation to avoid the bad feelings that may come with acknowledgment of the situation.
Outgroup favoritism: When some socially disadvantaged groups will express favorable attitudes (and even preferences) toward social, cultural, or ethnic groups other than their own.
Pygmalion effect: The phenomenon whereby others' expectations of a target person affect the target person's performance.
Selective perception, the tendency for expectations to affect perception.
Self-serving bias, the tendency to claim more responsibility for successes than failures. It may also manifest itself as a tendency for people to evaluate ambiguous information in a way beneficial to their interests (see also group-serving bias).
Ultimate attribution error, similar to the fundamental attribution error, in this error a person is likely to make an internal attribution to an entire group instead of the individuals within the group.

In a recall or memory task, people are asked to recall or recognize previous material.
Association:
Boundary extension: Remembering the background of an image as being larger or more expansive than the foreground
Childhood amnesia: The retention of few memories from before the age of four.
Consistency bias: Incorrectly remembering one's past attitudes and behaviour as resembling present attitudes and behaviour.
Contrast effect, the enhancement or reduction of a certain stimulus's perception when compared with a recently observed, contrasting object.
Cryptomnesia, where a memory is mistaken for novel thought or imagination, because there is no subjective experience of it being a memory.
Cue-dependent forgetting context effect: That cognition and memory are dependent on context, such that out-of-context memories are more difficult to retrieve than in-context memories (e.g., recall time and accuracy for a work-related memory will be lower at home, and vice versa).
Google effect: The tendency to forget information that can be found readily online by using Internet search engines.
Duration neglect, the neglect of the duration of an episode in determining its value.
Fading affect bias: A bias in which the emotion associated with unpleasant memories fades more quickly than the emotion associated with pleasant ones.
False memory, where imagination is mistaken for a memory.
Humor effect: That humorous items are more easily remembered than non-humorous ones, which might be explained by the distinctiveness of humor, the increased cognitive processing time to understand the humor, or the emotional arousal caused by the humor.
Implicit association, where the speed with which people can match words depends on how closely they are associated.
Lag effect: The phenomenon whereby learning is greater when studying is spread out over time, as opposed to studying the same amount of time in a single session. See also spacing effect.
Levels-of-processing effect: That different methods of encoding information into memory have different levels of effectiveness.
Leveling and sharpening: Memory distortions introduced by the loss of details in a recollection over time, often concurrent with sharpening or selective recollection of certain details that take on exaggerated significance in relation to the details or aspects of the experience lost through leveling. Both biases may be reinforced over time, and by repeated recollection or re-telling of a memory.
Memory inhibition: Being shown some items from a list makes it harder to retrieve the other items (e.g., Slamecka, 1968).
Misinformation effect: Memory becoming less accurate because of interference from post-event information. cf. continued influence effect, where misinformation about an event, despite later being corrected, continues to influence memory about the event.
Modality effect: That memory recall is higher for the last items of a list when the list items were received via speech than when they were received through writing.
Repetition blindness: Unexpected difficulty in remembering more than one instance of a visual sequence
Mood-congruent memory bias (state-dependent memory): The improved recall of information congruent with one's current mood.
Next-in-line effect: When taking turns speaking in a group using a predetermined order (e.g. going clockwise around a room, taking numbers, etc.) people tend to have diminished recall for the words of the person who spoke immediately before them.
Part-list cueing effect: That being shown some items from a list and later retrieving one item causes it to become harder to retrieve the other items.
Peak–end rule: That people seem to perceive not the sum of an experience but the average of how it was at its peak (e.g., pleasant or unpleasant) and how it ended.
Persistence: The unwanted recurrence of memories of a traumatic event.
The Perky effect, where real images can influence imagined images, or be misremembered as imagined rather than real
Picture superiority effect: The notion that concepts that are learned by viewing pictures are more easily and frequently recalled than are concepts that are learned by viewing their written word form counterparts.
Positivity effect (Socioemotional selectivity theory): Older adults' tendency to favor good over bad information in their memories. See also euphoric recall
Processing difficulty effect: That information that takes longer to read and is thought about more (processed with more difficulty) is more easily remembered. See also levels-of-processing effect.
Reminiscence bump: The recalling of more personal events from adolescence and early adulthood than personal events from other lifetime periods.
Social cryptomnesia, a failure by people and society in general to remember the origin of a change, in which people know that a change has occurred in society, but forget how this change occurred; that is, the steps that were taken to bring this change about, and who took these steps. This has led to reduced social credit towards the minorities who made major sacrifices that led to a change in societal values.
Source confusion, episodic memories are confused with other information, creating distorted memories.
Spacing effect: That information is better recalled if exposure to it is repeated over a long span of time rather than a short one.
Suffix effect: Diminishment of the recency effect because a sound item is appended to the list that the subject is not required to recall. A form of serial position effect. Cf. recency effect and primacy effect.
Suggestibility, where ideas suggested by a questioner are mistaken for memory.
Telescoping effect: The tendency to displace recent events backwards in time and remote events forward in time, so that recent events appear more remote, and remote events, more recent.
Testing effect: The fact that one more easily recall information one has read by rewriting it instead of rereading it. Frequent testing of material that has been committed to memory improves memory recall.
Tip of the tongue phenomenon: When a subject is able to recall parts of an item, or related information, but is frustratingly unable to recall the whole item. This is thought to be an instance of "blocking" where multiple similar memories are being recalled and interfere with each other.
Verbatim effect: That the "gist" of what someone has said is better remembered than the verbatim wording. This is because memories are representations, not exact copies.
Zeigarnik effect: That uncompleted or interrupted tasks are remembered better than completed ones.
Baseline:
Bizarreness effect: Bizarre material is better remembered than common material.
Frequency illusion or Baader–Meinhof phenomenon. The frequency illusion is that once something has been noticed then every instance of that thing is noticed, leading to the belief it has a high frequency of occurrence (a form of selection bias). The Baader–Meinhof phenomenon is the illusion where something that has recently come to one's attention suddenly seems to appear with improbable frequency shortly afterwards. It was named after an incidence of frequency illusion in which the Baader–Meinhof Group was mentioned.
List-length effect: A smaller percentage of items are remembered in a longer list, but as the length of the list increases, the absolute number of items remembered increases as well.
Negativity bias or Negativity effect: The phenomenon of having better recall of unpleasant memories than of pleasant ones. (see also actor-observer bias, group attribution error, positivity effect, and negativity effect).
Primacy effect: Where an item at the beginning of a list is more easily recalled. A form of serial position effect. See also recency effect and suffix effect.
Recency effect: A form of serial position effect where an item at the end of a list is easier to recall. This can be disrupted by the suffix effect. See also primacy effect.
Serial position effect: That items near the end of a sequence are the easiest to recall, followed by the items at the beginning of a sequence; items in the middle are the least likely to be remembered. See also recency effect, primacy effect and suffix effect.
von Restorff effect: That an item that sticks out is more likely to be remembered than other items.
Inertia:
Attentional bias, the tendency of perception to be affected by recurring thoughts.
Continued influence effect: Misinformation continues to influence memory and reasoning about an event, despite the misinformation having been corrected. cf. misinformation effect, where the original memory is affected by incorrect information received later.
Stereotype bias or stereotypical bias: Memory distorted towards stereotypes (e.g., racial or gender).
Outcome:
Choice-supportive bias: The tendency to remember one's choices as better than they actually were.
Declinism: The predisposition to view the past favorably (rosy retrospection) and the future unfavorably.
Euphoric recall: The tendency of people to remember past experiences favorably while overlooking bad experiences associated with them.
Hindsight bias: Sometimes called the "I-knew-it-all-along" effect, or the "Hindsight is 20/20" effect, is the tendency to see past events as having been predictable before they happened.
Recency illusion: The illusion that a phenomenon one has noticed only recently is itself recent. Often used to refer to linguistic phenomena; the illusion that a word or language usage that one has noticed only recently is an innovation when it is, in fact, long-established (see also frequency illusion). Also recency bias is a cognitive bias that favors recent events over historic ones.  A memory bias, recency bias gives "greater importance to the most recent event", such as the final lawyer's closing argument a jury hears before being dismissed to deliberate.
Rosy retrospection: The remembering of the past as having been better than it really was.
Self-perspective:
Cross-race effect: The tendency for people of one race to have difficulty identifying members of a race other than their own.
Gender differences in eyewitness memory: The tendency for a witness to remember more details about someone of the same gender.
Generation effect (Self-generation effect): That self-generated information is remembered best. For instance, people are better able to recall memories of statements that they have generated than similar statements generated by others.
Placement bias: Tendency to remember ourselves to be better than others at tasks at which we rate ourselves above average (also Illusory superiority or Better-than-average effect) and tendency to remember ourselves to be worse than others at tasks at which we rate ourselves below average (also Worse-than-average effect).
Self-relevance effect: That memories relating to the self are better recalled than similar information relating to others.

In an opinion reporting task, people answer questions regarding their beliefs or opinions on political, moral, or social issues.
Association:
Halo effect, the tendency for a person's good or bad traits to "spill over" from one personality area to another in others' perceptions of them (see also physical attractiveness stereotype).
Moral credential effect: Occurs when someone who does something good gives themselves permission to be less good in the future.
Negativity bias: Social judgments are more influenced by negative information than positive information.
Inertia:
Backfire effect, a tendency to react to disconfirming evidence by strengthening one's previous beliefs.
End-of-history illusion: The age-independent belief that one will change less in the future than one has in the past.
Omission bias: The tendency to judge harmful actions (commissions) as worse, or less moral, than equally harmful inactions (omissions).
Outcome:
Bandwagon effect, the tendency to do (or believe) things because many other people do (or believe) the same. Related to groupthink and herd behavior.
Courtesy bias, the tendency to give an opinion that is more socially correct than one's true opinion, so as to avoid offending anyone.
Illusion of learning, a false belief that if you understand something you learned and acquired a knowledge about it.
Moral luck, the tendency for people to ascribe greater or lesser moral standing based on the outcome of an event.
Misinterpreted-effort hypothesis: Perceiving effort as a poor learning.
Social desirability bias, the tendency to over-report socially desirable characteristics or behaviours in oneself and under-report socially undesirable characteristics or behaviours. See also: § Courtesy bias.
Stereotyping, expecting a member of a group to have certain characteristics without having actual information about that individual.
Women are wonderful effect: A tendency to associate more good attributes with women than with men.
Self-perspective:
Anthropocentric thinking, the tendency to use human analogies as a basis for reasoning about other, less familiar, biological phenomena.
Anthropomorphism is characterization of animals, objects, and abstract concepts as possessing human traits, emotions, or intentions. The opposite bias, of not attributing feelings or thoughts to another person, is dehumanised perception, a type of objectification.
Ben Franklin effect, where a person who has performed a favor for someone is more likely to do another favor for that person than they would be if they had received a favor from that person.
Bias blind spot, the tendency to see oneself as less biased than other people, or to be able to identify more cognitive biases in others than in oneself.
Fundamental pain bias: The tendency for people to believe they accurately report their own pain levels while holding the paradoxical belief that others exaggerate it.
Illusion of asymmetric insight, where people perceive their knowledge of their peers to surpass their peers' knowledge of them.
Illusory superiority, the tendency to overestimate one's desirable qualities, and underestimate undesirable qualities, relative to other people. (Also known as "Lake Wobegon effect", "better-than-average effect", or "superiority bias".)
Impostor Syndrome, a psychological occurrence in which an individual doubts their skills, talents, or accomplishments and has a persistent internalized fear of being exposed as a fraud.  Also known as impostor phenomenon.
Naïve realism, the belief that we see reality as it really is—objectively and without bias; that the facts are plain for all to see; that rational people will agree with us; and that those who do not are either uninformed, lazy, irrational, or biased.
Third-person effect, a tendency to believe that mass-communicated media messages have a greater effect on others than on themselves.
Trait ascription bias, the tendency for people to view themselves as relatively variable in terms of personality, behavior, and mood while viewing others as much more predictable.
Zero-sum bias, where a situation is incorrectly perceived to be like a zero-sum game, in which any gain by one person necessarily comes at the expense of another.

In psychology, decision-making (also spelled decision making and decisionmaking) is regarded as the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options. It could be either rational or irrational. The decision-making process is a reasoning process based on assumptions of values, preferences and beliefs of the decision-maker. Every decision-making process produces a final choice, which may or may not prompt action.
Research about decision-making is also published under the label problem solving, particularly in European psychological research.

Decision-making can be regarded as a problem-solving activity yielding a solution deemed to be optimal, or at least satisfactory. It is therefore a process which can be more or less rational or irrational and can be based on explicit or tacit knowledge and beliefs. Tacit knowledge is often used to fill the gaps in complex decision-making processes. Usually, both of these types of knowledge, tacit and explicit, are used together in the decision-making process.
Human performance has been the subject of active research from several perspectives:
Psychological: examining individual decisions in the context of a set of needs, preferences and values the individual has or seeks.
Cognitive: the decision-making process is regarded as a continuous process integrated in the interaction with the environment.
Normative: the analysis of individual decisions concerned with the logic of decision-making, or communicative rationality, and the invariant choice it leads to.
A major part of decision-making involves the analysis of a finite set of alternatives described in terms of evaluative criteria. Then the task might be to rank these alternatives in terms of how attractive they are to the decision-maker(s) when all the criteria are considered simultaneously. Another task might be to find the best alternative or to determine the relative total priority of each alternative (for instance, if alternatives represent projects competing for funds) when all the criteria are considered simultaneously. Solving such problems is the focus of multiple-criteria decision analysis (MCDA). This area of decision-making, although long established, has attracted the interest of many researchers and practitioners and is still highly debated as there are many MCDA methods which may yield very different results when they are applied to exactly the same data. This leads to the formulation of a decision-making paradox. Logical decision-making is an important part of all science-based professions, where specialists apply their knowledge in a given area to make informed decisions. For example, medical decision-making often involves a diagnosis and the selection of appropriate treatment. But naturalistic decision-making research shows that in situations with higher time pressure, higher stakes, or increased ambiguities, experts may use intuitive decision-making rather than structured approaches. They may follow a recognition-primed decision that fits their experience, and arrive at a course of action without weighing alternatives.
The decision-maker's environment can play a part in the decision-making process. For example, environmental complexity is a factor that influences cognitive function. A complex environment is an environment with a large number of different possible states which come and go over time. Studies done at the University of Colorado have shown that more complex environments correlate with higher cognitive function, which means that a decision can be influenced by the location. One experiment measured complexity in a room by the number of small objects and appliances present; a simple room had less of those things. Cognitive function was greatly affected by the higher measure of environmental complexity, making it easier to think about the situation and make a better decision.

It is important to differentiate between problem solving, or problem analysis, and decision-making. Problem solving is the process of investigating the given information and finding all possible solutions through invention or discovery. Traditionally, it is argued that problem solving is a step towards decision making, so that the information gathered in that process may be used towards decision-making.
Characteristics of problem-solving
Problems are merely deviations from performance standards.
Problems must be precisely identified and described
Problems are caused by a change from a distinctive feature
Something can always be used to distinguish between what has and has not been affected by a cause
Causes of problems can be deduced from relevant changes found in analyzing the problem
The most likely cause of a problem is the one that exactly explains all the facts while having the fewest (or weakest) assumptions (Occam's razor).
Characteristics of decision-making
Objectives must first be established
Objectives must be classified and placed in order of importance
Alternative actions must be developed
The alternatives must be evaluated against all the objectives
The alternative that is able to achieve all the objectives is the tentative decision
The tentative decision is evaluated for more possible consequences
Decisive actions are taken, and additional actions are taken to prevent any adverse consequences from becoming problems and starting both systems (problem analysis and decision-making) all over again
There are steps that are generally followed that result in a decision model that can be used to determine an optimal production plan
In a situation featuring conflict, role-playing may be helpful for predicting decisions to be made by involved parties
When participants do not agree on what the future will look like, Decision-making Under Deep Uncertainty may play a role.

When a group or individual is unable to make it through the problem-solving step on the way to making a decision, they could be experiencing analysis paralysis. Analysis paralysis is the state that a person enters where they are unable to make a decision, in effect paralyzing the outcome. Some of the main causes for analysis paralysis is the overwhelming flood of incoming data or the tendency to overanalyze the situation at hand. There are said to be three different types of analysis paralysis.
The first is analysis process paralysis. This type of paralysis is often spoken of as a cyclical process. One is unable to make a decision because they get stuck going over the information again and again for fear of making the wrong decision.
The second is decision precision paralysis. This paralysis is cyclical, just like the first one, but instead of going over the same information, the decision-maker will find new questions and information from their analysis and that will lead them to explore into further possibilities rather than making a decision.
The third is risk uncertainty paralysis. This paralysis occurs when the decision-maker wants to eliminate any uncertainty but the examination of provided information is unable to get rid of all uncertainty.

On the opposite side of analysis paralysis is the phenomenon called extinction by instinct. Extinction by instinct is the state that a person is in when they make careless decisions without detailed planning or thorough systematic processes. Extinction by instinct can possibly be fixed by implementing a structural system, like checks and balances into a group or one's life. Analysis paralysis is the exact opposite where a group's schedule could be saturated by too much of a structural checks and balance system.
Groupthink is another occurrence that falls under the idea of extinction by instinct. Groupthink is when members in a group become more involved in the "value of the group (and their being part of it) higher than anything else"; thus, creating a habit of making decisions quickly and unanimously. In other words, a group stuck in groupthink is participating in the phenomenon of extinction by instinct.

Information overload is "a gap between the volume of information and the tools we have to assimilate it". Information used in decision-making is to reduce or eliminate the uncertainty. Excessive information affects problem processing and tasking, which affects decision-making. Psychologist George Armitage Miller suggests that humans' decision making becomes inhibited because human brains can only hold a limited amount of information. Crystal C. Hall and colleagues described an "illusion of knowledge", which means that as individuals encounter too much knowledge, it can interfere with their ability to make rational decisions. Other names for information overload are information anxiety, information explosion, infobesity, and infoxication.

Decision fatigue is when a sizable amount of decision-making leads to a decline in decision-making skills. People who make decisions in an extended period of time begin to lose mental energy needed to analyze all possible solutions. Impulsive decision-making and decision avoidance are two possible paths that extend from decision fatigue. Impulse decisions are made more often when a person is tired of analysis situations or solutions; the solution they make is to act and not think. Decision avoidance is when a person evades the situation entirely by not ever making a decision. Decision avoidance is different from analysis paralysis because this sensation is about avoiding the situation entirely, while analysis paralysis is continually looking at the decisions to be made but still unable to make a choice.

Evaluation and analysis of past decisions are complementary to decision-making. See also mental accounting and Postmortem documentation.

Decision-making is a region of intense study in the fields of systems neuroscience, and cognitive neuroscience. Several brain structures, including the anterior cingulate cortex (ACC), orbitofrontal cortex, and the overlapping ventromedial prefrontal cortex are believed to be involved in decision-making processes. A neuroimaging study found distinctive patterns of neural activation in these regions depending on whether decisions were made on the basis of perceived personal volition or following directions from someone else. Patients with damage to the ventromedial prefrontal cortex have difficulty making advantageous decisions.
A common laboratory paradigm for studying neural decision-making is the two-alternative forced choice task (2AFC), in which a subject has to choose between two alternatives within a certain time. A study of a two-alternative forced choice task involving rhesus monkeys found that neurons in the parietal cortex not only represent the formation of a decision but also signal the degree of certainty (or "confidence") associated with the decision. A 2012 study found that rats and humans can optimally accumulate incoming sensory evidence, to make statistically optimal decisions.  Another study found that lesions to the ACC in the macaque resulted in impaired decision-making in the long run of reinforcement guided tasks suggesting that the ACC may be involved in evaluating past reinforcement information and guiding future action. It has recently been argued that the development of formal frameworks will allow neuroscientists to study richer and more naturalistic paradigms than simple 2AFC decision tasks; in particular, such decisions may involve planning and information search across temporally extended environments.

Emotion appears able to aid the decision-making process. Decision-making often occurs in the face of uncertainty about whether one's choices will lead to benefit or harm (see also Risk). The somatic marker hypothesis is a neurobiological theory of how decisions are made in the face of uncertain outcomes. This theory holds that such decisions are aided by emotions, in the form of bodily states, that are elicited during the deliberation of future consequences and that mark different options for behavior as being advantageous or disadvantageous. This process involves an interplay between neural systems that elicit emotional/bodily states and neural systems that map these emotional/bodily states. A recent lesion mapping study of 152 patients with focal brain lesions conducted by Aron K. Barbey and colleagues provided evidence to help discover the neural mechanisms of emotional intelligence.

Decision-making techniques can be separated into two broad categories: group decision-making techniques and individual decision-making techniques. Individual decision-making techniques can also often be applied by a group.

Consensus decision-making tries to avoid "winners" and "losers". Consensus requires that a majority approve a given course of action, but that the minority agree to go along with the course of action. In other words, if the minority opposes the course of action, consensus requires that the course of action be modified to remove objectionable features.
Voting-based methods:
Majority requires support from more than 50% of the members of the group. Thus, the bar for action is lower than with consensus. See also Condorcet method.
Plurality, where the largest faction in a group decides, even if it falls short of a majority.
Score voting (or range voting) lets each member score one or more of the available options, specifying both preference and intensity of preference information. The option with the highest total or average is chosen. This method has experimentally been shown to produce the lowest Bayesian regret among common voting methods, even when voters are strategic. It addresses issues of voting paradox and majority rule. See also approval voting.
Quadratic voting allows participants to cast their preference and intensity of preference for each decision (as opposed to a simple for or against decision). As in score voting, it addresses issues of voting paradox and majority rule.
Delphi method is a structured communication technique for groups, originally developed for collaborative forecasting but has also been used for policy making.
Dotmocracy is a facilitation method that relies on the use of special forms called Dotmocracy. They are sheets that allows large groups to collectively brainstorm and recognize agreements on an unlimited number of ideas they have each written.
Participative decision-making occurs when an authority opens up the decision-making process to a group of people for a collaborative effort.
Decision engineering uses a visual map of the decision-making process based on system dynamics and can be automated through a decision modeling tool, integrating big data, machine learning, and expert knowledge as appropriate.

Decisional balance sheet: listing the advantages and disadvantages (benefits and costs, pros and cons) of each option, as suggested by Plato's Protagoras and by Benjamin Franklin.
Expected-value optimization: choosing the alternative with the highest probability-weighted utility, possibly with some consideration for risk aversion. This may involve considering the opportunity cost of different alternatives. See also Decision analysis and Decision theory.
Satisficing: examining alternatives only until the first acceptable one is found. The opposite is maximizing or optimizing, in which many or all alternatives are examined in order to find the best option.
Acquiesce to a person in authority or an "expert"; "just following orders".
Anti-authoritarianism: taking the most opposite action compared to the advice of mistrusted authorities.
Flipism e.g. flipping a coin, cutting a deck of playing cards, and other random or coincidence methods – or prayer, tarot cards, astrology, augurs, revelation, or other forms of divination, superstition or pseudoscience.
Automated decision support: setting up criteria for automated decisions.
Decision support systems: using decision-making software when faced with highly complex decisions or when considering many stakeholders, categories, or other factors that affect decisions.
Decision coaching refers to support given by a health-care professionals to assist a person when making a health-related or medical-related decision. Decision coaching is an active process where the health professional and the patient are active in the decision-making process.

A variety of researchers have formulated similar prescriptive steps aimed at improving decision-making.

In the 1980s, psychologist Leon Mann and colleagues developed a decision-making process called GOFER, which they taught to adolescents, as summarized in the book Teaching Decision Making To Adolescents. The process was based on extensive earlier research conducted with psychologist Irving Janis. GOFER is an acronym for five decision-making steps:
Goals clarification: Survey values and objectives.
Options generation: Consider a wide range of alternative actions.
Facts-finding: Search for information.
Consideration of Effects: Weigh the positive and negative consequences of the options.
Review and implementation: Plan how to review the options and implement them.

In 2007, Pam Brown of Singleton Hospital in Swansea, Wales, divided the decision-making process into seven steps:
Outline the goal and outcome.
Gather data.
Develop alternatives (i.e., brainstorming).
List pros and cons of each alternative.
Make the decision.
Immediately take action to implement it.
Learn from and reflect on the decision.
In 2008, Kristina Guo published the DECIDE model of decision-making, which has six parts:
Define the problem
Establish or Enumerate all the criteria (constraints)
Consider or Collect all the alternatives
Identify the best alternative
Develop and implement a plan of action
Evaluate and monitor the solution and examine feedback when necessary
In 2009, professor John Pijanowski described how the Arkansas Program, an ethics curriculum at the University of Arkansas, used eight stages of moral decision-making based on the work of James Rest:
Establishing community: Create and nurture the relationships, norms, and procedures that will influence how problems are understood and communicated. This stage takes place prior to and during a moral dilemma.
Perception: Recognize that a problem exists.
Interpretation: Identify competing explanations for the problem, and evaluate the drivers behind those interpretations.
Judgment: Sift through various possible actions or responses and determine which is more justifiable.
Motivation: Examine the competing commitments which may distract from a more moral course of action and then prioritize and commit to moral values over other personal, institutional or social values.
Action: Follow through with action that supports the more justified decision.
Reflection in action.
Reflection on action.

There are four stages or phases that should be involved in all group decision-making:
Orientation. Members meet for the first time and start to get to know each other.
Conflict. Once group members become familiar with each other, disputes, little fights and arguments occur. Group members eventually work it out.
Emergence. The group begins to clear up vague opinions by talking about them.
Reinforcement. Members finally make a decision and provide justification for it.
It is said that establishing critical norms in a group improves the quality of decisions, while the majority of opinions (called consensus norms) do not.
Conflicts in socialization are divided in to functional and dysfunctional types. Functional conflicts are mostly the questioning the managers assumptions in their decision making and dysfunctional conflicts are like personal attacks and every action which decrease team effectiveness. Functional conflicts are the better ones to gain higher quality decision-making caused by the increased team knowledge and shared understanding.

In economics, it is thought that if humans are rational and free to make their own decisions, then they would behave according to rational choice theory. Rational choice theory says that a person consistently makes choices that lead to the best situation for themselves, taking into account all available considerations including costs and benefits; the rationality of these considerations is from the point of view of the person themselves, so a decision is not irrational just because someone else finds it questionable.
In reality, however, there are some factors that affect decision-making abilities and cause people to make irrational decisions – for example, to make contradictory choices when faced with the same problem framed in two different ways (see also Allais paradox).
Rational decision-making is a multi-step process for making choices between alternatives. The process of rational decision-making favors logic, objectivity, and analysis over subjectivity and insight. The irrational decision is more counter to logic. The decisions are made in haste and outcomes are not considered.
One of the most prominent theories of decision-making is subjective expected utility (SEU) theory, which describes the rational behavior of the decision-maker. The decision maker assesses different alternatives by their utilities and the subjective probability of occurrence.
Rational decision-making is often grounded on experience and theories that are able to put this approach on solid mathematical grounds so that subjectivity is reduced to a minimum, see e.g. scenario optimization.
Rational decision is generally seen as the best or most
likely decision to achieve the set goals or outcome.

It has been found that, unlike adults, children are less likely to have research strategy behaviors. One such behavior is adaptive decision-making, which is described as funneling and then analyzing the more promising information provided if the number of options to choose from increases. Adaptive decision-making behavior is somewhat present for children, ages 11–12 and older, but decreases in the presence the younger they are. The reason children are not as fluid in their decision making is that they lack the ability to weigh the cost and effort needed to gather information in the decision-making process. Some possibilities that explain this inability are knowledge deficits and lack of utilization skills. Children lack the metacognitive knowledge necessary to know when to use any strategies they do possess to change their approach to decision-making.
When it comes to the idea of fairness in decision-making, children and adults differ much less. Children are able to understand the concept of fairness in decision-making from an early age. Toddlers and infants, ranging from 9–21 months, understand basic principles of equality. The main difference found is that more complex principles of fairness in decision making such as contextual and intentional information do not come until children get older.

During their adolescent years, teens are known for their high-risk behaviors and rash decisions. Research has shown that there are differences in cognitive processes between adolescents and adults during decision-making. Researchers have concluded that differences in decision-making are not due to a lack of logic or reasoning, but more due to the immaturity of psychosocial capacities that influence decision-making. Examples of their undeveloped capacities which influence decision-making would be impulse control, emotion regulation, delayed gratification and resistance to peer pressure. In the past, researchers have thought that adolescent behavior was simply due to incompetency regarding decision-making. Currently, researchers have concluded that adults and adolescents are both competent decision-makers, not just adults. However, adolescents' competent decision-making skills decrease when psychosocial capacities become present.
Research has shown that risk-taking behaviors in adolescents may be the product of interactions between the socioemotional brain network and its cognitive-control network. The socioemotional part of the brain processes social and emotional stimuli and has been shown to be important in reward processing. The cognitive-control network assists in planning and self-regulation. Both of these sections of the brain change over the course of puberty. However, the socioemotional network changes quickly and abruptly, while the cognitive-control network changes more gradually. Because of this difference in change, the cognitive-control network, which usually regulates the socioemotional network, struggles to control the socioemotional network when psychosocial capacities are present.
When adolescents are exposed to social and emotional stimuli, their socioemotional network is activated as well as areas of the brain involved in reward processing. Because teens often gain a sense of reward from risk-taking behaviors, their repetition becomes ever more probable due to the reward experienced. In this, the process mirrors addiction. Teens can become addicted to risky behavior because they are in a high state of arousal and are rewarded for it not only by their own internal functions but also by their peers around them. A recent study suggests that adolescents have difficulties adequately adjusting beliefs in response to bad news (such as reading that smoking poses a greater risk to health than they thought), but do not differ from adults in their ability to alter beliefs in response to good news. This creates biased beliefs, which may lead to greater risk-taking.

Adults are generally better able to control their risk-taking because their cognitive-control system has matured enough to the point where it can control the socioemotional network, even in the context of high arousal or when psychosocial capacities are present. Also, adults are less likely to find themselves in situations that push them to do risky things. For example, teens are more likely to be around peers who peer pressure them into doing things, while adults are not as exposed to this sort of social setting.

Biases usually affects decision-making processes. They appear more when decision task has time pressure, is done under high stress and/or are highly complex.
Here is a list of commonly debated biases in judgment and decision-making:
Selective search for evidence (also known as confirmation bias): People tend to be willing to gather facts that support certain conclusions but disregard other facts that support different conclusions. Individuals who are highly defensive in this manner show significantly greater left prefrontal cortex activity as measured by EEG than do less defensive individuals.
Premature termination of search for evidence: People tend to accept the first alternative that looks like it might work.
Cognitive inertia is the unwillingness to change existing thought patterns in the face of new circumstances.
Selective perception: People actively screen out information that they do not think is important (see also Prejudice). In one demonstration of this effect, the discounting of arguments with which one disagrees (by judging them as untrue or irrelevant) was decreased by selective activation of the right prefrontal cortex.
Wishful thinking is a tendency to want to see things in a certain – usually positive – light, which can distort perception and thinking.
Choice-supportive bias occurs when people distort their memories of chosen and rejected options to make the chosen options seem more attractive.
Recency: People tend to place more attention on more recent information and either ignore or forget more distant information (see Semantic priming). The opposite effect in the first set of data or other information is termed primacy effect.
Repetition bias is a willingness to believe what one has been told most often and by the greatest number of different sources.
Anchoring and adjustment: Decisions are unduly influenced by initial information that shapes our view of subsequent information.
Groupthink is peer pressure to conform to the opinions held by the group.
Source credibility bias is a tendency to reject a person's statement on the basis of a bias against the person, organization, or group to which the person belongs. People preferentially accept statements by others that they like (see also Prejudice).
Incremental decision-making and escalating commitment: People look at a decision as a small step in a process, and this tends to perpetuate a series of similar decisions. This can be contrasted with zero-based decision-making (see Slippery slope).
Attribution asymmetry: People tend to attribute their own success to internal factors, including abilities and talents, but explain their failures in terms of external factors such as bad luck. The reverse bias is shown when people explain others' success or failure.
Role fulfillment is a tendency to conform to others' decision-making expectations.
Underestimating uncertainty and the illusion of control: People tend to underestimate future uncertainty because of a tendency to believe they have more control over events than they really do.
Framing bias: This is best avoided by increasing numeracy and presenting data in several formats (for example, using both absolute and relative scales).
Sunk-cost fallacy is a specific type of framing effect that affects decision-making. It involves an individual making a decision about a current situation based on what they have previously invested in the situation. An example of this would be an individual who is refraining from dropping a class that they are most likely to fail, due to the fact that they feel as though they have done so much work in the course thus far.
Prospect theory involves the idea that when faced with a decision-making event, an individual is more likely to take on a risk when evaluating potential losses, and is more likely to avoid risks when evaluating potential gains. This can influence one's decision-making depending if the situation entails a threat or opportunity.
Optimism bias is a tendency to overestimate the likelihood of positive events occurring in the future and underestimate the likelihood of negative life events. Such biased expectations are generated and maintained in the face of counter-evidence through a tendency to discount undesirable information. An optimism bias can alter risk perception and decision-making in many domains, ranging from finance to health.
Reference class forecasting was developed to eliminate or reduce cognitive biases in decision-making.

In groups, people generate decisions through active and complex processes. One method consists of three steps: initial preferences are expressed by members; the members of the group then gather and share information concerning those preferences; finally, the members combine their views and make a single choice about how to face the problem. Although these steps are relatively ordinary, judgements are often distorted by cognitive and motivational biases, including "sins of commission", "sins of omission", and "sins of imprecision".

Herbert A. Simon coined the phrase "bounded rationality" to express the idea that human decision-making is limited by available information, available time and the mind's information-processing ability. Further psychological research has identified individual differences between two cognitive styles: maximizers try to make an optimal decision, whereas satisficers simply try to find a solution that is "good enough". Maximizers tend to take longer to make decisions due to the need to maximize performance across all variables and make tradeoffs carefully; they also tend to more often regret their decisions (perhaps because they are more able than satisficers to recognize that a decision turned out to be sub-optimal).

The psychologist Daniel Kahneman, adopting terms originally proposed by the psychologists Keith Stanovich and Richard West, has theorized that a person's decision-making is the result of an interplay between two kinds of cognitive processes: an automatic intuitive system (called "System 1") and an effortful rational system (called "System 2"). System 1 is a bottom-up, fast, and implicit system of decision-making, while system 2 is a top-down, slow, and explicit system of decision-making. System 1 includes simple heuristics in judgment and decision-making such as the affect heuristic, the availability heuristic, the familiarity heuristic, and the representativeness heuristic.

Styles and methods of decision-making were elaborated by Aron Katsenelinboigen, the founder of predispositioning theory. In his analysis of styles and methods, Katsenelinboigen referred to the game of chess, saying that "chess does disclose various methods of operation, notably the creation of predisposition methods which may be applicable to other, more complex systems."
Katsenelinboigen states that apart from the methods (reactive and selective) and sub-methods randomization, predispositions, programming), there are two major styles: positional and combinational. Both styles are utilized in the game of chess. The two styles reflect two basic approaches to uncertainty: deterministic (combinational style) and indeterministic (positional style). Katsenelinboigen's definition of the two styles is the following.
The combinational style is characterized by:
a very narrow, clearly defined, primarily material goal; and
a program that links the initial position with the outcome.
In defining the combinational style in chess, Katsenelinboigen wrote: "The combinational style features a clearly formulated limited objective, namely the capture of material (the main constituent element of a chess position). The objective is implemented via a well-defined, and in some cases, unique sequence of moves aimed at reaching the set goal. As a rule, this sequence leaves no options for the opponent. Finding a combinational objective allows the player to focus all his energies on efficient execution, that is, the player's analysis may be limited to the pieces directly partaking in the combination. This approach is the crux of the combination and the combinational style of play.
The positional style is distinguished by:
a positional goal; and
a formation of semi-complete linkages between the initial step and final outcome.
"Unlike the combinational player, the positional player is occupied, first and foremost, with the elaboration of the position that will allow him to develop in the unknown future. In playing the positional style, the player must evaluate relational and material parameters as independent variables. ... The positional style gives the player the opportunity to develop a position until it becomes pregnant with a combination. However, the combination is not the final goal of the positional player – it helps him to achieve the desirable, keeping in mind a predisposition for future development. The pyrrhic victory is the best example of one's inability to think positionally."
The positional style serves to:
create a predisposition to the future development of the position;
induce the environment in a certain way;
absorb an unexpected outcome in one's favor; and
avoid the negative aspects of unexpected outcomes.

According to Isabel Briggs Myers, a person's decision-making process depends to a significant degree on their cognitive style. Myers developed a set of four bi-polar dimensions, called the Myers–Briggs Type Indicator (MBTI). The terminal points on these dimensions are: thinking and feeling; extroversion and introversion; judgment and perception; and sensing and intuition. She claimed that a person's decision-making style correlates well with how they score on these four dimensions. For example, someone who scored near the thinking, extroversion, sensing, and judgment ends of the dimensions would tend to have a logical, analytical, objective, critical, and empirical decision-making style. However, some psychologists say that the MBTI lacks reliability and validity and is poorly constructed.
Other studies suggest that these national or cross-cultural differences in decision-making exist across entire societies. For example, Maris Martinsons has found that American, Japanese and Chinese business leaders each exhibit a distinctive national style of decision-making.
The Myers–Briggs typology has been the subject of criticism regarding its poor psychometric properties.

In the general decision-making style (GDMS) test developed by Suzanne Scott and Reginald Bruce, there are five decision-making styles: rational, intuitive, dependent, avoidant, and spontaneous. These five different decision-making styles change depending on the context and situation, and one style is not necessarily better than any other. In the examples below, the individual is working for a company and is offered a job from a different company.
The rational style is an in-depth search for, and a strong consideration of, other options and/or information prior to making a decision. In this style, the individual would research the new job being offered, review their current job, and look at the pros and cons of taking the new job versus staying with their current company.
The intuitive style is confidence in one's initial feelings and gut reactions. In this style, if the individual initially prefers the new job because they have a feeling that the work environment is better suited for them, then they would decide to take the new job. The individual might not make this decision as soon as the job is offered.
The dependent style is asking for other people's input and instructions on what decision should be made. In this style, the individual could ask friends, family, coworkers, etc., but the individual might not ask all of these people.
The avoidant style is averting the responsibility of making a decision. In this style, the individual would not make a decision. Therefore, the individual would stick with their current job.
The spontaneous style is a need to make a decision as soon as possible rather than waiting to make a decision. In this style, the individual would either reject or accept the job as soon as it is offered.

A heuristic or heuristic technique (problem solving, mental shortcut, rule of thumb) is any approach to problem solving that employs a pragmatic method that is not fully optimized, perfected, or rationalized, but is nevertheless "good enough" as an approximation or attribute substitution. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision.
Heuristic reasoning is often based on induction, or on analogy ... Induction is the process of discovering general laws ... Induction tries to find regularity and coherence ... Its most conspicuous instruments are generalization, specialization, analogy. [...] Heuristic discusses human behavior in the face of problems [... that have been] preserved in the wisdom of proverbs.

A heuristic is a strategy that ignores part of the information, with the goal of making decisions more quickly, frugally, and/or accurately than more complex methods...
Heuristics are strategies based on rules to generate optimal decisions, like the anchoring effect and utility maximization problem. These strategies depend on using readily accessible, though loosely applicable, information to control problem solving in human beings, machines and abstract issues. When an individual applies a heuristic in practice, it generally performs as expected. However it can alternatively create systematic errors.
The most fundamental heuristic is trial and error, which can be used in everything from matching nuts and bolts to finding the values of variables in algebra problems. In mathematics, some common heuristics involve the use of visual representations, additional assumptions, forward/backward reasoning and simplification.
Dual process theory concerns embodied heuristics.

Lakatosian heuristics is based on the key term: Justification (epistemology).

One-reason decisions are algorithms that are made of three rules: search rules, confirmation rules (stopping), and decision rules
Take-the-best heuristic – Decision-making strategy
Hiatus heuristic: a "recency-of-last-purchase rule"
Default effect – Tendency to accept the default option
Priority heuristic
Take-the-first heuristic

A class whose function is to determine and filter out superfluous things.
Recognition heuristic
Fluency heuristic – Mental heuristic

Tracking heuristics is a class of heuristics.
Gaze heuristic
Pointing and calling – Railway safety technique

Trade-off – Situational decision
Tallying heuristic
Equality heuristic

Social heuristics – Decision-making processes in social environments
Imitation – Behaviour in which an individual observes and replicates another's behaviour
Tit for tat – English saying meaning "equivalent retaliation"
Wisdom of the crowd – Collective perception of a group of people

Propositional attitude – Concept in epistemology
Essence – That which makes or defines an entity
Analysis – Process of understanding a complex topic or substance
Falsifiability – Property of a statement that can be logically contradicted
Hierarchy of evidence – Heuristic ranking science research results

Affect heuristic – Mental shortcut based on emotion
Feedback – Process where information about current status is used to influence future status
Reinforcement – Consequence affecting an organism's future behavior
Stimulus–response model – Conceptual framework in psychology

Satisficing – Cognitive heuristic of searching for an acceptable decision
Representativeness heuristic – Tool for assisting judgement in uncertainty
Availability heuristic – Bias towards recently acquired information
Awareness – Perception or knowledge of something
Base and superstructure – Model of society in Marxist theory
Social organism – Model of social interactions
Dialectic – Method of reasoning via argumentation and contradiction
Continuum limit – Continuum limit in lattice models
Johari window – Technique in personality development
Social rationality
Desert (philosophy) – Condition of being deserving of something, whether good or bad
Less-is-better effect – Cognitive bias
Minimalist heuristic
Unification of theories in physics – Idea of connecting all of physics into one set of equations
Backward induction – Process of reasoning backwards in sequence

Optimality
Survival of the fittest – Phrase to describe the mechanism of natural selection
Mechanical equilibrium – When the net force on a particle is zero
Chemical equilibrium – When the ratio of reactants to products of a chemical reaction is constant with time
Homeostasis – State of steady internal conditions maintained by living things
Entropy – Property of a thermodynamic system

George Polya studied and published on heuristics in 1945. Polya (1945) cites Pappus of Alexandria as having written a text that Polya dubs Heuristic. Pappus' heuristic problem-solving methods consist of analysis and synthesis.

George Polya
Herbert A. Simon
Daniel Kahneman
Amos Tversky
Gerd Gigerenzer
Judea Pearl
Robin Dunbar
David Perkins Page
Herbert Spencer
Charles Alexander McMurry
Frank Morton McMurry
Lawrence Zalcman
Imre Lakatos
William C. Wimsatt
Alan Hodgkin
Andrew Huxley

Meno
How to solve it
Mathematics and Plausible Reasoning

The study of heuristics in human decision-making was developed in the 1970s and the 1980s, by the psychologists Amos Tversky and Daniel Kahneman, although the concept had been originally introduced by the Nobel laureate Herbert A. Simon. Simon's original primary object of research was problem solving that showed that we operate within what he calls bounded rationality. He coined the term satisficing, which denotes a situation in which people seek solutions, or accept choices or judgements, that are "good enough" for their purposes although they could be optimised.
Rudolf Groner analysed the history of heuristics from its roots in ancient Greece up to contemporary work in cognitive psychology and artificial intelligence, proposing a cognitive style "heuristic versus algorithmic thinking", which can be assessed by means of a validated questionnaire.

The adaptive toolbox contains strategies for fabricating heuristic devices. The core mental capacities are recall (memory), frequency, object permanence, and imitation. Gerd Gigerenzer and his research group argued that models of heuristics need to be formal to allow for predictions of behavior that can be tested. They study the fast and frugal heuristics in the "adaptive toolbox" of individuals or institutions, and the ecological rationality of these heuristics; that is, the conditions under which a given heuristic is likely to be successful. The descriptive study of the "adaptive toolbox" is done by observation and experiment, while the prescriptive study of ecological rationality requires mathematical analysis and computer simulation. Heuristics – such as the recognition heuristic, the take-the-best heuristic and fast-and-frugal trees – have been shown to be effective in predictions, particularly in situations of uncertainty. It is often said that heuristics trade accuracy for effort but this is only the case in situations of risk. Risk refers to situations where all possible actions, their outcomes and probabilities are known. In the absence of this information, that is under uncertainty, heuristics can achieve higher accuracy with lower effort. This finding, known as a less-is-more effect, would not have been found without formal models. The valuable insight of this program is that heuristics are effective not despite their simplicity – but because of it. Furthermore, Gigerenzer and Wolfgang Gaissmaier found that both individuals and organisations rely on heuristics in an adaptive way.

Heuristics, through greater refinement and research, have begun to be applied to other theories, or be explained by them. For example, the cognitive-experiential self-theory (CEST) is also an adaptive view of heuristic processing. CEST breaks down two systems that process information. At some times, roughly speaking, individuals consider issues rationally, systematically, logically, deliberately, effortfully, and verbally. On other occasions, individuals consider issues intuitively, effortlessly, globally, and emotionally. From this perspective, heuristics are part of a larger experiential processing system that is often adaptive, but vulnerable to error in situations that require logical analysis.

In 2002, Daniel Kahneman and Shane Frederick proposed that cognitive heuristics work by a process called attribute substitution, which happens without conscious awareness. According to this theory, when somebody makes a judgement (of a "target attribute") that is computationally complex, a more easily calculated "heuristic attribute" is substituted. In effect, a cognitively difficult problem is dealt with by answering a rather simpler problem, without being aware of this happening. This theory explains cases where judgements fail to show regression toward the mean. Heuristics can be considered to reduce the complexity of clinical judgments in health care.

In psychology, heuristics are simple, efficient rules, either learned or inculcated by evolutionary processes. These psychological heuristics have been proposed to explain how people make decisions, come to judgements, and solve problems. These rules typically come into play when people face complex problems or incomplete information. Researchers employ various methods to test whether people use these rules. The rules have been shown to work well under most circumstances, but in certain cases can lead to systematic errors or cognitive biases.

A heuristic device is used when an entity X exists to enable understanding of, or knowledge concerning, some other entity Y.
A good example is a model that, as it is never identical with what it models, is a heuristic device to enable understanding of what it models. Stories, metaphors, etc., can also be termed heuristic in this sense. A classic example is the notion of utopia as described in Plato's best-known work, The Republic. This means that the "ideal city" as depicted in The Republic is not given as something to be pursued, or to present an orientation-point for development. Rather, it shows how things would have to be connected, and how one thing would lead to another (often with highly problematic results), if one opted for certain principles and carried them through rigorously.
Heuristic is also often used as a noun to describe a rule of thumb, procedure, or method. Philosophers of science have emphasised the importance of heuristics in creative thought and the construction of scientific theories. Seminal works include Karl Popper's The Logic of Scientific Discovery and others by Imre Lakatos, Lindley Darden, and William C. Wimsatt.

In legal theory, especially in the theory of law and economics, heuristics are used in the law when case-by-case analysis would be impractical, insofar as "practicality" is defined by the interests of a governing body.
The present securities regulation regime largely assumes that all investors act as perfectly rational persons. In truth, actual investors face cognitive limitations from biases, heuristics, and framing effects. For instance, in all states in the United States the legal drinking age for unsupervised persons is 21 years, because it is argued that people need to be mature enough to make decisions involving the risks of alcohol consumption. However, assuming people mature at different rates, the specific age of 21 would be too late for some and too early for others. In this case, the somewhat arbitrary delineation is used because it is impossible or impractical to tell whether an individual is sufficiently mature for society to trust them with that kind of responsibility. Some proposed changes, however, have included the completion of an alcohol education course rather than the attainment of 21 years of age as the criterion for legal alcohol possession. This would put youth alcohol policy more on a case-by-case basis and less on a heuristic one, since the completion of such a course would presumably be voluntary and not uniform across the population.
The same reasoning applies to patent law. Patents are justified on the grounds that inventors must be protected so they have incentive to invent. It is therefore argued that it is in society's best interest that inventors receive a temporary government-granted monopoly on their idea, so that they can recoup investment costs and make economic profit for a limited period. In the United States, the length of this temporary monopoly is 20 years from the date the patent application was filed, though the monopoly does not actually begin until the application has matured into a patent. However, like the drinking age problem above, the specific length of time would need to be different for every product to be efficient. A 20-year term is used because it is difficult to tell what the number should be for any individual patent. More recently, some, including University of North Dakota law professor Eric E. Johnson, have argued that patents in different kinds of industries – such as software patents – should be protected for different lengths of time.

The bias–variance tradeoff gives insight into describing the less-is-more strategy. A heuristic can be used in artificial intelligence systems while searching a solution space. The heuristic is derived by using some function that is put into the system by the designer, or by adjusting the weight of branches based on how likely each branch is to lead to a goal node.

Heuristics refers to the cognitive shortcuts that individuals use to simplify decision-making processes in economic situations. Behavioral economics is a field that integrates insights from psychology and economics to better understand how people make decisions.
Anchoring and adjustment is one of the most extensively researched heuristics in behavioural economics. Anchoring is the tendency of people to make future judgements or conclusions based too heavily on the original information supplied to them. This initial knowledge functions as an anchor, and it can influence future judgements even if the anchor is entirely unrelated to the decisions at hand. Adjustment, on the other hand, is the process through which individuals make gradual changes to their initial judgements or conclusions.
Anchoring and adjustment has been observed in a wide range of decision-making contexts, including financial decision-making, consumer behavior, and negotiation. Researchers have identified a number of strategies that can be used to mitigate the effects of anchoring and adjustment, including providing multiple anchors, encouraging individuals to generate alternative anchors, and providing cognitive prompts to encourage more deliberative decision-making.
Other heuristics studied in behavioral economics include the representativeness heuristic, which refers to the tendency of individuals to categorize objects or events based on how similar they are to typical examples, and the availability heuristic, which refers to the tendency of individuals to judge the likelihood of an event based on how easily it comes to mind.

Stereotyping is a type of heuristic that people use to form opinions or make judgements about things they have never seen or experienced. They work as a mental shortcut to assess everything from the social status of a person (based on their actions), to classifying a plant as a tree based on it being tall, having a trunk, and that it has leaves (even though the person making the evaluation might never have seen that particular type of tree before).
Stereotypes, as first described by journalist Walter Lippmann in his book Public Opinion (1922), are the pictures we have in our heads that are built around experiences as well as what we are told about the world.

Behavioral economics is the study of the psychological (e.g. cognitive, behavioral, affective, social) factors involved in the decisions of individuals or institutions, and how these decisions deviate from those implied by traditional economic theory.
Behavioral economics is primarily concerned with the bounds of rationality of economic agents. Behavioral models typically integrate insights from psychology, neuroscience and microeconomic theory.
Behavioral economics began as a distinct field of study in the 1970s and 1980s, but can be traced back to 18th-century economists, such as Adam Smith, who deliberated how the economic behavior of individuals could be influenced by their desires.
The status of behavioral economics as a subfield of economics is a fairly recent development; the breakthroughs that laid the foundation for it were published through the last three decades of the 20th century. Behavioral economics is still growing as a field, being used increasingly in research and in teaching.

Early classical economists included psychological reasoning in much of their writing, though psychology at the time was not a recognized field of study. In The Theory of Moral Sentiments, Adam Smith wrote on concepts later popularized by modern Behavioral Economic theory, such as loss aversion. Jeremy Bentham, a Utilitarian philosopher in the 1700s conceptualized utility as a product of psychology. Other economists who incorporated psychological explanations in their works included Francis Edgeworth, Vilfredo Pareto and Irving Fisher.
A rejection and elimination of psychology from economics in the early 1900s brought on a period defined by a reliance on empiricism. There was a lack of confidence in hedonic theories, which saw pursuance of maximum benefit as an essential aspect in understanding human economic behavior. Hedonic analysis had shown little success in predicting human behavior, leading many to question its viability as a reliable source for prediction.
There was also a fear among economists that the involvement of psychology in shaping economic models was inordinate and a departure from accepted principles. They feared that an increased emphasis on psychology would undermine the mathematic components of the field.
To boost the ability of economics to predict accurately, economists started looking to tangible phenomena rather than theories based on human psychology. Psychology was seen as unreliable to many of these economists as it was a new field, not regarded as sufficiently scientific. Though a number of scholars expressed concern towards the positivism within economics, models of study dependent on psychological insights became rare. Economists instead conceptualized humans as purely rational and self-interested decision makers, illustrated in the concept of homo economicus.
The resurgence of psychology within economics, which facilitated the expansion of behavioral economics, has been linked to the cognitive revolution. In the 1960s, cognitive psychology began to shed more light on the brain as an information processing device (in contrast to behaviorist models). Psychologists in this field, such as Ward Edwards, Amos Tversky and Daniel Kahneman began to compare their cognitive models of decision-making under risk and uncertainty to economic models of rational behavior. These developments spurred economists to reconsider how psychology could be applied to economic models and theories. Concurrently, the Expected utility hypothesis and discounted utility models began to gain acceptance. In challenging the accuracy of generic utility, these concepts established a practice foundational in behavioral economics: Building on standard models by applying psychological knowledge.
Mathematical psychology reflects a long-standing interest in preference transitivity and the measurement of utility.

In 2017, Niels Geiger, a lecturer in economics at the University of Hohenheim conducted an investigation into the proliferation of behavioral economics. Geiger's research looked at studies that had quantified the frequency of references to terms specific to behavioral economics, and how often influential papers in behavioral economics were cited in journals on economics. The quantitative study found that there was a significant spread in behavioral economics after Kahneman and Tversky's work in the 1990s and into the 2000s.

Bounded rationality is the idea that when individuals make decisions, their rationality is limited by the tractability of the decision problem, their cognitive limitations and the time available.
Herbert A. Simon proposed bounded rationality as an alternative basis for the mathematical modeling of decision-making. It complements "rationality as optimization", which views decision-making as a fully rational process of finding an optimal choice given the information available. Simon used the analogy of a pair of scissors, where one blade represents human cognitive limitations and the other the "structures of the environment", illustrating how minds compensate for limited resources by exploiting known structural regularity in the environment. Bounded rationality implicates the idea that humans take shortcuts that may lead to suboptimal decision-making. Behavioral economists engage in mapping the decision shortcuts that agents use in order to help increase the effectiveness of human decision-making. Bounded rationality finds that actors do not assess all available options appropriately, in order to save on search and deliberation costs. As such decisions are not always made in the sense of greatest self-reward as limited information is available. Instead agents shall choose to settle for an acceptable solution. One approach, adopted by Richard M. Cyert and James G. March in their 1963  book A Behavioral Theory of the Firm, was to view firms as coalitions of groups whose targets were based on satisficing rather than optimizing behaviour.  Another treatment of this idea comes from Cass Sunstein and Richard Thaler's Nudge. Sunstein and Thaler recommend that choice architectures are modified in light of human agents' bounded rationality. A widely cited proposal from Sunstein and Thaler urges that healthier food be placed at sight level in order to increase the likelihood that a person will opt for that choice instead of less healthy option. Some critics of Nudge have lodged attacks that modifying choice architectures will lead to people becoming worse decision-makers.

In 1979, Kahneman and Tversky published Prospect Theory: An Analysis of Decision Under Risk, that used cognitive psychology to explain various divergences of economic decision making from neo-classical theory. Kahneman and Tversky utilising prospect theory determined three generalisations; gains are treated differently than losses, outcomes received with certainty are overweighed relative to uncertain outcomes and the structure of the problem may affect choices. These arguments were supported in part by altering a survey question so that it was no longer a case of achieving gains but averting losses and the majority of respondents altered their answers accordingly. In essence proving that emotions such as fear of loss, or greed can alter decisions, indicating the presence of an irrational decision-making process. Prospect theory has two stages: an editing stage and an evaluation stage. In the editing stage, risky situations are simplified using various heuristics. In the evaluation phase, risky alternatives are evaluated using various psychological principles that include:
Reference dependence: When evaluating outcomes, the decision maker considers a "reference level". Outcomes are then compared to the reference point and classified as "gains" if greater than the reference point and "losses" if less than the reference point.
Loss aversion: Losses are avoided more than equivalent gains are sought. In their 1992 paper, Kahneman and Tversky found the median coefficient of loss aversion to be about 2.25, i.e., losses hurt about 2.25 times more than equivalent gains reward.
Non-linear probability weighting: Decision makers overweigh small probabilities and underweigh large probabilities—this gives rise to the inverse-S shaped "probability weighting function".
Diminishing sensitivity to gains and losses: As the size of the gains and losses relative to the reference point increase in absolute value, the marginal effect on the decision maker's utility or satisfaction falls.
In 1992, in the Journal of Risk and Uncertainty, Kahneman and Tversky gave a revised account of prospect theory that they called cumulative prospect theory. The new theory eliminated the editing phase in prospect theory and focused just on the evaluation phase. Its main feature was that it allowed for non-linear probability weighting in a cumulative manner, which was originally suggested in John Quiggin's rank-dependent utility theory. Psychological traits such as overconfidence, projection bias and the effects of limited attention are now part of the theory. Other developments include a conference at the University of Chicago, a special behavioral economics edition of the Quarterly Journal of Economics ("In Memory of Amos Tversky"), and Kahneman's 2002 Nobel Prize for having "integrated insights from psychological research into economic science, especially concerning human judgment and decision-making under uncertainty."
A further argument of Behavioural Economics relates to the impact of the individual's cognitive limitations as a factor in limiting the rationality of people's decisions. Sloan first argued this in his paper 'Bounded Rationality' where he stated that our cognitive limitations are somewhat the consequence of our limited ability to foresee the future, hampering the rationality of decision. Daniel Kahneman further expanded upon the effect cognitive ability and processes have on decision making in his book Thinking, Fast and Slow. He delved into two forms of thought: fast thinking, which he considered "operates automatically and quickly, with little or no effort and no sense of voluntary control", and slow thinking, the allocation of cognitive ability, choice, and concentration. Fast thinking utilises heuristics, or shortcuts, which provide an immediate but often irrational and imperfect solution. He proposed that hindsight bias, confirmation bias, and outcome bias (among others) originate from such shortcuts. A key example of fast thinking and the resultant irrational decisions is the 2008 financial crisis.

Nudge is a concept in behavioral science, political theory and economics which proposes designs or changes in decision environments as ways to influence the behavior and decision making of groups or individuals—in other words, it's "a way to manipulate people's choices to lead them to make specific decisions".
The first formulation of the term and associated principles was developed in cybernetics by James Wilk before 1995 and described by Brunel University academic D. J. Stewart as "the art of the nudge" (sometimes referred to as micronudges). It also drew on methodological influences from clinical psychotherapy tracing back to Gregory Bateson, including contributions from Milton Erickson, Watzlawick, Weakland and Fisch, and Bill O'Hanlon. In this variant, the nudge is a microtargeted design geared towards a specific group of people, irrespective of the scale of intended intervention.
In 2008, Richard Thaler and Cass Sunstein's book Nudge: Improving Decisions About Health, Wealth, and Happiness brought nudge theory to prominence. It also gained a following among US and UK politicians, in the private sector and in public health. The authors refer to influencing behavior without coercion as libertarian paternalism and the influencers as choice architects. Thaler and Sunstein defined their concept as:
A nudge, as we will use the term, is any aspect of the choice architecture that alters people's behavior in a predictable way without forbidding any options or significantly changing their economic incentives. To count as a mere nudge, the intervention must be easy and cheap to avoid. Nudges are not mandates. Putting fruit at eye level counts as a nudge. Banning junk food does not.
Nudging techniques aim to capitalise on the judgemental heuristics of people. In other words, a nudge alters the environment so that when heuristic, or System 1, decision-making is used, the resulting choice will be the most positive or desired outcome. An example of such a nudge is switching the placement of junk food in a store, so that fruit and other healthy options are located next to the cash register, while junk food is relocated to another part of the store.
In 2008, the United States appointed Sunstein, who helped develop the theory, as administrator of the Office of Information and Regulatory Affairs.
Notable applications of nudge theory include the formation of the British Behavioural Insights Team in 2010. It is often called the "Nudge Unit", at the British Cabinet Office, headed by David Halpern. In addition, the Penn Medicine Nudge Unit is the world's first behavioral design team embedded within a health system.
Nudge theory has also been applied to business management and corporate culture, such as in relation to health, safety and environment (HSE) and human resources. Regarding its application to HSE, one of the primary goals of nudge is to achieve a "zero accident culture".
In contrast to nudges, which aim to guide people toward better decisions, Richard Thaler and Cass Sunstein also introduced the concept of sludge, which refers to unnecessary frictions or barriers that make it harder for individuals to take beneficial actions—such as confusing forms or hidden fees.

Cass Sunstein has responded to critiques at length in his The Ethics of Influence making the case in favor of nudging against charges that nudges diminish autonomy, threaten dignity, violate liberties, or reduce welfare. Ethicists have debated this rigorously. These charges have been made by various participants in the debate from Bovens to Goodwin. Wilkinson for example charges nudges for being manipulative, while others such as Yeung question their scientific credibility.
Behavioral economists such as Bob Sugden have pointed out that the underlying normative benchmark of nudging is still homo economicus, despite the proponents' claim to the contrary.
Recent scholarship has raised concerns about the use of vibrational nudges in digital consumer environments, suggesting that haptic feedback—such as subtle mobile phone vibrations—can increase purchasing behavior without conscious awareness. The study questions whether such subliminal tactics cross ethical boundaries by manipulating impulse control in ways that blur the line between persuasion and coercion.
It has been remarked that nudging is also a euphemism for psychological manipulation as practiced in social engineering.
There exists an anticipation and, simultaneously, implicit criticism of the nudge theory in works of Hungarian social psychologists who emphasize the active participation in the nudge of its target (Ferenc Merei and Laszlo Garai).

Behavioral economics aims to improve or overhaul traditional economic theory by studying failures in its assumptions that people are rational and selfish. Specifically, it studies the biases, tendencies and heuristics of people's economic decisions. It aids in determining whether people make good choices and whether they could be helped to make better choices. It can be applied both before and after a decision is made.

Behavioral economics proposes search heuristics as an aid for evaluating options. It is motivated by the fact that it is costly to gain information about options and it aims to maximise the utility of searching for information. While each heuristic is not wholistic in its explanation of the search process alone, a combination of these heuristics may be used in the decision-making process. There are three primary search heuristics.
Satisficing
Satisficing is the idea that there is some minimum requirement from the search and once that has been met, stop searching. After satisficing, a person may not have the most optimal option (i.e. the one with the highest utility), but would have a "good enough" one. This heuristic may be problematic if the aspiration level is set at such a level that no products exist that could meet the requirements.
Directed cognition
Directed cognition is a search heuristic in which a person treats each opportunity to research information as their last. Rather than a contingent plan that indicates what will be done based on the results of each search, directed cognition considers only if one more search should be conducted and what alternative should be researched.
Elimination by aspects
Whereas satisficing and directed cognition compare choices, elimination by aspects compares certain qualities. A person using the elimination by aspects heuristic first chooses the quality that they value most in what they are searching for and sets an aspiration level. This may be repeated to refine the search. i.e. identify the second most valued quality and set an aspiration level. Using this heuristic, options will be eliminated as they fail to meet the minimum requirements of the chosen qualities.

Besides searching, behavioral economists and psychologists have identified other heuristics and other cognitive effects that affect people's decision making. These include:
Mental accounting
Mental accounting refers to the propensity to allocate resources for specific purposes. Mental accounting is a behavioral bias that causes one to separate money into different categories known as mental accounts either based on the source or the intention of the money.
Anchoring
Anchoring describes when people have a mental reference point with which they compare results to. For example, a person who anticipates that the weather on a particular day would be raining, but finds that on the day it is actually clear blue skies, would gain more utility from the pleasant weather because they anticipated that it would be bad.
Herd behavior
This is a relatively simple bias that reflects the tendency of people to mimic what everyone else is doing and follow the general consensus.
Framing effects
People tend to choose differently depending on how the options are presented to them. People tend to have little control over their susceptibility to the framing effect, as often their choice-making process is based on intuition.

While heuristics are tactics or mental shortcuts to aid in the decision-making process, people are also affected by a number of biases and fallacies. Behavioral economics identifies a number of these biases that negatively affect decision making such as:
Present bias
Present bias reflects the human tendency to want rewards sooner. It describes people who are more likely to forego a greater payoff in the future in favour of receiving a smaller benefit sooner. An example of this is a smoker who is trying to quit. Although they know that in the future they will suffer health consequences, the immediate gain from the nicotine hit is more favourable to a person affected by present bias. Present bias is commonly split into people who are aware of their present bias (sophisticated) and those who are not (naive).
Gambler's fallacy
The gambler's fallacy stems from law of small numbers. It is the belief that an event that has occurred often in the past is less likely to occur in the future, despite the probability remaining constant. For example, if a coin had been flipped three times and turned up heads every single time, a person influenced by the gambler's fallacy would predict that the next one ought to be tails because of the abnormal number of heads flipped in the past, even though the probability of a heads occurring is still 50%.
Hot hand fallacy
The hot hand fallacy is the opposite of the gambler's fallacy. It is the belief that an event that has occurred often in the past is more likely to occur again in the future such that the streak will continue. This fallacy is particularly common within sports. For example, if a football team has consistently won the last few games they have participated in, then it is often said that they are 'on form' and thus, it is expected that the football team will maintain their winning streak.
Narrative fallacy
Narrative fallacy refers to when people use narratives to connect the dots between random events to make sense of arbitrary information. The term stems from Nassim Taleb's book The Black Swan: The Impact of the Highly Improbable. The narrative fallacy can be problematic as it can lead to individuals making false cause-effect relationships between events. For example, a startup may get funding because investors are swayed by a narrative that sounds plausible, rather than by a more reasoned analysis of available evidence.
Loss aversion
Loss aversion refers to the tendency to place greater weight on losses compared to equivalent gains. In other words, this means that when an individual receives a loss, this will cause their utility to decline more so than the same-sized gain. This means that they are far more likely to try to assign a higher priority on avoiding losses than making investment gains. As a result, some investors might want a higher payout to compensate for losses. If the high payout is not likely, they might try to avoid losses altogether even if the investment's risk is acceptable from a rational standpoint.
Recency bias
Recency bias is the belief that a particular outcome is more probable simply because it had just occurred. For example, if the previous one or two flips were heads, a person affected by recency bias would continue to predict that heads would be flipped.
Confirmation bias
Confirmation bias is the tendency to prefer information consistent with one's beliefs and discount evidence inconsistent with them.
Familiarity bias
Familiarity bias simply describes the tendency of people to return to what they know and are comfortable with. Familiarity bias discourages affected people from exploring new options and may limit their ability to find an optimal solution.
Status quo bias
Status quo bias describes the tendency of people to keep things as they are. It is a particular aversion to change in favor of remaining comfortable with what is known.
Connected to this concept is the endowment effect, a theory that people value things more if they own them - they require more to give up an object than they would be willing to pay to acquire it.

Behavioral finance is the study of the influence of psychology on the behavior of investors or financial analysts. It assumes that investors are not always rational, have limits to their self-control and are influenced by their own biases. For example, behavioral law and economics scholars studying the growth of financial firms' technological capabilities have attributed decision science to irrational consumer decisions. It also includes the subsequent effects on the markets. Behavioral Finance attempts to explain the reasoning patterns of investors and measures the influential power of these patterns on the investor's decision making. The central issue in behavioral finance is explaining why market participants make irrational systematic errors contrary to assumption of rational market participants. Such errors affect prices and returns, creating market inefficiencies.

Behavioral game theory, invented by Colin Camerer, analyzes interactive strategic decisions and behavior using the methods of game theory, experimental economics, and experimental psychology. Experiments include testing deviations from typical simplifications of economic theory such as the independence axiom and neglect of altruism, fairness, and framing effects. On the positive side, the method has been applied to interactive learning and social preferences. As a research program, the subject is a development of the last three decades.

Much of the decisions are more and more made either by human beings with the assistance of artificial intelligent machines or wholly made by these machines. Tshilidzi Marwala and Evan Hurwitz in their book, studied the utility of behavioral economics in such situations and concluded that these intelligent machines reduce the impact of bounded rational decision making. In particular, they observed that these intelligent machines reduce the degree of information asymmetry in the market, improve decision making and thus making markets more rational.
The use of AI machines in the market in applications such as online trading and decision making has changed major economic theories. Other theories where AI has had impact include in rational choice, rational expectations, game theory, Lewis turning point, portfolio optimization and counterfactual thinking.

Other branches of behavioral economics enrich the model of the utility function without implying inconsistency in preferences. Ernst Fehr, Armin Falk, and Rabin studied fairness, inequity aversion and reciprocal altruism, weakening the neoclassical assumption of perfect selfishness. This work is particularly applicable to wage setting. The work on "intrinsic motivation by Uri Gneezy and Aldo Rustichini and "identity" by George Akerlof and Rachel Kranton assumes that agents derive utility from adopting personal and social norms in addition to conditional expected utility. According to Aggarwal, in addition to behavioral deviations from rational equilibrium, markets are also likely to suffer from lagged responses, search costs, externalities of the commons, and other frictions making it difficult to disentangle behavioral effects in market behavior.
"Conditional expected utility" is a form of reasoning where the individual has an illusion of control, and calculates the probabilities of external events and hence their utility as a function of their own action, even when they have no causal ability to affect those external events.
Behavioral economics caught on among the general public with the success of books such as Dan Ariely's Predictably Irrational. Practitioners of the discipline have studied quasi-public policy topics such as broadband mapping. Behavioral economics has also been applied to public health policies, including front-of-pack nutrition labeling, sugar-sweetened beverage taxes, and advertising restrictions for unhealthy foods. These measures use behavioral insights to reshape consumer decisions without eliminating choice.
Applications for behavioral economics include the modeling of the consumer decision-making process for applications in artificial intelligence and machine learning. The Silicon Valley–based start-up Singularities is using the AGM postulates proposed by Alchourrón, Gärdenfors, and Makinson—the formalization of the concepts of beliefs and change for rational entities—in a symbolic logic to create a "machine learning and deduction engine that uses the latest data science and big data algorithms in order to generate the content and conditional rules (counterfactuals) that capture customer's behaviors and beliefs."
Applications of behavioral economics also exist in other disciplines, for example in the area of supply chain management.

In 1978 Herbert Simon was awarded the Nobel Memorial Prize in Economic Sciences "for his pioneering research into the decision-making process within economic organizations". Simon earned his Bachelor of Arts and his Ph.D. in Political Science from the University of Chicago before going on to teach at Carnegie Tech. Herbert was praised for his work on bounded rationality, a challenge to the assumption that humans are rational actors.

In 2002, psychologist Daniel Kahneman and economist Vernon L. Smith were awarded the Nobel Memorial Prize in Economic Sciences. Kahneman was awarded the prize "for having integrated insights from psychological research into economic science, especially concerning human judgment and decision-making under uncertainty", while Smith was awarded the prize "for having established laboratory experiments as a tool in empirical economic analysis, especially in the study of alternative market mechanisms."

In 2017, economist Richard Thaler was awarded the Nobel Memorial Prize in Economic Sciences for "his contributions to behavioral economics and his pioneering work in establishing that people are predictably irrational in ways that defy economic theory." Thaler was especially recognized for presenting inconsistencies in standard Economic theory and for his formulation of mental accounting and Libertarian paternalism

The work of Andrei Shleifer focused on behavioral finance and made observations on the limits of the efficient market hypothesis. Shleifer received the 1999 John Bates Clark Medal from the American Economic Association for his work.

Matthew Rabin received the "genius" award from the MarArthur Foundation in 2000. The American Economic Association chose Rabin as the recipient of the 2001 John Bates Clark medal. Rabin's awards were given to him primarily on the basis of his work on fairness and reciprocity, and on present bias.

Sendhil Mullainathan was the youngest of the chosen MacArthur Fellows in 2002, receiving a fellowship grant of $500,000 in 2003. Mullainathan was praised by the MacArthur Foundation as working on economics and psychology as an aggregate. Mullainathan's research focused on the salaries of executives on Wall Street; he also has looked at the implications of racial discrimination in markets in the United States.

Taken together, two landmark papers in economic theory which were published before the field of Behavioral Economics emerged, provide a justification for standard neoclassical economic analysis. The first is the paper "Uncertainty, Evolution, and Economic Theory" by Armen Alchian from 1950 and the second is the paper "Irrational Behavior and Economic Theory" from 1962 by Gary Becker, both of which were published in the Journal of Political Economy. Alchian's 1950 paper uses the logic of natural selection, the Evolutionary Landscape model, stochastic processes, probability theory, and several other lines of reasoning to justify many of the results derived from standard supply analysis assuming firms which maximizing their profits, are certain about the future, and have accurate foresight without having to assume any of those things. Becker's 1962 paper shows that downward sloping market demand curves (the most important implication of the law of demand) do not actually require an assumption that the consumers in that market are rational, as is claimed by behavioral economists and they also follow from a wide variety of irrational behavior as well.
The lines of reasoning and argumentation used in these two papers is re-expressed and expanded upon in (at least) one other professional economic publication for each of them. As for Alchian's evolutionary economics via natural selection by way of environmental adoption thesis, it is summarized, followed by an explicit exploration of its theoretical implications for Behavioral Economic theory, then illustrated via examples in several different industries including banking, hospitality, and transportation, in the 2014 paper "Uncertainty, Evolution, and Behavioral Economic Theory," by Manne and Zywicki. And the argument made in Becker's 1962 paper, that a 'pure' increase in the (relative) price (or terms of trade) of good X must reduce the amount of X demanded in the market for good X, is explained in greater detail in chapters (or as he calls them, "Lectures" because this textbook is more or less a transcription of his lectures given in his Price Theory course taught to 1st year PhD students several years earlier) 4 (called The Opportunity Set) and 5 (called Substitution Effects) of Gary Becker's graduate level textbook Economic Theory, originally published in 1971.
Besides the three critical aforementioned articles, critics of behavioral economics typically stress the rationality of economic agents. A fundamental critique is provided by Maialeh (2019) who argues that no behavioral research can establish an economic theory. Examples provided on this account include pillars of behavioral economics such as satisficing behavior or prospect theory, which are confronted from the neoclassical perspective of utility maximization and expected utility theory respectively. The author shows that behavioral findings are hardly generalizable and that they do not disprove typical mainstream axioms related to rational behavior.
Others, such as the essayist and former trader Nassim Taleb note that cognitive theories, such as prospect theory, are models of decision-making, not generalized economic behavior, and are only applicable to the sort of once-off decision problems presented to experiment participants or survey respondents. It is noteworthy that in the episode of EconTalk in which Taleb said this, he and the host, Russ Roberts discuss the significance of Gary Becker's 1962 paper cited in the first paragraph in this section as an argument against any implications which can be drawn from one shot psychological experiments on market level outcomes outside of laboratory settings, i.e. in the real world. Others argue that decision-making models, such as the endowment effect theory, that have been widely accepted by behavioral economists may be erroneously established as a consequence of poor experimental design practices that do not adequately control subject misconceptions.
Despite a great deal of rhetoric, no unified behavioral theory has yet been espoused: behavioral economists have proposed no alternative unified theory of their own to replace neoclassical economics with.
David Gal has argued that many of these issues stem from behavioral economics being too concerned with understanding how behavior deviates from standard economic models rather than with understanding why people behave the way they do. Understanding why behavior occurs is necessary for the creation of generalizable knowledge, the goal of science. He has referred to behavioral economics as a "triumph of marketing" and particularly cited the example of loss aversion. David Gal also has argued that loss aversion may not be as robust or universal as often claimed. He suggests that alternative explanations, such as psychological inertia, better explain certain behaviors like the endowment effect and status quo bias.
Traditional economists are skeptical of the experimental and survey-based techniques that behavioral economics uses extensively. Economists typically stress revealed preferences over stated preferences (from surveys) in the determination of economic value. Experiments and surveys are at risk of systemic biases, strategic behavior and lack of incentive compatibility. Some researchers point out that participants of experiments conducted by behavioral economists are not representative enough and drawing broad conclusions on the basis of such experiments is not possible. An acronym WEIRD has been coined in order to describe the studies participants—as those who come from Western, Educated, Industrialized, Rich, and Democratic societies. Critics have also highlighted that much of behavioral economics research relies on WEIRD  populations, raising questions about the generalizability of findings to other cultures and socioeconomic groups.

Matthew Rabin dismisses these criticisms, countering that consistent results typically are obtained in multiple situations and geographies and can produce good theoretical insight. Behavioral economists, however, responded to these criticisms by focusing on field studies rather than lab experiments. Some economists see a fundamental schism between experimental economics and behavioral economics, but prominent behavioral and experimental economists tend to share techniques and approaches in answering common questions. For example, behavioral economists are investigating neuroeconomics, which is entirely experimental and has not been verified in the field.
The epistemological, ontological, and methodological components of behavioral economics are increasingly debated, in particular by historians of economics and economic methodologists.
According to some researchers, when studying the mechanisms that form the basis of decision-making, especially financial decision-making, it is necessary to recognize that most decisions are made under stress because, "Stress is the nonspecific body response to any demands presented to it."

A widely noted concern is the replicability crisis; many behavioral economics studies fail to reproduce when repeated. A 2024 WSJ review reports that fewer than half of nudging-based experiments sustain long-term impact when re-tested, bringing into question their external validity in complex, real-world conditions. Supporting this, behavioral economics scholars have acknowledged that "small effect sizes and limited sample diversity" often hinder the generalizability of results.

Experimental economics is the application of experimental methods, including statistical, econometric, and computational, to study economic questions. Data collected in experiments are used to estimate effect size, test the validity of economic theories, and illuminate market mechanisms. Economic experiments usually use cash to motivate subjects, in order to mimic real-world incentives. Experiments are used to help understand how and why markets and other exchange systems function as they do. Experimental economics have also expanded to understand institutions and the law (experimental law and economics).
A fundamental aspect of the subject is design of experiments. Experiments may be conducted in the field or in laboratory settings, whether of individual or group behavior.
Variants of the subject outside such formal confines include natural and quasi-natural experiments.

Neuroeconomics is an interdisciplinary field that seeks to explain human decision making, the ability to process multiple alternatives and to follow a course of action. It studies how economic behavior can shape our understanding of the brain, and how neuroscientific discoveries can constrain and guide models of economics. It combines research methods from neuroscience, experimental and behavioral economics, and cognitive and social psychology. As research into decision-making behavior becomes increasingly computational, it has also incorporated new approaches from theoretical biology, computer science, and mathematics.
Neuroeconomics studies decision making by using a combination of tools from these fields so as to avoid the shortcomings that arise from a single-perspective approach. In mainstream economics, expected utility (EU) and the concept of rational agents are still being used. Many economic behaviors are not fully explained by these models, such as heuristics and framing. Behavioral economics emerged to account for these anomalies by integrating social, cognitive, and emotional factors in understanding economic decisions. Neuroeconomics adds another layer by using neuroscientific methods in understanding the interplay between economic behavior and neural mechanisms. By using tools from various fields, some scholars claim that neuroeconomics offers a more integrative way of understanding decision making.

An evolutionary psychology perspective states that many of the perceived limitations in rational choice can be explained as being rational in the context of maximizing biological fitness in the ancestral environment, but not necessarily in the current one. Thus, when living at subsistence level where a reduction of resources may result in death, it may have been rational to place a greater value on preventing losses than on obtaining gains. It may also explain behavioral differences between groups, such as males being less risk-averse than females since males have more variable reproductive success than females. While unsuccessful risk-seeking may limit reproductive success for both sexes, males may potentially increase their reproductive success from successful risk-seeking much more than females can.

Behavioral development economics brings together the disciplines of behavioral economics and development economics to study economic decision-making in low-income countries under the influence of cognitive biases and social norms. This is important because while non-standard economic behavior exists globally, it is more pronounced in the Global South due to interactions with higher poverty rates, financial instability, informal labor markets and weaker formal institutions. The understanding of these regional differences is vital for effective policy-making in the Global South.

Recent research in behavioral development economics highlights how the experience of poverty itself can impair cognitive function and economic decision-making, potentially perpetuating poverty through behavioral channels. Scarcity captures attention, narrowing focus on immediate financial concerns while reducing mental “bandwidth” for other tasks. While this focused attention may improve decision-making in some contexts—such as heightened price awareness —the overall effect of constant financial stress is detrimental. A study provides experimental evidence of this cognitive burden: when exposed to scenarios involving large financial stakes, low-income individuals in the U.S. exhibited reduced cognitive performance, while high-income individuals did not. Complementary field evidence from Indian sugarcane farmers showed significantly lower cognitive performance before harvest (when finances are tight) compared to post-harvest, suggesting that poverty-induced cognitive load, rather than fixed individual traits, drives decision-making quality.

Financial inclusion remains low in the Global South despite several attempts to improve the situation through financial literacy campaigns. This is unlikely to suffice in low-income setting because of behavioral barriers. Trust in financial institutions is much lower in the Global South as a result of historical experiences with inflation, bank failures and corruption. Accordingly, economic agents exhibit stronger status quo bias relying mostly on informal financial institutions through their social networks such as rotating savings and credit associations (ROSCAs). 
Accordingly, interventions that aimed at strengthening these informal institutions such as mobile money platforms like M-Pesa in Kenya and village savings and loans associations (VSLAs) have performed much better than financial literacy programs. These interventions leveraged the existing trust structures and social networks to encourage adoption.

Behavioral challenges in health decision-making also differ in developing contexts. While procrastination in preventive health care is universal, it is exacerbated in low-income countries where immediate financial costs often outweigh perceived future benefits. For example, while vaccine hesitancy exists globally, in the Global South, uptake is also hindered by limited access, misinformation, and lack of trust in public health institutions. Compared to developed countries, where reminder systems or behavioral nudges can improve compliance, in the Global South, additional interventions, such as financial incentives or subsidized transportation, are often necessary to achieve significant behavioral change.
Historical experiences with colonial medicine have also created persistent mistrust in the health sector. A study shows that regions in Central Africa that were more exposed to cruel French colonial medical campaigns between 1921 and 1956 exhibit today lower vaccination rates and willingness to undertake non-invasive blood tests. Moreover, World Bank health projects in those regions have shown lower success rates. It is important here to note that within the same culture, different behavioral barriers exist because of different historical experiences.

Labor markets differ substantially in the Global South compared to high-income countries because of higher rates of informal employment, self-employment and casual labor. For instance, an experiment that randomly assigned workers to industrial jobs in Ethiopia found that workers quickly quit or move to different sectors. This low preference for full-time jobs is driven by unpredictable demands on one's time where workers prefer flexibility to be able to meet family demands. On the other hand, US workers show little valuation of work hours flexibility. Given less formal contracts and fixed schedules, workers are more prone to behavioral biases. Self-control problems are especially relevant in labor markets. For instance, many low-income workers in India are inebriated on the job because they are self-employed. Without external monitoring or standardized hours, self-employed individuals must rely on internal discipline, which is often limited. Evidence from Indian data-entry workers shows that many voluntarily opt into contracts that penalize them for underperformance—known as dominated contracts—in order to commit themselves to working harder. This suggests that workers are aware of their own time-inconsistency and use commitment mechanisms to increase productivity. Additionally, many workers engage in income-targeting behavior, adjusting labor supply based on daily financial needs rather than wage levels. For example, bicycle-taxi drivers in Kenya work longer when they have specific cash needs but do not reduce labor supply when given cash in the morning, indicating reference-dependent preferences focused on earned income.
Behavioral frictions also extend to the workplace environment. While noise and heat are common in urban areas of the Global South, workers often underestimate how these factors affect their productivity, reflecting bounded rationality. In Kenya, exposure to moderate increases in noise significantly reduced textile output, yet workers were unaware of the impact and unwilling to pay for quieter conditions even when earnings depended on output. Similar results are found for heat exposure in Indian factories, where temperature reductions improved output but were only implemented to save energy, not to boost productivity. These findings show that misperceptions about environmental effects on work performance can lead to suboptimal decisions by both workers and firms.
Wage-setting also reflects behavioral elements. In informal labor markets, wage rigidity persists despite the absence of formal institutions like unions. In Indian agriculture, wages rise with positive productivity shocks but are rarely reduced when shocks are negative, due to fairness concerns and social norms. Similarly, low-wage offers are accepted more often when made privately rather than publicly, as workers face social pressure to reject “unfair” wages in public settings. Furthermore, even small wage differences within work teams can reduce attendance and productivity, not only among lower-paid workers but also among those paid more, indicating that inequality itself demotivates workers through social comparison.
Behavioral constraints are also relevant for female labor-force participation (FLFP), which remains low in many developing countries. Psychological factors such as low self-efficacy and misperceptions about social norms contribute to this. In India, an intervention to raise self-efficacy significantly increased women's employment, while in Saudi Arabia, correcting men's beliefs about their peers' support for women working outside the home led to greater spousal support for job-seeking. These findings show how behavioral interventions can shift labor supply decisions, particularly for marginalized groups.

Confirmation bias (also confirmatory bias, myside bias, or congeniality bias) is the tendency to search for, interpret, favor and recall information in a way that confirms or supports one's prior beliefs or values. People display this bias when they select information that supports their views, ignoring contrary information or when they interpret ambiguous evidence as supporting their existing attitudes. The effect is strongest for desired outcomes, for emotionally charged issues, and for deeply entrenched beliefs.
Biased search for information, biased interpretation of this information, and biased memory recall have been invoked to explain four specific effects:
attitude polarization (when a disagreement becomes more extreme even though the different parties are exposed to the same evidence)
belief perseverance (when beliefs persist after the evidence for them is shown to be false)
the irrational primacy effect (a greater reliance on information encountered early in a series)
illusory correlation (when people falsely perceive an association between two events or situations).
A series of psychological experiments in the 1960s suggested that people are biased toward confirming their existing beliefs. Later work re-interpreted these results as a tendency to test ideas in a one-sided way, focusing on one possibility and ignoring alternatives. Explanations for the observed biases include wishful thinking and the limited human capacity to process information. Another proposal is that people show confirmation bias because they are pragmatically assessing the costs of being wrong rather than investigating in a neutral, scientific way.
Flawed decisions due to confirmation bias have been found in a wide range of political, organizational, financial, and scientific contexts. These biases contribute to overconfidence in personal beliefs and can maintain or strengthen beliefs in the face of contrary evidence. For example, confirmation bias produces systematic errors in scientific research based on inductive reasoning (the gradual accumulation of supportive evidence). Similarly, a police detective may identify a suspect early in an investigation, but then may only seek confirming rather than disconfirming evidence. A medical practitioner may prematurely focus on a particular disorder early in a diagnostic session, and then seek only confirming evidence. In social media, confirmation bias is amplified by the use of filter bubbles and "algorithmic editing", which display to individuals only information they are likely to agree with, while excluding opposing views.

Confirmation bias, previously used as a "catch-all phrase", was refined by English psychologist Peter Wason, as "a preference for information that is consistent with a hypothesis rather than information which opposes it."
Confirmation biases are effects in information processing. They differ from what is sometimes called the behavioral confirmation effect, commonly known as self-fulfilling prophecy, in which a person's expectations influence their own behavior, bringing about the expected result.
Some psychologists restrict the term "confirmation bias" to selective collection of evidence that supports what one already believes while ignoring or rejecting evidence that supports a different conclusion. Others apply the term more broadly to the tendency to preserve one's existing beliefs when searching for evidence, interpreting it, or recalling it from memory. Confirmation bias is a result of automatic, unintentional strategies rather than deliberate deception.

Experiments have found repeatedly that people tend to test hypotheses in a one-sided way, by searching for evidence consistent with their current hypothesis. Rather than searching through all the relevant evidence, they phrase questions to receive an affirmative answer that supports their theory. They look for the consequences that they would expect if their hypothesis were true, rather than what would happen if it were false. For example, someone using yes/no questions to find a number they suspect to be the number 3 might ask, "Is it an odd number?" People prefer this type of question, called a "positive test", even when a negative test such as "Is it an even number?" would yield exactly the same information. However, this does not mean that people seek tests that guarantee a positive answer. In studies where subjects could select either such pseudo-tests or genuinely diagnostic ones, they favored the genuinely diagnostic.
The preference for positive tests in itself is not a bias, since positive tests can be highly informative. However, in combination with other effects, this strategy can confirm existing beliefs or assumptions, independently of whether they are true. In real-world situations, evidence is often complex and mixed. For example, various contradictory ideas about someone could each be supported by concentrating on one aspect of his or her behavior. Thus any search for evidence in favor of a hypothesis is likely to succeed. One illustration of this is the way the phrasing of a question can significantly change the answer. For example, people who are asked, "Are you happy with your social life?" report greater satisfaction than those asked, "Are you unhappy with your social life?"
Even a small change in a question's wording can affect how people search through available information, and hence the conclusions they reach. This was shown using a fictional child custody case. Participants read that Parent A was moderately suitable to be the guardian in multiple ways. Parent B had a mix of salient positive and negative qualities: a close relationship with the child but a job that would take them away for long periods of time. When asked, "Which parent should have custody of the child?" the majority of participants chose Parent B, looking mainly for positive attributes. However, when asked, "Which parent should be denied custody of the child?" they looked for negative attributes and the majority answered that Parent B should be denied custody, implying that Parent A should have custody.
Similar studies have demonstrated how people engage in a biased search for information, but also that this phenomenon may be limited by a preference for genuine diagnostic tests. In an initial experiment, participants rated another person on the introversion–extroversion personality dimension on the basis of an interview. They chose the interview questions from a given list. When the interviewee was introduced as an introvert, the participants chose questions that presumed introversion, such as, "What do you find unpleasant about noisy parties?" When the interviewee was described as extroverted, almost all the questions presumed extroversion, such as, "What would you do to liven up a dull party?" These loaded questions gave the interviewees little or no opportunity to falsify the hypothesis about them. A later version of the experiment gave the participants less presumptive questions to choose from, such as, "Do you shy away from social interactions?" Participants preferred to ask these more diagnostic questions, showing only a weak bias towards positive tests. This pattern, of a main preference for diagnostic tests and a weaker preference for positive tests, has been replicated in other studies.
Personality traits influence and interact with biased search processes. Individuals vary in their abilities to defend their attitudes from external attacks in relation to selective exposure. Selective exposure occurs when individuals search for information that is consistent, rather than inconsistent, with their personal beliefs. An experiment examined the extent to which individuals could refute arguments that contradicted their personal beliefs. People with high confidence levels more readily seek out contradictory information to their personal position to form an argument. This can take the form of an oppositional news consumption, where individuals seek opposing partisan news in order to counterargue. Individuals with low confidence levels do not seek out contradictory information and prefer information that supports their personal position. People generate and evaluate evidence in arguments that are biased towards their own beliefs and opinions. Heightened confidence levels decrease preference for information that supports individuals' personal beliefs.
Another experiment gave participants a complex rule-discovery task that involved moving objects simulated by a computer. Objects on the computer screen followed specific laws, which the participants had to figure out. So, participants could "fire" objects across the screen to test their hypotheses. Despite making many attempts over a ten-hour session, none of the participants figured out the rules of the system. They typically attempted to confirm rather than falsify their hypotheses, and were reluctant to consider alternatives. Even after seeing objective evidence that refuted their working hypotheses, they frequently continued doing the same tests. Some of the participants were taught proper hypothesis-testing, but these instructions had almost no effect.

Confirmation biases are not limited to the collection of evidence. Even if two individuals have the same information, the way they interpret it can be biased.
A team at Stanford University conducted an experiment involving participants who felt strongly about capital punishment, with half in favor and half against it. Each participant read descriptions of two studies: a comparison of U.S. states with and without the death penalty, and a comparison of murder rates in a state before and after the introduction of the death penalty. After reading a quick description of each study, the participants were asked whether their opinions had changed. Then, they read a more detailed account of each study's procedure and had to rate whether the research was well-conducted and convincing. In fact, the studies were fictional. Half the participants were told that one kind of study supported the deterrent effect and the other undermined it, while for other participants the conclusions were swapped.
The participants, whether supporters or opponents, reported shifting their attitudes slightly in the direction of the first study they read. Once they read the more detailed descriptions of the two studies, they almost all returned to their original belief regardless of the evidence provided, pointing to details that supported their viewpoint and disregarding anything contrary. Participants described studies supporting their pre-existing view as superior to those that contradicted it, in detailed and specific ways. Writing about a study that seemed to undermine the deterrence effect, a death penalty proponent wrote, "The research didn't cover a long enough period of time," while an opponent's comment on the same study said, "No strong evidence to contradict the researchers has been presented." The results illustrated that people set higher standards of evidence for hypotheses that go against their current expectations. This effect, known as "disconfirmation bias", has been supported by other experiments.
Another study of biased interpretation occurred during the 2004 U.S. presidential election and involved participants who reported having strong feelings about the candidates. They were shown apparently contradictory pairs of statements, either from Republican candidate George W. Bush, Democratic candidate John Kerry or a politically neutral public figure. They were also given further statements that made the apparent contradiction seem reasonable. From these three pieces of information, they had to decide whether each individual's statements were inconsistent. There were strong differences in these evaluations, with participants much more likely to interpret statements from the candidate they opposed as contradictory.
In this experiment, the participants made their judgments while in a magnetic resonance imaging (MRI) scanner which monitored their brain activity. As participants evaluated contradictory statements by their favored candidate, emotional centers of their brains were aroused. This did not happen with the statements by the other figures. The experimenters inferred that the different responses to the statements were not due to passive reasoning errors. Instead, the participants were actively reducing the cognitive dissonance induced by reading about their favored candidate's irrational or hypocritical behavior.
Biases in belief interpretation are persistent, regardless of intelligence level. Participants in an experiment took the SAT test (a college admissions test used in the United States) to assess their intelligence levels. They then read information regarding safety concerns for vehicles, and the experimenters manipulated the national origin of the car. American participants provided their opinion if the car should be banned on a six-point scale, where one indicated "definitely yes" and six indicated "definitely no". Participants firstly evaluated if they would allow a dangerous German car on American streets and a dangerous American car on German streets. Participants believed that the dangerous German car on American streets should be banned more quickly than the dangerous American car on German streets. There was no difference among intelligence levels at the rate participants would ban a car.
Biased interpretation is not restricted to emotionally significant topics. In another experiment, participants were told a story about a theft. They had to rate the evidential importance of statements arguing either for or against a particular character being responsible. When they hypothesized that character's guilt, they rated statements supporting that hypothesis as more important than conflicting statements.

People may remember evidence selectively to reinforce their expectations, even if they gather and interpret evidence in a neutral manner. This effect is called "selective recall", "confirmatory memory", or "access-biased memory". Psychological theories differ in their predictions about selective recall. Schema theory predicts that information matching prior expectations will be more easily stored and recalled than information that does not match. Some alternative approaches say that surprising information stands out and so is memorable. Predictions from both these theories have been confirmed in different experimental contexts, with no theory winning outright.
In one study, participants read a profile of a woman which described a mix of introverted and extroverted behaviors. They later had to recall examples of her introversion and extroversion. One group was told this was to assess the woman for a job as a librarian, while a second group were told it was for a job in real estate sales. There was a significant difference between what these two groups recalled, with the "librarian" group recalling more examples of introversion and the "sales" groups recalling more extroverted behavior. A selective memory effect has also been shown in experiments that manipulate the desirability of personality types. In one of these, a group of participants were shown evidence that extroverted people are more successful than introverts. Another group were told the opposite. In a subsequent, apparently unrelated study, participants were asked to recall events from their lives in which they had been either introverted or extroverted. Each group of participants provided more memories connecting themselves with the more desirable personality type, and recalled those memories more quickly.
Changes in emotional states can also influence memory recall. Participants rated how they felt when they had first learned that O. J. Simpson had been acquitted of murder charges. They described their emotional reactions and confidence regarding the verdict one week, two months, and one year after the trial. Results indicated that participants' assessments for Simpson's guilt changed over time. The more that participants' opinion of the verdict had changed, the less stable were the participant's memories regarding their initial emotional reactions. When participants recalled their initial emotional reactions two months and a year later, past appraisals closely resembled current appraisals of emotion. People demonstrate sizable myside bias when discussing their opinions on controversial topics. Memory recall and construction of experiences undergo revision in relation to corresponding emotional states.
Myside bias has been shown to influence the accuracy of memory recall. In an experiment, widows and widowers rated the intensity of their experienced grief six months and five years after the deaths of their spouses. Participants noted a higher experience of grief at six months rather than at five years. Yet, when the participants were asked after five years how they had felt six months after the death of their significant other, the intensity of grief participants recalled was highly correlated with their current level of grief. Individuals appear to utilize their current emotional states to analyze how they must have felt when experiencing past events. Emotional memories are reconstructed by current emotional states.
One study showed how selective memory can maintain belief in extrasensory perception (ESP). Believers and disbelievers were each shown descriptions of ESP experiments. Half of each group were told that the experimental results supported the existence of ESP, while the others were told they did not. In a subsequent test, participants recalled the material accurately, apart from believers who had read the non-supportive evidence. This group remembered significantly less information and some of them incorrectly remembered the results as supporting ESP.

Myside bias was once believed to be correlated with intelligence; however, studies have shown that myside bias can be more influenced by ability to rationally think as opposed to level of intelligence. Myside bias can cause an inability to effectively and logically evaluate the opposite side of an argument. Studies have stated that myside bias is an absence of "active open-mindedness", meaning the active search for why an initial idea may be wrong. Typically, myside bias is operationalized in empirical studies as the quantity of evidence used in support of their side in comparison to the opposite side.
A study has found individual differences in myside bias. This study investigates individual differences that are acquired through learning in a cultural context and are mutable. The researcher found important individual difference in argumentation. Studies have suggested that individual differences such as deductive reasoning ability, ability to overcome belief bias, epistemological understanding, and thinking disposition are significant predictors of the reasoning and generating arguments, counterarguments, and rebuttals.
A study by Christopher Wolfe and Anne Britt also investigated how participants' views of "what makes a good argument?" can be a source of myside bias that influences the way a person formulates their own arguments. The study investigated individual differences of argumentation schema and asked participants to write essays. The participants were randomly assigned to write essays either for or against their preferred side of an argument and were given research instructions that took either a balanced or an unrestricted approach. The balanced-research instructions directed participants to create a "balanced" argument, i.e., that included both pros and cons; the unrestricted-research instructions included nothing on how to create the argument.
Overall, the results revealed that the balanced-research instructions significantly increased the incidence of opposing information in arguments. These data also reveal that personal belief is not a source of myside bias; however, that those participants, who believe that a good argument is one that is based on facts, are more likely to exhibit myside bias than other participants. This evidence is consistent with the claims proposed in Baron's article—that people's opinions about what makes good thinking can influence how arguments are generated.

Before psychological research on confirmation bias, the phenomenon had been observed throughout history. Beginning with the Greek historian Thucydides (c. 460 BC – c. 395 BC), who wrote of misguided reason in The Peloponnesian War; "... for it is a habit of mankind to entrust to careless hope what they long for, and to use sovereign reason to thrust aside what they do not fancy". Italian poet Dante Alighieri (1265–1321) noted it in the Divine Comedy, in which St. Thomas Aquinas cautions Dante upon meeting in Paradise, "opinion—hasty—often can incline to the wrong side, and then affection for one's own opinion binds, confines the mind". Ibn Khaldun noticed the same effect in his Muqaddimah:
Untruth naturally afflicts historical information. There are various reasons that make this unavoidable. One of them is partisanship for opinions and schools. ... if the soul is infected with partisanship for a particular opinion or sect, it accepts without a moment's hesitation the information that is agreeable to it. Prejudice and partisanship obscure the critical faculty and preclude critical investigation. The result is that falsehoods are accepted and transmitted. In the Novum Organum, English philosopher and scientist Francis Bacon (1561–1626) noted that biased assessment of evidence drove "all superstitions, whether in astrology, dreams, omens, divine judgments or the like". He wrote:
The human understanding when it has once adopted an opinion ... draws all things else to support and agree with it. And though there be a greater number and weight of instances to be found on the other side, yet these it either neglects or despises, or else by some distinction sets aside or rejects[.]
In the second volume of his The World as Will and Representation (1844), German philosopher Arthur Schopenhauer observed that "An adopted hypothesis gives us lynx-eyes for everything that confirms it and makes us blind to everything that contradicts it."
In his essay (1897) What Is Art?, Russian novelist Leo Tolstoy wrote:
I know that most men—not only those considered clever, but even those who are very clever, and capable of understanding most difficult scientific, mathematical, or philosophic problems—can very seldom discern even the simplest and most obvious truth if it be such as to oblige them to admit the falsity of conclusions they have formed, perhaps with much difficulty—conclusions of which they are proud, which they have taught to others, and on which they have built their lives. In his essay (1894) The Kingdom of God Is Within You, Tolstoy had earlier written:
The most difficult subjects can be explained to the most slow-witted man if he has not formed any idea of them already; but the simplest thing cannot be made clear to the most intelligent man if he is firmly persuaded that he knows already, without a shadow of doubt, what is laid before him.

In Peter Wason's initial experiment published in 1960 (which does not mention the term "confirmation bias"), he repeatedly challenged participants to identify a rule applying to triples of numbers. They were told that (2,4,6) fits the rule. They generated triples, and the experimenter told them whether each triple conformed to the rule.
The actual rule was simply "any ascending sequence", but participants had great difficulty in finding it, often announcing rules that were far more specific, such as "the middle number is the average of the first and last". The participants seemed to test only positive examples—triples that obeyed their hypothesized rule. For example, if they thought the rule was, "Each number is two greater than its predecessor," they would offer a triple that fitted (confirmed) this rule, such as (11,13,15) rather than a triple that violated (falsified) it, such as (11,12,19).
Wason interpreted his results as showing a preference for confirmation over falsification, hence he coined the term "confirmation bias". Wason also used confirmation bias to explain the results of his selection task experiment. Participants repeatedly performed badly on various forms of this test, in most cases ignoring information that could potentially refute (falsify) the specified rule.

Klayman and Ha's 1987 paper argues that the Wason experiments do not actually demonstrate a bias towards confirmation, but instead a tendency to make tests consistent with the working hypothesis. They called this the "positive test strategy". This strategy is an example of a heuristic: a reasoning shortcut that is imperfect but easy to compute. Klayman and Ha used Bayesian probability and information theory as their standard of hypothesis-testing, rather than the falsificationism used by Wason. According to these ideas, each answer to a question yields a different amount of information, which depends on the person's prior beliefs. Thus a scientific test of a hypothesis is one that is expected to produce the most information. Since the information content depends on initial probabilities, a positive test can either be highly informative or uninformative. Klayman and Ha argued that when people think about realistic problems, they are looking for a specific answer with a small initial probability. In this case, positive tests are usually more informative than negative tests. However, in Wason's rule discovery task the answer—three numbers in ascending order—is very broad, so positive tests are unlikely to yield informative answers. Klayman and Ha supported their analysis by citing an experiment that used the labels "DAX" and "MED" in place of "fits the rule" and "doesn't fit the rule". This avoided implying that the aim was to find a low-probability rule. Participants had much more success with this version of the experiment.
In light of this and other critiques, the focus of research moved away from confirmation versus falsification of an hypothesis, to examining whether people test hypotheses in an informative way, or an uninformative but positive way. The search for "true" confirmation bias led psychologists to look at a wider range of effects in how people process information.

There are currently three main information processing explanations of confirmation bias, plus a recent addition.

According to Robert MacCoun, most biased evidence processing occurs through a combination of "cold" (cognitive) and "hot" (motivated) mechanisms.
Cognitive explanations for confirmation bias are based on limitations in people's ability to handle complex tasks, and the shortcuts, called heuristics, that they use. For example, people may judge the reliability of evidence by using the availability heuristic that is, how readily a particular idea comes to mind. It is also possible that people can only focus on one thought at a time, so find it difficult to test alternative hypotheses in parallel. Another heuristic is the positive test strategy identified by Klayman and Ha, in which people test a hypothesis by examining cases where they expect a property or event to occur. This heuristic avoids the difficult or impossible task of working out how diagnostic each possible question will be. However, it is not universally reliable, so people can overlook challenges to their existing beliefs.
Motivational explanations involve an effect of desire on belief. It is known that people prefer positive thoughts over negative ones in a number of ways: this is called the "Pollyanna principle". Applied to arguments or sources of evidence, this could explain why desired conclusions are more likely to be believed true. According to experiments that manipulate the desirability of the conclusion, people demand a high standard of evidence for unpalatable ideas and a low standard for preferred ideas. In other words, they ask, "Can I believe this?" for some suggestions and, "Must I believe this?" for others. Although consistency is a desirable feature of attitudes, an excessive drive for consistency is another potential source of bias because it may prevent people from neutrally evaluating new, surprising information. Social psychologist Ziva Kunda combines the cognitive and motivational theories, arguing that motivation creates the bias, but cognitive factors determine the size of the effect.

Explanations in terms of cost-benefit analysis assume that people do not just test hypotheses in a disinterested way, but assess the costs of different errors. Using ideas from evolutionary psychology, James Friedrich suggests that people do not primarily aim at truth in testing hypotheses, but try to avoid the most costly errors. For example, employers might ask one-sided questions in job interviews because they are focused on weeding out unsuitable candidates. Yaacov Trope and Akiva Liberman's refinement of this theory assumes that people compare the two different kinds of error: accepting a false hypothesis or rejecting a true hypothesis. For instance, someone who underestimates a friend's honesty might treat him or her suspiciously and so undermine the friendship. Overestimating the friend's honesty may also be costly, but less so. In this case, it would be rational to seek, evaluate or remember evidence of their honesty in a biased way. When someone gives an initial impression of being introverted or extroverted, questions that match that impression come across as more empathic. This suggests that when talking to someone who seems to be an introvert, it is a sign of better social skills to ask, "Do you feel awkward in social situations?" rather than, "Do you like noisy parties?" The connection between confirmation bias and social skills was corroborated by a study of how college students get to know other people. Highly self-monitoring students, who are more sensitive to their environment and to social norms, asked more matching questions when interviewing a high-status staff member than when getting to know fellow students.

Psychologists Jennifer Lerner and Philip Tetlock distinguish two different kinds of thinking process. Exploratory thought neutrally considers multiple points of view and tries to anticipate all possible objections to a particular position, while confirmatory thought seeks to justify a specific point of view. Lerner and Tetlock say that when people expect to justify their position to others whose views they already know, they will tend to adopt a similar position to those people, and then use confirmatory thought to bolster their own credibility. However, if the external parties are overly aggressive or critical, people will disengage from thought altogether, and simply assert their personal opinions without justification. Lerner and Tetlock say that people only push themselves to think critically and logically when they know in advance they will need to explain themselves to others who are well-informed, genuinely interested in the truth, and whose views they do not already know. Because those conditions rarely exist, they argue, most people are using confirmatory thought most of the time.

Developmental psychologist Eve Whitmore has argued that beliefs and biases involved in confirmation bias have their roots in childhood coping through make-believe, which becomes "the basis for more complex forms of self-deception and illusion into adulthood." The friction brought on by questioning as an adolescent with developing critical thinking can lead to the rationalization of false beliefs, and the habit of such rationalization can become unconscious over the years.

Recent research in economics has challenged the traditional view of confirmation bias as purely a cognitive flaw. Under conditions where acquiring and processing information is costly, seeking confirmatory evidence can actually be an optimal strategy. Instead of pursuing contrarian or disconfirming evidence, it may be more efficient to focus on sources likely to align with one's existing beliefs, given the constraints on time and resources.
Economist Weijie Zhong has developed a model demonstrating that individuals who must make decisions under time pressure, and who face costs for obtaining more information, will often prefer confirmatory signals. According to this model, when individuals believe strongly in a certain hypothesis, they optimally seek information that confirms it, allowing them to build confidence more efficiently. If the expected confirmatory signals are not received, their confidence in the initial hypothesis will gradually decline, leading to belief updating. This approach shows that seeking confirmation is not necessarily biased but may be a rational allocation of limited attention and resources.

In social media, confirmation bias is amplified by the use of filter bubbles and echo chambers (or "algorithmic editing"), which displays to individuals only information they are likely to agree with, while excluding opposing views. Some have argued that confirmation bias is the reason why society can never escape from filter bubbles, because individuals are psychologically hardwired to seek information that agrees with their preexisting values and beliefs. Others have further argued that the mixture of the two is degrading democracy—claiming that this "algorithmic editing" removes diverse viewpoints and information—and that unless filter bubble algorithms are removed, voters will be unable to make fully informed political decisions.
The rise of social media has contributed greatly to the rapid spread of fake news, that is, false and misleading information that is presented as credible news from a seemingly reliable source. Confirmation bias (selecting or reinterpreting evidence to support one's beliefs) is one of three main hurdles cited as to why critical thinking goes astray in these circumstances. The other two are shortcut heuristics (when overwhelmed or short of time, people rely on simple rules such as group consensus or trusting an expert or role model) and social goals (social motivation or peer pressure can interfere with objective analysis of facts at hand).
In combating the spread of fake news, social media sites have considered turning toward "digital nudging". This can currently be done in two different forms of nudging. This includes nudging of information and nudging of presentation. Nudging of information entails social media sites providing a disclaimer or label questioning or warning users of the validity of the source while nudging of presentation includes exposing users to new information which they may not have sought out but could introduce them to viewpoints that may combat their own confirmation biases.

A distinguishing feature of scientific thinking is the search for confirming or supportive evidence (inductive reasoning) as well as falsifying evidence (deductive reasoning).
Many times in the history of science, scientists have resisted new discoveries by selectively interpreting or ignoring unfavorable data. Several studies have shown that scientists rate studies that report findings consistent with their prior beliefs more favorably than studies reporting findings inconsistent with their previous beliefs.
However, assuming that the research question is relevant, the experimental design adequate and the data are clearly and comprehensively described, the empirical data obtained should be important to the scientific community and should not be viewed prejudicially, regardless of whether they conform to current theoretical predictions. In practice, researchers may misunderstand, misinterpret, or not read at all studies that contradict their preconceptions, or wrongly cite them anyway as if they actually supported their claims.
Further, confirmation biases can sustain scientific theories or research programs in the face of inadequate or even contradictory evidence. The discipline of parapsychology is often cited as an example.
An experimenter's confirmation bias can potentially affect which data are reported. Data that conflict with the experimenter's expectations may be more readily discarded as unreliable, producing the so-called file drawer effect. To combat this tendency, scientific training teaches ways to prevent bias. For example, experimental design of randomized controlled trials (coupled with their systematic review) aims to minimize sources of bias.
The social process of peer review aims to mitigate the effect of individual scientists' biases, even though the peer review process itself may be susceptible to such biases. Confirmation bias may thus be especially harmful to objective evaluations regarding nonconforming results since biased individuals may regard opposing evidence to be weak in principle and give little serious thought to revising their beliefs. Scientific innovators often meet with resistance from the scientific community, and research presenting controversial results frequently receives harsh peer review.

Confirmation bias can lead investors to be overconfident, ignoring evidence that their strategies will lose money. In studies of political stock markets, investors made more profit when they resisted bias. For example, participants who interpreted a candidate's debate performance in a neutral rather than partisan way were more likely to profit. To combat the effect of confirmation bias, investors can try to adopt a contrary viewpoint "for the sake of argument". In one technique, they imagine that their investments have collapsed and ask themselves why this might happen.

Cognitive biases are important variables in clinical decision-making by medical general practitioners (GPs) and medical specialists. Two important ones are confirmation bias and the overlapping availability bias. A GP may make a diagnosis early on during an examination, and then seek confirming evidence rather than falsifying evidence. This cognitive error is partly caused by the availability of evidence about the supposed disorder being diagnosed. For example, the client may have mentioned the disorder, or the GP may have recently read a much-discussed paper about the disorder. The basis of this cognitive shortcut or heuristic (termed anchoring) is that the doctor does not consider multiple possibilities based on evidence, but prematurely latches on (or anchors to) a single cause. In emergency medicine, because of time pressure, there is a high density of decision-making, and shortcuts are frequently applied. The potential failure rate of these cognitive decisions needs to be managed by education about the 30 or more cognitive biases that can occur, so as to set in place proper debiasing strategies. Confirmation bias may also cause doctors to perform unnecessary medical procedures due to pressure from adamant patients.
Mental disorders may be prone to misdiagnosis in being based upon observations and self-reporting rather than objective testing. Confirmation bias may play a role when practitioners stick with an early diagnosis.
Raymond Nickerson, a psychologist, blames confirmation bias for the ineffective medical procedures that were used for centuries before the arrival of scientific medicine. If a patient recovered, medical authorities counted the treatment as successful, rather than looking for alternative explanations such as that the disease had run its natural course. Biased assimilation is a factor in the modern appeal of alternative medicine, whose proponents are swayed by positive anecdotal evidence but treat scientific evidence hyper-critically.
Cognitive therapy was developed by Aaron T. Beck in the early 1960s and has become a popular approach. According to Beck, biased information processing is a factor in depression. His approach teaches people to treat evidence impartially, rather than selectively reinforcing negative outlooks. Phobias and hypochondria have also been shown to involve confirmation bias for threatening information.

Nickerson argues that reasoning in judicial and political contexts is sometimes subconsciously biased, favoring conclusions that judges, juries or governments have already committed to. Since the evidence in a jury trial can be complex, and jurors often reach decisions about the verdict early on, it is reasonable to expect an attitude polarization effect. The prediction that jurors will become more extreme in their views as they see more evidence has been borne out in experiments with mock trials. Both inquisitorial and adversarial criminal justice systems are affected by confirmation bias.
Confirmation bias can be a factor in creating or extending conflicts, from emotionally charged debates to wars: by interpreting the evidence in their favor, each opposing party can become overconfident that it is in the stronger position. On the other hand, confirmation bias can result in people ignoring or misinterpreting the signs of an imminent or incipient conflict. For example, psychologists Stuart Sutherland and Thomas Kida have each argued that U.S. Navy Admiral Husband E. Kimmel showed confirmation bias when playing down the first signs of the Japanese attack on Pearl Harbor.
A two-decade study of political pundits by Philip E. Tetlock found that, on the whole, their predictions were not much better than chance. Tetlock divided experts into "foxes" who maintained multiple hypotheses, and "hedgehogs" who were more dogmatic. In general, the hedgehogs were much less accurate. Tetlock blamed their failure on confirmation bias, and specifically on their inability to make use of new information that contradicted their existing theories.
In police investigations, a detective may identify a suspect early in an investigation, but then sometimes largely seek supporting or confirming evidence, ignoring or downplaying falsifying evidence.

Social psychologists have identified two tendencies in the way people seek or interpret information about themselves. Self-verification is the drive to reinforce the existing self-image and self-enhancement is the drive to seek positive feedback. Both are served by confirmation biases. In experiments where people are given feedback that conflicts with their self-image, they are less likely to attend to it or remember it than when given self-verifying feedback. They reduce the impact of such information by interpreting it as unreliable. Similar experiments have found a preference for positive feedback, and the people who give it, over negative feedback.

Confirmation bias can play a key role in the propagation of mass delusions. Witch trials are frequently cited as an example.
For another example, in the Seattle windshield pitting epidemic, there seemed to be a "pitting epidemic" in which windshields were damaged due to an unknown cause. As news of the apparent wave of damage spread, more and more people checked their windshields, discovered that their windshields too had been damaged, thus confirming belief in the supposed epidemic. In fact, the windshields were previously damaged, but the damage went unnoticed until people checked their windshields as the delusion spread.

One factor in the appeal of alleged psychic readings is that listeners apply a confirmation bias which fits the psychic's statements to their own lives. By making a large number of ambiguous statements in each sitting, the psychic gives the client more opportunities to find a match. This is one of the techniques of cold reading, with which a psychic can deliver a subjectively impressive reading without any prior information about the client. Investigator James Randi compared the transcript of a reading to the client's report of what the psychic had said, and found that the client showed a strong selective recall of the "hits".
As a striking illustration of confirmation bias in the real world, Nickerson mentions numerological pyramidology: the practice of finding meaning in the proportions of the Egyptian pyramids. There are many different length measurements that can be made of, for example, the Great Pyramid of Giza and many ways to combine or manipulate them. Hence it is almost inevitable that people who look at these numbers selectively will find superficially impressive correspondences, for example with the dimensions of the Earth.

Unconscious cognitive bias (including confirmation bias) in job recruitment can affect hiring decisions and can potentially prohibit a diverse and inclusive workplace. There are a variety of unconscious biases that can affect recruitment decisions, and confirmation bias is one of the major ones, especially during the interview stage. The interviewer may select a candidate that confirms their own beliefs, even though other candidates are equally or better qualified.

When people with opposing views interpret new information in a biased way, their views can move even further apart. This is called "attitude polarization". The effect was demonstrated by an experiment that involved drawing a series of red and black balls from one of two concealed "bingo baskets". Participants knew that one basket contained 60 percent black and 40 percent red balls; the other, 40 percent black and 60 percent red. The experimenters looked at what happened when balls of alternating color were drawn in turn, a sequence that does not favor either basket. After each ball was drawn, participants in one group were asked to state out loud their judgments of the probability that the balls were being drawn from one or the other basket. These participants tended to grow more confident with each successive draw—whether they initially thought the basket with 60 percent black balls or the one with 60 percent red balls was the more likely source, their estimate of the probability increased. Another group of participants were asked to state probability estimates only at the end of a sequence of drawn balls, rather than after each ball. They did not show the polarization effect, suggesting that it does not necessarily occur when people simply hold opposing positions, but rather when they openly commit to them.
A less abstract study was the Stanford biased interpretation experiment, in which participants with strong opinions about the death penalty read about mixed experimental evidence. Twenty-three percent of the participants reported that their views had become more extreme, and this self-reported shift correlated strongly with their initial attitudes. In later experiments, participants also reported their opinions becoming more extreme in response to ambiguous information. However, comparisons of their attitudes before and after the new evidence showed no significant change, suggesting that the self-reported changes might not be real. Based on these experiments, Deanna Kuhn and Joseph Lao concluded that polarization is a real phenomenon but far from inevitable, only happening in a small minority of cases, and it was prompted not only by considering mixed evidence, but by merely thinking about the topic.
Charles Taber and Milton Lodge argued that the Stanford team's result had been hard to replicate because the arguments used in later experiments were too abstract or confusing to evoke an emotional response. The Taber and Lodge study used the emotionally charged topics of gun control and affirmative action. They measured the attitudes of their participants towards these issues before and after reading arguments on each side of the debate. Two groups of participants showed attitude polarization: those with strong prior opinions and those who were politically knowledgeable. In part of this study, participants chose which information sources to read, from a list prepared by the experimenters. For example, they could read arguments on gun control from the National Rifle Association of America and the Brady Anti-Handgun Coalition. Even when instructed to be even-handed, participants were more likely to read arguments that supported their existing attitudes than arguments that did not. This biased search for information correlated well with the polarization effect.
The backfire effect is a name for the finding that given evidence against their beliefs, people can reject the evidence and believe even more strongly. The phrase was coined by Brendan Nyhan and Jason Reifler in 2010. However, subsequent research has since failed to replicate findings supporting the backfire effect. One study conducted out of the Ohio State University and George Washington University studied 10,100 participants with 52 different issues expected to trigger a backfire effect. While the findings did conclude that individuals are reluctant to embrace facts that contradict their already held ideology, no cases of backfire were detected. The backfire effect has since been noted to be a rare phenomenon rather than a common occurrence (compare the boomerang effect).

Confirmation biases provide one plausible explanation for the persistence of beliefs when the initial evidence for them is removed or when they have been sharply contradicted. This belief perseverance effect has been first demonstrated experimentally by Festinger, Riecken, and Schachter. These psychologists spent time with a cult whose members were convinced that the world would end on 21 December 1954. After the prediction failed, most believers still clung to their faith. Their book describing this research is aptly named When Prophecy Fails.
The term belief perseverance, however, was coined in a series of experiments using what is called the "debriefing paradigm": participants read fake evidence for a hypothesis, their attitude change is measured, then the fakery is exposed in detail. Their attitudes are then measured once more to see if their belief returns to its previous level.
A common finding is that at least some of the initial belief remains even after a full debriefing. In one experiment, participants had to distinguish between real and fake suicide notes. The feedback was random: some were told they had done well while others were told they had performed badly. Even after being fully debriefed, participants were still influenced by the feedback. They still thought they were better or worse than average at that kind of task, depending on what they had initially been told.
In another study, participants read job performance ratings of two firefighters, along with their responses to a risk aversion test. This fictional data was arranged to show either a negative or positive association: some participants were told that a risk-taking firefighter did better, while others were told they did less well than a risk-averse colleague. Even if these two case studies were true, they would have been scientifically poor evidence for a conclusion about firefighters in general. However, the participants found them subjectively persuasive. When the case studies were shown to be fictional, participants' belief in a link diminished, but around half of the original effect remained. Follow-up interviews established that the participants had understood the debriefing and taken it seriously. Participants seemed to trust the debriefing, but regarded the discredited information as irrelevant to their personal belief.
The continued influence effect is the tendency for misinformation to continue to influence memory and reasoning about an event, despite the misinformation having been retracted or corrected. This occurs even when the individual believes the correction.

Experiments have shown that information is weighted more strongly when it appears early in a series, even when the order is unimportant. For example, people form a more positive impression of someone described as "intelligent, industrious, impulsive, critical, stubborn, envious" than when they are given the same words in reverse order. This irrational primacy effect is independent of the primacy effect in memory in which the earlier items in a series leave a stronger memory trace. Biased interpretation offers an explanation for this effect: seeing the initial evidence, people form a working hypothesis that affects how they interpret the rest of the information.
One demonstration of irrational primacy used colored chips supposedly drawn from two urns. Participants were told the color distributions of the urns, and had to estimate the probability of a chip being drawn from one of them. In fact, the colors appeared in a prearranged order. The first thirty draws favored one urn and the next thirty favored the other. The series as a whole was neutral, so rationally, the two urns were equally likely. However, after sixty draws, participants favored the urn suggested by the initial thirty.
Another experiment involved a slide show of a single object, seen as just a blur at first and in slightly better focus with each succeeding slide. After each slide, participants had to state their best guess of what the object was. Participants whose early guesses were wrong persisted with those guesses, even when the picture was sufficiently in focus that the object was readily recognizable to other people.

Illusory correlation is the tendency to see non-existent correlations in a set of data. This tendency was first demonstrated in a series of experiments in the late 1960s. In one experiment, participants read a set of psychiatric case studies, including responses to the Rorschach inkblot test. The participants reported that the homosexual men in the set were more likely to report seeing buttocks, anuses or sexually ambiguous figures in the inkblots. In fact the fictional case studies had been constructed so that the homosexual men were no more likely to report this imagery or, in one version of the experiment, were less likely to report it than heterosexual men. In a survey, a group of experienced psychoanalysts reported the same set of illusory associations with homosexuality.
Another study recorded the symptoms experienced by arthritic patients, along with weather conditions over a 15-month period. Nearly all the patients reported that their pains were correlated with weather conditions, although the real correlation was zero.
This effect is a kind of biased interpretation, in that objectively neutral or unfavorable evidence is interpreted to support existing beliefs. It is also related to biases in hypothesis-testing behavior. In judging whether two events, such as illness and bad weather, are correlated, people rely heavily on the number of positive-positive cases: in this example, instances of both pain and bad weather. They pay relatively little attention to the other kinds of observation (of no pain or good weather). This parallels the reliance on positive tests in hypothesis testing. It may also reflect selective recall, in that people may have a sense that two events are correlated because it is easier to recall times when they happened together.

The anchoring effect is a psychological phenomenon in which an individual's judgments or decisions are influenced by a reference point or "anchor" which can be completely irrelevant. Both numeric and non-numeric anchoring have been reported through research. In numeric anchoring, once the value of the anchor is set, subsequent arguments, estimates, etc. made by an individual may change from what they would have otherwise been without the anchor. For example, an individual may be more likely to purchase a car if it is placed alongside a more expensive model (the anchor). Prices discussed in negotiations that are lower than the anchor may seem reasonable, perhaps even cheap to the buyer, even if said prices are still relatively higher than the actual market value of the car. Another example may be when estimating the orbit of Mars, one might start with the Earth's orbit (365 days) and then adjust upward until they reach a value that seems reasonable (usually less than 687 days, the correct answer).
The original description of the anchoring effect came from psychophysics. When judging stimuli along a continuum, it was noticed that the first and last stimuli were used to compare the other stimuli (this is also referred to as "end anchoring"). This was applied to attitudes by Muzafer Sherif et al. in their 1958 article "Assimilation and Contrast Effects of Anchoring Stimuli on Judgments".

The anchoring and adjustment heuristic was first theorized by Amos Tversky and Daniel Kahneman. In one of their first studies, participants were separated into one of two conditions, and either asked to compute, within 5 seconds, the product of the numbers one through to eight, either as 1 × 2 × 3 × 4 × 5 × 6 × 7 × 8 or reversed as 8 × 7 × 6 × 5 × 4 × 3 × 2 × 1. Because participants did not have enough time to calculate the full answer, they had to make an estimate after their first few multiplications. When these first multiplications gave a small answer – because the sequence started with small numbers – the median estimate was 512; when the sequence started with the larger numbers, the median estimate was 2,250. (The correct answer is 40,320.) In another study by Tversky and Kahneman, participants were asked to estimate the percentage of African countries in the United Nations. Before estimating, the participants first observed a roulette wheel that was predetermined to stop on either 10 or 65. Participants whose wheel stopped on 10 guessed lower values (25% on average) than participants whose wheel stopped at 65 (45% on average). The pattern has held in other experiments for a wide variety of different subjects of estimation.
As a second example, in a study by Dan Ariely, an audience is first asked to write the last two digits of their social security number and consider whether they would pay this number of dollars for items whose value they did not know, such as wine, chocolate and computer equipment. They were then asked to bid for these items, with the result that the audience members with higher two-digit numbers would submit bids that were between 60 percent and 120 percent higher than those with the lower social security numbers, which had become their anchor. When asked if they believed the number was informative of the value of the item, quite a few said yes. Trying to avoid this confusion, a small number of studies used procedures that were clearly random, such as Excel random generator button and die roll, and failed to replicate anchoring effects.
The anchoring effect was also found to be present in a study in the Journal of Real Estate Research in relation to house prices. In this investigation, it was established that the 2-year and 9-year highs on the Case-Shiller House Price Index could be used as anchors in predicting current house prices. The findings were used to indicate that, in forecasting house prices, these 2-year and 9-years highs might be relevant.
The anchoring effect was also found to be present in a study in the Journal of Behavioral Finance in relation to stock purchase behavior. The study found that when using an app-based stock brokerage, an investor’s first stock purchase price serves as an anchor for future stock purchases. The findings indicate that when investors start by making only a small stock purchase, they end up with less accumulated investments in the long run.

Various studies have shown that anchoring is very difficult to avoid. For example, in one study students were given anchors that were wrong. They were asked whether Mahatma Gandhi died before or after age 9, or before or after age 140. Clearly neither of these anchors can be correct, but when the two groups were asked to suggest when they thought he had died, they guessed significantly differently (average age of 50 vs. average age of 67).
Other studies have tried to eliminate anchoring much more directly. In a study exploring the causes and properties of anchoring, participants were exposed to an anchor and asked to guess how many physicians were listed in the local phone book. In addition, they were explicitly informed that anchoring would "contaminate" their responses, and that they should do their best to correct for that. A control group received no anchor and no explanation. Regardless of how they were informed and whether they were informed correctly, all of the experimental groups reported higher estimates than the control group. Thus, despite being expressly aware of the anchoring effect, most participants were still unable to avoid it. A later study found that even when offered monetary incentives, most people are unable to effectively adjust from an anchor.
Although it has been found through many research and experiments that attempt to mitigate the decision heuristic of anchoring bias is either marginally significant or not successful at all, it can be found that the consider-the-opposite (COS strategy) has been the most reliable in mitigating the anchoring bias (Adame, 2016). In short, the COS strategy is proposed to an individual by asking them to consider the possibilities the opposite of their perceptions and beliefs. Therefore, depriving the individual of their preexisting attitudes and limiting the decision bias.

Anchoring effects are also shown to remain adequately present given the accessibility of knowledge pertaining to the target. This, in turn, suggests that despite a delay in judgement towards a target, the extent of anchoring effects have seen to remain unmitigated within a given time period. A series of three experiments were conducted to test the longevity of anchoring effects. It was observed that despite a delay of one week being introduced for half the sample population of each experiment, similar results of immediate judgement and delayed judgement of the target were achieved.  The experiments concluded that external information experienced within the delayed judgement period shows little influence relative to self-generated anchors even with commonly encountered targets (temperature) used in one of the experiments, showing that anchoring effects may precede priming in duration especially when the anchoring effects were formed during the task. Further research to conclude an effect that is effectively retained over a substantial period of time has proven inconsistent.

One notable characteristic of the anchoring effect is its pervasiveness across diverse judgment scenarios. Furnham and Boo (2011) highlight that anchoring occurs not only in abstract estimation tasks (like guessing the height of Mount Everest) but also in real-world contexts such as legal sentencing, consumer purchasing, salary negotiations, and forecasting. Anchoring persists even when the anchor is implausible or clearly irrelevant (e.g., spinning a random wheel), demonstrating that anchoring can operate automatically, outside of conscious awareness or logical evaluation.

It is often presumed that groups come to a more unbiased decision relative to individuals. However, this assumption is supported with varied findings that could not come to a general consensus. Nevertheless, while some groups are able to perform better than an individual member, they are found to be just as biased or even more biased relative to their individual counterparts. A possible cause would be the discriminatory fashion in which information is communicated, processed and aggregated based on each individual's anchored knowledge and belief. This results in a diminished quality in the decision-making process and consequently, amplifies the pre-existing anchored biases.
The cause of group anchoring remains unsure. Group anchors may have been established at the group level or may simply be the culmination of several individual's personal anchors. Prior studies have shown that when given an anchor before the experiment, individual members consolidated the respective anchors to attain a decision in the direction of the anchor placed. However, a distinction between individual and group-based anchor biases does exist, with groups tending to ignore or disregard external information due to the confidence in the joint decision-making process. The presence of pre-anchor preferences also impeded the extent to which external anchors affected the group decision, as groups tend to allocate more weight to self-generated anchors, according to the 'competing anchor hypothesis'.
Recently, it has been suggested that the group member who speaks first often has an unproportionally high impact on the final decision.
A series of experiments were conducted to investigate anchoring bias in groups and possible solutions to avoid or mitigate anchoring. The first experiment established that groups are indeed influenced by anchors while the other two experiments highlighted methods to overcome group anchoring bias. Methods that were utilized include the use of process accountability and motivation through competition instead of cooperation to reduce the influence of anchors within groups.

A peer-reviewed study sought to investigate the effect of business intelligence (BI) systems on the anchoring effect. Business intelligence denotes an array of software and services used by businesses to gather valuable insights into an organisation's performance. The extent to which cognitive bias is mitigated by using such systems was the overarching question in this study. While the independent variable was the use of the BI system, the dependent variable was the outcome of the decision-making process. The subjects were presented with a 'plausible' anchor and a 'spurious' anchor in a forecasting decision. It was found that, while the BI system mitigated the negative effects of the spurious anchor, it had no influence on the effects of the plausible anchor. This is important in a business context, because it shows that humans are still susceptible to cognitive biases, even when using sophisticated technological systems. One of the subsequent recommendations from the experimenters was to implement a forewarning into BI systems as to the anchoring effect.

The present literature does not reach a consensus as to the cause of anchoring. However, scholars agree that anchoring is a phenomenon that can be easily demonstrated but is hard to explain. Some scholars suggest that anchoring is, in fact, caused by a combination of factors.

The most prevalent explanation of the anchoring effect is the argument originally made by Tversky and Kahneman, termed anchoring-as-adjusting. Based on this theory, individuals set an anchor according to available information, whether it is provided or individuals already have an anchor in mind, and use this anchor as a point of reference to adjust their answers. This theory explains inaccuracy in guessing by suggesting that people adjust insufficiently, rendering their final guess closer to the anchors. This phenomenon was further investigated in other studies and used as an explanation for biases such as hindsight bias and egocentric bias.
For instance, when asked to guess the price of a drink in a coffee shop, individuals often look for the price of other drinks or recall the price of similar drinks at other stores, and base their answer on the anchor that they set. Specifically, the original study found that by simply providing an arbitrary anchor to each group, people provided significantly different responses, suggesting that their answers were generated through anchoring and adjusting. In addition, this finding suggests that individuals are gullible when setting an anchor, and almost automatically begin the anchoring-and-adjusting process.
On the other hand, Epley and Gilovich found that when anchors are self-generated, people will stop adjusting once they believe they have adjusted their answers to an acceptable range. Although this process does not guarantee insufficient adjustments, it does result in an answer that is as close to the anchor as possible in the acceptable range. However, insufficient adjustments are diminished when individuals are able and motivated by external factors, such as monetary compensation, to continue adjusting for a more accurate answer, thereby reducing the anchoring effect. The anchoring effect is also reduced when individuals know which way to adjust from the anchor because it eliminates the possible answers by half.
Besides general knowledge, anchoring is also observed in social settings. The simulation theory of empathy suggests that people use their own mental state and reasoning to infer the actions of others. People assume that those who are similar to us will act in a similar way. Aligned with this idea, Tamir and Mitchell found that judgments of the attitude of others are made more quickly for those who are similar to the judge, and greater self-other discrepancy resulted in longer reaction time. In this case, individuals use their own attitudes as an anchor and make adjustments to predict the attitude of others. Note that this process only takes place when the judge perceives great similarity between self and others.
Overall, this theory describes the process of anchoring and adjusting away from the anchor, as well as the phenomenon of inaccurate responses due to insufficient adjusting. However, proponents of alternative theories argued that adjusting is only possible when the original anchor lies outside the acceptable range. According to Epley and Gilovich, individuals will not adjust at all and give an answer that is identical to their anchor if the anchor is already within the acceptable range. Using the previous example, the actual price of the drink can be identical to other drinks served at that store, meaning that people should theoretically consider their anchor as a potential answer rather than adjusting. When a reasonable anchor is given, there will be no adjustment. Therefore, this theory does not explain all cases of anchoring.

An alternative explanation of the anchoring effect is the idea that the accessibility of anchor-consistent information is enhanced when individuals consider it as a potential answer. After determining that the initial anchor is not the correct answer, they move on to consider other possibilities. However, because the anchor was just made salient and accessible to them, they will take the anchor into consideration while evaluating other possibilities. As a result, this comparative assessment can result in answers that are disproportionally consistent with the anchor. In this sense, anchoring is a special case of semantic priming, where the anchor acts as the prime.
Unlike anchoring-and-adjusting, this theory suggests that people consider the relevant attributes of the initial anchor to determine if the anchor is plausible. Rather than accepting any plausible anchor as their answer and insufficiently adjusting anchors outside of the acceptable range, people will remain motivated to find a more accurate answer after rejecting the initial anchor. The preceding adjustments that individuals make are relevant to the anchor because people evaluate hypotheses through attempting to confirm them. In line with this idea, when investigating whether the plausibility of an anchor affects comparative and absolute judgments, Strack and Mussweiler found that individuals take longer to provide an absolute judgment when the anchor is implausible. Comparative judgments refer to the process of using an anchor to determine the final answer. Most paradigms used in anchoring studies also ask participants to provide an absolute judgment (i.e., provide a concrete number as their answer) after comparative judgments. When plausible anchors were used in comparative judgments, the anchor became more accessible, shortening the response time for consequent absolute judgments. In contrast, implausible anchors result in longer response time because there is no relevant information that can be used as primes.
Although selective accessibility and anchoring-and-adjusting provide conflicting explanations for the anchoring effect, some scholars have argued that anchoring is influenced by multiple factors, and these theories complement each other in explaining the anchoring effect. For instance, Simmons and colleagues proposed an integrative theory suggesting that an anchor can result in selective reliance on anchor-consistent information, rendering the range of plausible answers closer to the anchor. On the other hand, people adjust away from (or possibly back toward) anchors before settling on their final estimate. This integrative theory is more parsimonious because it suggests that neither the source of the anchor (self-generated or provided) nor the plausibility of the anchor has a significant effect on judgments.

On the contrary of the two previous theories, the attitude change view suggests that individuals' attitude towards an anchor, specifically provided anchors, can heavily affect the extent of the anchoring effect. Recall that individuals often seek to confirm their hypothesis rather than objectively evaluating all information. When individuals disagree with the provided anchor, they will selectively seek evidence that supports their own attitudes instead of the provided anchor, resulting in an unobserved anchoring effect. This theory highlights the idea that the distinct anchoring effect observed with self-generated and provided anchors roots from individual attitude (i.e., does the individual believe in the anchor) rather than the anchors themselves, supporting the integrative theory proposed by Simmons and colleagues. Furthermore, supporters of this view have argued that attitude change is an alternative explanation of the anchoring effect. Providing an anchor generates favorable attitudes in individuals toward the anchor, biasing consequent judgments.

Extremeness aversion is a robust phenomenon where people try to avoid the extremes during decision-making, such as selecting the middle options more often than other extreme options and avoiding reporting the maximum or minimum on a Likert scale. This desire to avoid extremes is at least partially responsible for the anchoring effect. For instance, upon setting an anchor, participants who were told they could adjust up to 6 units made significantly smaller adjustments compared to those who were told that they could adjust up to 15 units, suggesting that people avoid extremes when making decisions. Importantly, the maximum allowable adjustment  acted as an anchor that affected the final judgment, highlighting the prevalent aversion to extremes. As a result, the final judgment is close to the anchor because people do not want to adjust too close to the extremes.

A wide range of research has linked sad or depressed moods with more extensive and accurate evaluation of problems. As a result of this, earlier studies hypothesized that people with more depressed moods would tend to use anchoring less than those with happier moods. However, more recent studies have shown the opposite effect: sad people are more likely to use anchoring than people with happy or neutral mood. In a study focusing on medical practitioners, it was found that physicians that possess positive moods are less susceptible to anchoring bias, when compared to physicians with neutral moods. This was specifically found to be because a positive mood leads to information processing that is more systematic which leads to more efficient problem solving. This leads to a decreased anchoring effect.

Early research found that experts (those with high knowledge, experience, or expertise in some field) were more resistant to the anchoring effect. However, anchoring happens unconsciously which means that unless someone who is knowledgeable is warned prior, they are still susceptible to anchoring. Since then, however, numerous studies have demonstrated that while experience can sometimes reduce the effect, even experts are susceptible to anchoring. In a study concerning the effects of anchoring on judicial decisions, researchers found that even experienced legal professionals were affected by anchoring. This remained true even when the anchors provided were arbitrary and unrelated to the case in question. Also, this relates to goal setting, where more experienced individuals will set goals based on their past experiences which consequently affects end results in negotiations.
Expertise is when a judge has relevant knowledge. In a study using price estimation of cars, it was found that relevant knowledge positively influenced anchoring. Expertise in cognitive bias is related to experience however the two are not exclusively exhaustive. In a study using stock return estimates, it was found that expertise decreases behavioural bias significantly. It was found that other factors like cognitive ability and experience where there is no susceptibility to anchoring or a susceptibility as it increases, tend to become factors that decrease the effects of anchoring when they are an expert.

Correlational research on anchoring bias and personality traits yielded mixed results, with emphasis on the Big Five personality traits which includes: Conscientiousness (orderly and responsible), neuroticism (uneasy and anxious), extraversion (sociable and outgoing), openness (intelligence and creativity) and agreeableness (polite and trusting). One study found that participants who were high in the Openness trait were more influenced by anchors set by others when estimating the length of the Mississippi river, with no other personality traits correlating to the effects of anchoring.However, other research showed that it was conscientiousness and agreeableness that increased anchoring biases, while anchoring effects were diminished in participants high in the extraversion trait.

The impact of cognitive ability on anchoring is contested. A recent study on willingness to pay for consumer goods found that anchoring decreased in those with greater cognitive ability, though it did not disappear. Another study, however, found that cognitive ability had no significant effect on how likely people were to use anchoring. In a poker-like experiment that included people of differing academic achievement and psychometric reasoning scoring, it has been found that anchoring is not related to education level. It also found that numerical reasoning and reflection scores had a negative association with anchoring susceptibility.

Although overconfidence emanates from the heuristic and refers more specifically to a behavioural tendency to take their initial assessment and put more emphasis on it during making their initial assessment leading to cognitive conceit. Cognitive conceit or overconfidence arises from other factors like personal cognitive attributes such as knowledge and decision-making ability, decreasing the probability to pursue external sources of confirmation. This factor has also been shown to arise with tasks with greater difficulty. Even within subject matter experts, they were also prey to such behaviour of overconfidence and should more so, actively reduce such behaviour. Following the study of estimations under uncertain, despite several attempts to curb overconfidence proving unsuccessful, Tversky and Kahneman (1971) suggest an effective solution to overconfidence is for subjects to explicitly establish anchors to help reduce overconfidence in their estimates.

The motivation to be accurate in one's judgements seem to have mixed effects on the strength of anchoring. On one hand, According to Wegener's attitude change theory, it was widely accepted that the prevalent effects of anchoring was due to the pathway of low-elaboration, non-thoughtful processes. The lack of reward or consequences results in the assumption that anchors are a reasonable hint to the correct answer without considering contextual differences, categorical differences, or even the relevance of the anchor. There is also evidence that the effects of anchoring is diminished when there is prior warning about the phenomenon of insufficient adjustment  and self-generated anchors.
However, there is also conflicting evidence where increases in motivation does not correlate to a lowered rate of anchoring. There were no differences in the effects of anchoring when comparing participants who were offered monetary rewards for accurate answers to those who weren't. Moreover, findings by Wilson et al. (1996) concluded that incentives and forewarnings did not eliminate anchoring effects.
This could be explained by high elaborative anchoring - When motivated to be accurate, participants engage in more cognitively demanding thought processes, searching for existing information, including prior experiences and established anchors. The high need for accuracy lead to more effortful thought processes, and putting a heavier emphasis on anchors since they are representations of prior knowledge in what we perceive as similar categories. Findings have demonstrated that both a high and low need to be accurate result in susceptibility to the influence of anchoring effects, even when one is motivated to explicitly avoid them.

Culture has been identified as an influencing factor in susceptibility to the anchoring effect. A study comparing students from Poland and India found that while both groups were affected by anchoring, the degree of susceptibility varied significantly by cultural background. Polish students demonstrated lower susceptibility to anchoring compared to Indian students. Researchers proposed that individuals from cultures characterized by holistic thinking styles—more common in East and South Asian societies—are more prone to contextual influences and thus more susceptible to anchoring. Additionally, cultural differences in overconfidence were observed, with Indian students displaying a higher rate of overprecision compared to Polish students. This is explained by the differences in cultural difference in tolerances for ambiguity and risk (uncertainty avoidance), with Poland scoring high and India scoring medium to low on Hofstede’s Uncertainty Avoidance Index (UAI).

In the negotiation process anchoring serves to determine an accepted starting point for the subsequent negotiations. As soon as one side states their first price offer, the (subjective) anchor is set. The counterbid (counter-anchor) is the second-anchor.
In addition to the initial research conducted by Tversky and Kahneman, multiple other studies have shown that anchoring can greatly influence the estimated value of an object. For instance, although negotiators can generally appraise an offer based on multiple characteristics, studies have shown that they tend to focus on only one aspect. In this way, a deliberate starting point can strongly affect the range of possible counteroffers. The process of offer and counteroffer results in a mutually beneficial arrangement. However, multiple studies have shown that initial offers have a stronger influence on the outcome of negotiations than subsequent counteroffers.
An example of the power of anchoring has been conducted during the Strategic Negotiation Process Workshops. During the workshop, a group of participants is divided into two sections: buyers and sellers. Each side receives identical information about the other party before going into a one-on-one negotiation. Following this exercise, both sides debrief about their experiences. The results show that where the participants anchor the negotiation had a significant effect on their success.
Anchoring affects everyone, even people who are highly knowledgeable in a field. Northcraft and Neale conducted a study to measure the difference in the estimated value of a house between students and real-estate agents. In this experiment, both groups were shown a house and then given different listing prices. After making their offer, each group was then asked to discuss what factors influenced their decisions. In the follow-up interviews, the real-estate agents denied being influenced by the initial price, but the results showed that both groups were equally influenced by that anchor.
Anchoring can have more subtle effects on negotiations as well. Janiszewski and Uy investigated the effects of precision of an anchor. Participants read an initial price for a beach house, then gave the price they thought it was worth. They received either a general, seemingly nonspecific anchor (e.g., $800,000) or a more precise and specific anchor (e.g., $799,800). Participants with a general anchor adjusted their estimate more than those given a precise anchor ($751,867 vs $784,671). The authors propose that this effect comes from difference in scale; in other words, the anchor affects not only the starting value, but also the starting scale. When given a general anchor of $20, people will adjust in large increments ($19, $21, etc.), but when given a more specific anchor like $19.85, people will adjust on a lower scale ($19.75, $19.95, etc.). Thus, a more specific initial price will tend to result in a final price closer to the initial one.
As for the question of setting the first or second anchor, the party setting the second anchor has the advantage in that the counter-anchor determines the point midway between both anchors. Due to a possible lack of knowledge the party setting the first anchor can also set it too low, i.e. against their own interests. Generally negotiators who set the first anchor also tend to be less satisfied with the negotiation outcome, than negotiators who set the counter-anchor. This may be due to the regret or sense that they did not achieve or rather maximise the full potential of the negotiations. However, studies suggest that negotiators who set the first offer frequently achieve economically more advantageous results.

According to the theory, consumers’ shopping experiences are influenced by factors such as time restriction and specific environment. Enterprises design would set anchor values for consumers in order to get them to buy the products. 
When persuading consumers to purchase a particular product, sellers might use anchoring. Sellers often influence consumers’ price perception by anchoring a high reference price and that is an anchor value. Following are three ways to set the anchor value for consumers.

Sellers usually sort the prices of products from high to low and this method is common seen on the menus of restaurants. The high prices at the top of the menu act as anchor values in this situation. Consumers will have an expectation that the products are all expensive when knowing the relatively high prices of products on the top of the list. As a result, they will be pleased to see the cheaper products at the middle and bottom of the list and regard these prices as acceptable or cheaper than expected. Therefore, they are more likely to buy these products.

Decoy effect is defined as a situation where people tend to have a change in preference between two choices when they are showed with a third choice. The third choice is called a decoy which is designed to induce consumers to change their preferences. The decoy is usually considered as inferior. For example, it might be more expensive than option A while having lower quality than option B. In this case, the anchor is the decoy.
One decoy effect example is the bundle sales. For example, many restaurants often sell set meals to their consumers, while simultaneously having the meals’ components sold separately. The prices of the meals’ components are the decoy pricing and act as an anchor which enables to make the set meal more valuable to consumers. With the decoy effect it generates, the anchor increases consumers’ willingness to pay for the set meals, or the mixed bundles.

Incidental price is defined as the prices offered or showed by a seller for products which the consumers are not interested in. According to the theory, the incidental price serves as an anchor which increases consumers’ willingness to pay. This effect has been widely used in areas such as auctions, online vendors and retailers.

When examining the relationship between personality and individual determinants, such as extroversion and introversion, a relationship between high levels of conscientiousness and extraversion with anchoring biases was identified. Nonetheless, when measuring the Big Five personality traits and anchoring susceptibility, no significant correlation was found between personality and anchoring. The anchoring effect seems to be present regardless of personality.

Predrag Teovanović’s  investigated whether intelligence, cognitive reflection and personality traits affects the presence of anchoring effect in decision-making. Although measures of individual differences in susceptibility to anchoring were reliable, individual differences only explain a small portion of the variation. However, intelligence is negatively correlated with anchoring for participants who are more reflective. By critically thinking about their process of decision-making, reflective individuals might realize the unreasonable reliance on anchors and insufficient adjustments. Similarly, Welsh and colleagues found a weak, negative correlation between aptitude for rationality and overall cognitive measures and anchoring susceptibility.
Research supporting individual differences in anchoring suggests that individuals who recognize the potential bias of anchoring and actively reflect on their decision-making process tend to be less susceptible to its effects. By critically assessing whether their judgments are based on reliable data or influenced by arbitrary anchors, they are more likely to identify and correct for insufficient adjustments, or choose not to use unreliable anchors at all.

Susceptibility to anchoring is influenced by mood. Individuals in a sad mood were more susceptible to anchoring compared to those in a neutral or happy mood. According to the attitude change theory, people in a sad mood will engage in more effortful processing, in a similar fashion as hypothesis confirmation. On the other hand, based on the selective accessibility theory, sad mood results in more thorough information processing, thereby promoting the search for anchoring-consistent information to confirm their hypothesis.

The availability heuristic, also known as availability bias, is a mental shortcut that relies on immediate examples that come to a given person's mind when evaluating a specific topic, concept, method, or decision. This heuristic, operating on the notion that, if something can be recalled, it must be important, or at least more important than alternative solutions not as readily recalled, is inherently biased toward recently acquired information.
The mental availability of an action's consequences is positively related to those consequences' perceived magnitude. In other words, the easier it is to recall the consequences of something, the greater those consequences are often perceived to be. Most notably, people often rely on the content of their recall if its implications are not called into question by the difficulty they have in recalling it.

In the late 1960s and early 1970s, Amos Tversky and Daniel Kahneman began work on a series of papers examining "heuristic and biases" used in judgment under uncertainty. Prior to that, the predominant view in the field of human judgment was that humans are rational actors. Kahneman and Tversky explained that judgment under uncertainty often relies on a limited number of simplifying heuristics rather than extensive algorithmic processing. Soon, this idea spread beyond academic psychology, into law, medicine, and political science. This research questioned the descriptive adequacy of idealized models of judgment, and offered insights into the cognitive processes that explained human error without invoking motivated irrationality. One simplifying strategy people may rely on is the tendency to make a judgment about the frequency of an event based on how many similar instances are brought to mind. In 1973, Amos Tversky and Daniel Kahneman first studied this phenomenon and labeled it the "availability heuristic". An availability heuristic is a mental shortcut that relies on immediate examples that come to a given person's mind when evaluating a specific topic, concept, method, or decision. As follows, people tend to use a readily available fact to base their beliefs on a comparably distant concept. There has been much research done with this heuristic, but studies on the issue are still questionable with regard to the underlying process. Studies illustrate that manipulations intended to increase the subjective experience of ease of recall are also likely to affect the amount of recall. Furthermore, this makes it difficult to determine whether the obtained estimates of frequency, likelihood, or typicality are based on participants' phenomenal experiences or on a biased sample of recalled information.
However, some textbooks have chosen the latter interpretation introducing the availability heuristic as "one's judgments are always based on what comes to mind". For example, if a person is asked whether there are more words in the English language that start with a k or have k as the third letter, the person will probably be able to think of more words that begin with the letter k, concluding incorrectly that k is more frequent as the first letter than the third. In this Wikipedia article itself, for example, there are multiple instances of words such as "likely", "make", "take", "ask" and indeed "Wikipedia", but (aside from names) only a couple of initial K's: "know" and "key".

Chapman (1967) described a bias in the judgment of the frequency with which two events co-occur. This demonstration showed that the co-occurrence of paired stimuli resulted in participants overestimating the frequency of the pairings. To test this idea, participants were given information about several hypothetical mental patients. The data for each patient consisted of a clinical diagnosis and a drawing made by the patient. Later, participants estimated the frequency with which each diagnosis had been accompanied by various features of the drawing. The subjects vastly overestimated the frequency of this co-occurrence (such as suspiciousness and peculiar eyes). This effect was labeled the illusory correlation. Tversky and Kahneman suggested that availability provides a natural account for the illusory-correlation effect. The strength of the association between two events could provide the basis for the judgment of how frequently the two events co-occur. When the association is strong, it becomes more likely to conclude that the events have been paired frequently. Strong associations will be thought of as having occurred together frequently.
In Tversky and Kahneman's seminal paper, they include findings from several other studies, which also show support for the availability heuristic. Apart from their findings in the "K" study, they also found:
When participants were shown two visual structures and asked to pick the structure that had more paths, participants saw more paths in the structure that had more obvious available paths. In the structure that participants chose, there were more columns and shorter obvious paths, making it more available to them. When participants were asked to complete tasks involving estimation, they would often underestimate the end result. Participants were basing their final estimation on a quick first impression of the problem. Participants particularly struggled when the problems consisted of multiple steps. This occurred because participants were basing their estimation on an initial impression. Participants failed to account for the high rate of growth in the later steps due to the impression they formed in the initial steps. This was shown again in a task that asked participants to estimate the answer to a multiplication task, in which the numbers were presented as either 1x2x3x4x5x6x7x8 or 8x7x6x5x4x3x2x1. Participants who were presented the equation with the larger numbers first (8x7x6...), estimated a significantly higher result than participants with the lower numbers first (1x2x3...). Participants were given a short amount of time to make the estimation, thus participants based their estimates off of what was easily available, which in this case was the first few numbers in the sequence.

Many researchers have attempted to identify the psychological process which creates the availability heuristic.
Tversky and Kahneman argue that the number of examples recalled from memory is used to infer the frequency with which such instances occur. In an experiment to test this explanation, participants listened to lists of names containing either 19 famous women and 20 less famous men or 19 famous men and 20 less famous women. Subsequently, some participants were asked to recall as many names as possible whereas others were asked to estimate whether male or female names were more frequent on the list. The names of the famous celebrities were recalled more frequently compared to those of the less famous celebrities. The majority of the participants incorrectly judged that the gender associated with more famous names had been presented more often than the gender associated with less famous names. Tversky and Kahneman argue that although the availability heuristic is an effective strategy in many situations when judging probability, use of this heuristic can lead to predictable patterns of errors.
Schwarz and his colleagues, on the other hand, proposed the ease of retrieval explanation, wherein the ease with which examples come to mind, not the number of examples, is used to infer the frequency of a given class. In a study by Schwarz and colleagues to test their explanation, participants were asked to recall either six or twelve examples of their assertive or very unassertive behavior. Participants were later asked to rate their own assertiveness. Pretesting had indicated that although most participants were capable of generating twelve examples, this was a difficult task. The results indicated that participants rated themselves as more assertive after describing six examples of assertive compared with unassertive behavior, but rated themselves as less assertive after describing twelve examples of assertive compared with unassertive behavior. The study reflected that the extent to which recalled content impacted judgment was determined by the ease with which the content could be brought to mind (it was easier to recall 6 examples than 12), rather than the amount of content brought to mind.
Research by Vaughn (1999) looked at the effects of uncertainty on the use of the availability heuristic. College students were asked to list either three or eight different study methods they could use in order to get an A on their final exams. The researchers also manipulated the time during the semester they would ask the students to complete the questionnaire.  Approximately half of the participants were asked for their study methods during the third week of classes, and the other half were asked on the last day of classes. Next, participants were asked to rate how likely they would be to get an A in their easiest and hardest classes.  Participants were then asked to rank the difficulty they experienced in recalling the examples they had previously listed. The researchers hypothesized that students would use the availability heuristic, based on the number of study methods they listed, to predict their grade only when asked at the beginning of the semester and about their hardest final. Students were not expected to use the availability heuristic to predict their grades at the end of the semester or about their easiest final. The researchers predicted this use of the availability heuristic because participants would be uncertain about their performance throughout the semester. The results indicated that students used the availability heuristic, based on the ease of recall of the study methods they listed, to predict their performance when asked at the beginning of the semester and about their hardest final. If the student listed only three study methods, they predicted a higher grade at the end of the semester only on their hardest final. If students listed eight study methods, they had a harder time recalling the methods and thus predicted a lower final grade on their hardest final. The results were not seen in the easy final condition because the students were certain they would get an A, regardless of the study method. The results supported this hypothesis and gave evidence to the fact that levels of uncertainty affect the use of the availability heuristic.

After seeing news stories about child abductions, people may judge that the likelihood of this event is greater. Media coverage can help fuel a person's example bias with widespread and extensive coverage of unusual events, such as homicide or airline accidents, and less coverage of more routine, less sensational events, such as common diseases or car accidents. For example, when asked to rate the probability of a variety of causes of death, people tend to rate "newsworthy" events as more likely because they can more readily recall an example from memory. Moreover, unusual and vivid events like homicides, shark attacks, or lightning are more often reported in mass media than common and un-sensational causes of death like common diseases.
For example, many people think that the likelihood of dying from shark attacks is greater than that of dying from being hit by falling airplane parts when more people actually die from falling airplane parts. When a shark attack occurs, the deaths are widely reported in the media whereas deaths as a result of being hit by falling airplane parts are rarely reported in the media.
In a 2010 study exploring how vivid television portrayals are used when forming social reality judgments, people watching vivid violent media gave higher estimates of the prevalence of crime and police immorality in the real world than those not exposed to vivid television. These results suggest that television violence does in fact have a direct causal impact on participants' social reality beliefs. Repeated exposure to vivid violence leads to an increase in people's risk estimates about the prevalence of crime and violence in the real world.  Counter to these findings, researchers from a similar study argued that these effects may be due to effects of new information. Researchers tested the new information effect by showing movies depicting dramatic risk events and measuring their risk assessment after the film. Contrary to previous research, there were no long-term effects on risk perception due to exposure to dramatic movies. However, the study did find evidence of idiosyncratic effects of the movies - that is, people reacted immediately after the movies with enhanced or diminished risk beliefs, which faded after a period of 10 days.
Another measurable effect is the inaccurate estimation of the fraction of deaths caused by terrorism compared to homicides with other causes.

Researchers examined the role of cognitive heuristics in the AIDS risk-assessment process. 331 physicians reported worry about on-the-job HIV exposure, and experience with patients who have HIV. By analyzing answers to questionnaires handed out, researchers concluded that availability of AIDS information did not relate strongly to perceived risk.
Participants in a 1992 study read case descriptions of hypothetical patients who varied on their sex and sexual preference. These hypothetical patients showed symptoms that could have been caused by five different diseases (AIDS, leukemia, influenza, meningitis, or appendicitis). Participants were instructed to indicate which disease they thought the patient had and then they rated patient responsibility and interaction desirability. Consistent with the availability heuristic, either the more common (influenza) or the more publicized (AIDS) disease was chosen.

One study sought to analyze the role of the availability heuristic in financial markets. Researchers defined and tested two aspects of the availability heuristic:
Outcome Availability – availability of positive and negative investment outcomes, and
Risk Availability – availability of financial risk.
On days of substantial stock market moves, abnormal stock price reactions to upgrades are weaker, than those to downgrades. These availability effects are still significant even after controlling for event-specific and company-specific factors.
Similarly, research has pointed out that under the availability heuristic, humans are not reliable because they assess probabilities by giving more weight to current or easily recalled information instead of processing all relevant information. Since information regarding the current state of the economy is readily available, researchers attempted to expose the properties of business cycles to predict the availability bias in analysts' growth forecasts. They showed the availability heuristic to play a role in analysis of forecasts and influence investments because of this.
Additionally, a study by Hayibor and Wasieleski found that the availability of others who believe that a particular act is morally acceptable is positively related to others' perceptions of the morality of that act. This suggests that availability heuristic also has an effect on ethical decision making and ethical behavior in organizations.

A study done by Craig R. Fox provides an example of how availability heuristics can work in the classroom. In this study, Fox tests whether the difficulty of recall influences judgment, specifically with course evaluations among college students.  In his study he had two groups complete a course evaluation form.  He asked the first group to write two recommended improvements for the course (a relatively easy task) and then write two positives about the class. The second group was asked to write ten suggestions where the professor could improve (a relatively difficult task) and then write two positive comments about the course.  At the end of the evaluation, both groups were asked to rate the course on a scale from one to seven.  The results showed that students asked to write ten suggestions (difficult task) rated the course less harshly because it was more difficult for them to recall the information. Most of the students in the group that was asked to fill in 10 suggestions didn't fill in more than two being unable to recall more instances where they were unsatisfied with the class. Students asked to do the easier evaluation with only two complaints had less difficulty in terms of availability of information, so they rated the course more harshly.
Another study by Marie Geurten sought to test the availability heuristic in young children. Children of varying ages (from 4 to 8 years old) were tasked with generating a list of names, with some being asked for a shorter list and some for a longer list. The study then assessed the children's own impressions of their ability to recall names. Those children who were tasked with generating a shorter list had a higher perception of their ability to recall names than those who were tasked with generating a longer list. According to the study, this suggests that the children based their assessment of their recall abilities on their subjective experience of ease of recall.

The media usually focuses on violent or extreme cases, which are more readily available in the public's mind. This may come into play when it is time for the judicial system to evaluate and determine the proper punishment for a crime. In one study, respondents rated how much they agreed with hypothetical laws and policies such as "Would you support a law that required all offenders convicted of unarmed muggings to serve a minimum prison term of two years?" Participants then read cases and rated each case on several questions about punishment. As hypothesized, respondents recalled more easily from long-term memory stories that contain severe harm, which seemed to influence their sentencing choices to make them push for harsher punishments. This can be eliminated by adding high concrete or high contextually distinct details into the crime stories about less severe injuries.
A similar study asked jurors and college students to choose sentences on four severe criminal cases in which prison was a possible but not an inevitable sentencing outcome. Respondents answering questions about court performance on a public opinion formulated a picture of what the courts do and then evaluated the appropriateness of that behavior. Respondents recalled public information about crime and sentencing. This type of information is incomplete because the news media present a highly selective and non-representative selection of crime, focusing on the violent and extreme, rather than the ordinary. This makes most people think that judges are too lenient. But, when asked to choose the punishments, the sentences given by students were equal to or less severe than those given by judges. In other words, the availability heuristic made people believe that judges and jurors were too lenient in the courtroom, but the participants gave similar sentences when placed in the position of the judge, suggesting that the information they recalled was not correct.
Researchers in 1989 predicted that mock jurors would rate a witness to be more deceptive if the witness testified truthfully before lying than when the witness was caught lying first before telling the truth. If the availability heuristic played a role in this, lying second would remain in jurors' minds (since it was more recent) and they would most likely remember the witness lying over the truthfulness. To test the hypothesis, 312 university students played the roles of mock jurors and watched a videotape of a witness presenting testimony during a trial. Results confirmed the hypothesis, as mock jurors were most influenced by the most recent act.

Previous studies have indicated that explaining a hypothetical event makes the event seem more likely through the creation of causal connections. However, such effects could arise through the use of the availability heuristic; that is, subjective likelihood is increased by an event becoming easier to imagine.
A study done asked those participating to pick between two illnesses. Those doing the study wanted to know which disease they thought was more likely to cause death. In the study, they asked participants to choose between a stroke and asthma as to which one someone was more likely to die from. The researchers concluded that it depended on what experiences were available to them. If they knew someone or heard of someone that died from one of the diseases that is the one they perceived to be a higher risk to die from.

Two studies with 108 undergraduates investigated vivid information and its impact on social judgment and the availability heuristic and its role in mediating vividness effects.
In study 1, Subjects listened to a tape recording that described a woman who lived with her 7-year-old son. Subjects then heard arguments about the woman's fitness as a parent and were asked to draw their own conclusions regarding her fitness or unfitness. The concrete and colorful language were found to influence judgments about the woman's fitness as a mother.
In study 2, a series of male and female names were presented to subjects; for each name, subjects were told the university affiliation of the individual (Yale or Stanford). When some names were presented, subjects were simultaneously shown a photograph that purportedly portrayed the named individual. Subsequently, to assess what subjects could remember (as a measure of availability), each name was represented, as well as the appropriate photograph if one had been originally presented. The study considered whether the display or non-display of photographs biased subjects' estimates as to the percentage of Yale (vs Stanford) students in the sample of men and women whose names appeared on the original list, and whether these estimated percentages were causally related to the respondents' memory for the college affiliations of the individual students on the list. The presence of photographs affected judgments about the proportion of male and female students at the two universities. Such effects have typically been attributed to the ready accessibility of vividly presented information in memory—that is, to the availability heuristic.
In both studies, vividness affected both availability (ability to recall) and judgments. However, causal modeling results indicated that the availability heuristic did not play a role in the judgment process.

In general, availability is correlated with ecological frequency, but it is also affected by other factors. Consequently, the reliance on the availability heuristic leads to systematic biases. Such biases are demonstrated in the judged frequency of classes of words, of combinatoric outcomes, and of repeated events. The phenomenon of illusory correlation is explained as an availability bias.
In the original Tversky and Kahneman (1973) research, three major factors that are discussed are the frequency of repetition, frequency of co-occurrence, and illusory correlation. The use of frequency of repetition aids in the retrieval of relevant instances. The idea behind this phenomenon is that the more an instance is repeated within a category or list, the stronger the link between the two instances becomes. Individuals then use the strong association between the instances to determine the frequency of an instance. Consequently, the association between the category or list and the specific instance often influences frequency judgement. Frequency of co-occurrence strongly relates to Frequency of repetition, such that the more an item-pair is repeated, the stronger the association between the two items becomes, leading to a bias when estimating the frequency of co-occurrence. Due to the phenomena of frequency of co-occurrence, illusory correlations also often play a big role.
Another factor that affects the availability heuristic in frequency and probability is exemplars. Exemplars are the typical examples that stand out during the process of recall. If asked what participants thought different set sizes were (how many men and how many women are in the class), participants would use exemplars to determine the size of each set.  Participants would derive their answers on ease of recall of the names that stood out. Participants read a list of names of members of a class for 30 seconds, and then participants were asked the male to female ratio of the class. The participant's answer would depend on the recall of exemplars. If the participant reading the list recalled seeing more common male names, such as Jack, but the only female names in the class were uncommon names, such as Deepika, then the participant will recall that there were more men than women. The opposite would be true if there were more common female names on the list and uncommon male names. Due to the availability heuristic, names that are more easily available are more likely to be recalled, and can thus alter judgments of probability.
Another example of the availability heuristic and exemplars would be seeing a shark in the ocean. Seeing a shark has a greater impact on an individual's memory than seeing a dolphin.  If someone sees both sharks and dolphins in the ocean, they will be less aware of seeing the dolphins, because the dolphins had less of an impact on their memory. Due to the greater impact of seeing a shark, the availability heuristic can influence the probability judgement of the ratio of sharks and dolphins in the water. Thus, an individual who saw both a shark and a dolphin would assume a higher ratio of sharks in the water, even if there are more dolphins in reality.

One of the earliest and most powerful critiques of the original Tversky and Kahneman study on the availability heuristic was the Schwarz et al. study which found that the ease of recall was a key component in determining whether a concept became available.  Many studies since this criticism of the original availability heuristic model have repeated this initial criticism, that the ease of recall factor became an integral facet of the availability heuristic itself (see Research section).

Much of the criticism against the availability heuristic has claimed that making use of the content that becomes available in our mind is not based on the ease of recall as suggested by Schwarz et al.  For example, it could be argued that recalling more words that begin with K than words with the third letter being K could arise from how we categorize and process words into our memory.  If we categorize words by the first letter and recall them through the same process, this would show more support for the representative heuristic than the availability heuristic.  Based on the possibility of explanations such as these, some researchers have claimed that the classic studies on the availability heuristic are too vague in that they fail to account for people's underlying mental processes.  Indeed, a study conducted by Wanke et al. demonstrated this scenario can occur in situations used to test the availability heuristic.
A second line of study has shown that frequency estimation may not be the only strategy we use when making frequency judgments.  A recent line of research has shown that our situational working memory can access long-term memories, and this memory retrieval process includes the ability to determine more accurate probabilities.

In social psychology, the fundamental attribution error is a cognitive attribution bias in which observers underemphasize situational and environmental factors for the behavior of an actor while overemphasizing dispositional or personality factors. In other words, observers tend to overattribute the behaviors of others to their personality (e.g., he is late because he's selfish) and underattribute them to the situation or context (e.g., he is late because he got stuck in traffic). Although personality traits and predispositions are considered to be observable facts in psychology, the fundamental attribution error is an error because it misinterprets their effects.
The group attribution error is identical to the fundamental attribution error, where the bias is shown between members of different groups rather than different individuals.
The ultimate attribution error is a derivative of the fundamental attribution error and group attribution error relating to the actions of groups, with an additional layer of self-justification relating to whether the action of an individual is representative of the wider group.

The phrase was coined by Lee Ross 10 years after an experiment by Edward E. Jones and Victor Harris in 1967. Ross argued in a popular paper that the fundamental attribution error forms the conceptual bedrock for the field of social psychology. Jones wrote that he found Ross's phrase "overly provocative and somewhat misleading", and also joked: "Furthermore, I'm angry that I didn't think of it first." Some psychologists, including Daniel Gilbert, have used the phrase "correspondence bias" for the fundamental attribution error. Other psychologists have argued that the fundamental attribution error and correspondence bias are related but independent phenomena, with the former being a common explanation for the latter.

Jones and Harris hypothesized, based on the correspondent inference theory, that people would attribute apparently freely chosen behaviors to disposition and apparently chance-directed behaviors to situation. The hypothesis was confounded by the fundamental attribution error.
Subjects in an experiment read essays for and against Fidel Castro. Then they were asked to rate the pro-Castro attitudes of the writers. When the subjects believed that the writers freely chose positions for or against Castro, they would normally rate the people who praised Castro as having a more positive attitude towards Castro. However, contradicting Jones and Harris' initial hypothesis, when the subjects were told that the writers' positions were determined by a coin toss, they still rated writers who spoke in favor of Castro as having, on average, a more positive attitude towards Castro than those who spoke against him. In other words, the subjects were unable to properly see the influence of the situational constraints placed upon the writers; they could not refrain from attributing sincere belief to the writers. The experimental group provided more internal attributions towards the writer.

The hypothesis that people systematically overattribute behavior to traits (at least for other people's behavior) is contested. A 1986 study tested whether subjects over-, under-, or correctly estimated the empirical correlation among behaviors (i.e., traits, see trait theory). They found that estimates of correlations among behaviors correlated strongly with empirically-observed correlations among these behaviors. Subjects were sensitive to even very small correlations, and their confidence in the association tracked how far they were discrepant (i.e., if they knew when they did not know), and was higher for the strongest relations. Subjects also showed awareness of the effect of aggregation over occasions and used reasonable strategies to arrive at decisions. Epstein concluded that "Far from being inveterate trait believers, as has been previously suggested, [subjects'] intuitions paralleled psychometric principles in several important respects when assessing relations between real-life behaviors."
A 2006 meta-analysis found little support for a related bias, the actor–observer asymmetry, in which people attribute their own behavior more to the environment, but others' behavior to individual attributes. The implications for the fundamental attribution error, the author explained, were mixed. He explained that the fundamental attribution error has two versions:  
Observers tend to explain an actor's behavior with dispositional rather than environmental explanations;
Observers tend to draw conclusions about an actor's stable disposition based on the actor's behavior in a given situation.
The author of the meta-analysis concluded that the existing weight of evidence does not support the first form of the fundamental attribution error, but does not contradict the second.
In 2015, the fundamental attribution error was contested once again in an argument against the measures originally used from the 1967 demonstration study done by Jones and Harris, and the 1982 study done by Quattrone. In this argument, the authors posed that the degree to which behaviour is constrained by a situation is a vital determinant of whether or not a dispositional attribution will be made. Since situations are undeniably complex and are of different "strengths", this will interact with an individual's disposition and determine what kind of attribution is made; although some amount of attribution can consistently be allocated to disposition, the way in which this is balanced with situational attribution will be dependent on the kind of situation one is in and the information available in said situation. The authors analyzing the 2015 study claimed that the results found in the traditional fundamental attribution error studies were "interpreted as biased only because they have been compared to an inappropriate benchmark of rationality predicated on the assumption of deterministic dispositions and situations."

Several theories predict the fundamental attribution error, and thus both compete to explain it, and can be falsified if it does not occur. Some examples include:
Just-world fallacy. The belief that people get what they deserve and deserve what they get, the concept of which was first theorized by Melvin J. Lerner in 1977. Attributing failures to dispositional causes rather than situational causes—which are unchangeable and uncontrollable—satisfies our need to believe that the world is fair and that we have control over our lives. We are motivated to see a just world because this reduces our perceived threats, gives us a sense of security, helps us find meaning in difficult and unsettling circumstances, and benefits us psychologically. However, the just-world fallacy also results in a tendency for people to blame and disparage victims of an accident or a tragedy, such as rape and domestic abuse, to reassure themselves of their insusceptibility to such events. People may even blame the victim's faults in a "past life" to pursue justification for their bad outcome.
Salience of the actor. We tend to attribute an observed effect to potential causes that capture our attention. When we observe other people, the person is the primary reference point while the situation is overlooked as if it is nothing but mere background. As such, attributions for others' behavior are more likely to focus on the person we see, not the situational forces acting upon that person that we may not be aware of. (When we observe ourselves, we are more aware of the forces acting upon us. Such a differential inward versus outward orientation accounts for the actor–observer bias.)
Lack of effortful adjustment. Sometimes, even though we are aware that the person's behavior is constrained by situational factors, we still commit the fundamental attribution error. This is because we do not take into account behavioral and situational information simultaneously to characterize the dispositions of the actor. Initially, we use the observed behavior to characterize the person by automaticity. We need to make deliberate and conscious effort to adjust our inference by considering the situational constraints. Therefore, when situational information is not sufficiently taken into account for adjustment, the uncorrected dispositional inference creates the fundamental attribution error. This would also explain why people commit the fundamental attribution error to a greater degree when they're under cognitive load; i.e. when they have less motivation or energy for processing the situational information.
Culture. It has been suggested cultural differences occur in attribution error: people from individualistic (Western) cultures are reportedly more prone to the error while people from collectivistic cultures are less prone.  Based on cartoon-figure presentations to Japanese and American subjects, it has been suggested that collectivist subjects may be more influenced by information from context (for instance being influenced more by surrounding faces in judging facial expressions). Alternatively, individualist subjects may favor processing of focal objects, rather than contexts.  Others suggest Western individualism is associated with viewing both oneself and others as independent agents, therefore focusing more on individuals rather than contextual details. Another study found that in contrast to American children emphasizing dispositional factors to explain an event, Hindu children from India were also found to rely more on situational factors. This is due to individualistic cultures normalizing only valuing traits of each person, such as their skills, achievements, unique interests, and more. On the other hand, those in collectivistic cultures view each individual in terms of their social role, viewing them as valuable parts of a group. In these contexts, it is normalized to view decision making in terms of what benefits the larger group and aligns with social norms rather than one's own opinion. Based on these differences, participants in these studies mostly rely on aspects learned by their respective cultures, when making attributions.

The fundamental attribution error is commonly used interchangeably with "correspondence bias" (sometimes called "correspondence inference"), although this phrase refers to a judgment which does not necessarily constitute a bias, which arises when the inference drawn is incorrect, e.g., dispositional inference when the actual cause is situational. However, there has been debate about whether the two terms should be distinguished from each other. Three main differences between these two judgmental processes have been argued:
They seem to be elicited under different circumstances, as both correspondent dispositional inferences and situational inferences can be elicited spontaneously. Attributional processing, however, seems to only occur when the event is unexpected or conflicting with prior expectations. This notion is supported by a 1994 study, which found that different types of verbs invited different inferences and attributions. Correspondence inferences were invited to a greater degree by interpretative action verbs (such as "to help") than state action or state verbs, thus suggesting that the two are produced under different circumstances.
Correspondence inferences and causal attributions also differ in automaticity. Inferences can occur spontaneously if the behavior implies a situational or dispositional inference, while causal attributions occur much more slowly.
It has also been suggested that correspondence inferences and causal attributions are elicited by different mechanisms. It is generally agreed that correspondence inferences are formed by going through several stages. Firstly, the person must interpret the behavior, and then, if there is enough information to do so, add situational information and revise their inference. They may then further adjust their inferences by taking into account dispositional information as well. Causal attributions however seem to be formed either by processing visual information using perceptual mechanisms, or by activating knowledge structures (e.g. schemas) or by systematic data analysis and processing. Hence, due to the difference in theoretical structures, correspondence inferences are more strongly related to behavioral interpretation than causal attributions.
Based on the preceding differences between causal attribution and correspondence inference, some researchers argue that the fundamental attribution error should be considered as the tendency to make dispositional rather than situational explanations for behavior, whereas the correspondence bias should be considered as the tendency to draw correspondent dispositional inferences from behavior. With such distinct definitions between the two, some cross-cultural studies also found that cultural differences of correspondence bias are not equivalent to those of fundamental attribution error. While the latter has been found to be more prevalent in individualistic cultures than collectivistic cultures, correspondence bias occurs across cultures, suggesting differences between the two phrases. Further, disposition correspondent inferences made to explain the behavior of nonhuman actors (e.g., robots) do not necessarily constitute an attributional error because there is little meaningful distinction between the interior dispositions and observable actions of machine agents.

The fundamental attribution error is a multifaceted topic with many backgrounds of study. Some studies have looked deeper into the impacts of external environmental factors on this error, these are some of the factors researchers have found:
Contextual Influences: The context in which automation is used plays a crucial role. Situational factors, such as the presence of guidelines or prior knowledge about the system, can shape responsibility judgments. The more control one believes they have in a situation the more likely they are to attribute responsibility to themselves.
Cultural Values: Values such as individualism versus collectivism, can lead to different cognitive approaches, which in turn affects how judgements are made. Cultural backgrounds may have an influence on casual attribution, those raised in different cultural contexts could have varying perspectives on the causes of behavior and performance.
Feedback Mechanisms: When individuals receive feedback on their judgments, it often serves to confirm their initial biases, leading them to continue attributing behaviors of others to internal factors rather than considering situational influences. This feedback process could create a reinforcing loop, where individuals grow more confident in their internal attributions and less likely to incorporate external factors that may offer a more accurate explanation of behavior.

The Dunning–Kruger effect is a cognitive bias that describes the systematic tendency of people with low ability in a specific area to give overly positive assessments of this ability. The term may also describe the tendency of high performers to underestimate their skills. It was first described by the psychologists David Dunning and Justin Kruger in 1999. In popular culture, the Dunning–Kruger effect is sometimes misunderstood as claiming that people with low intelligence are generally overconfident, instead of describing the specific overconfidence of people unskilled at particular areas. 
The Dunning–Kruger effect has been demonstrated across multiple studies in a wide range of tasks from fields such as business, politics, medicine, driving, aviation, spatial memory, examinations in school, and literacy. The original study by Dunning and Kruger focused on logical reasoning, grammar, and social skills. The effect is usually measured by comparing self-assessment with objective performance. For example, participants may take a quiz and estimate their performance afterward, and their estimates are then compared to their actual results.  
A number of explanations for, and criticisms of, the Dunning–Kruger effect have been proposed. The metacognitive explanation holds that poor performers misjudge their abilities because they lack the ability to recognize the qualitative difference between their performances and the performances of others. The statistical explanation holds that the empirical effect may largely be the result of a mere statistical effect and the fact that people have a general tendency to think that one is better than average. The rational explanation holds that overly positive prior beliefs about one's skills are the source of false self-assessment. Another explanation claims that self-assessment is more difficult and error-prone for low performers because many of them have very similar skill levels.
There is also disagreement about where the effect applies and about how strong it is, as well as about the practical consequences of the effect. Inaccurate self-assessment could potentially lead people to making bad decisions, such as choosing a career for which they are unfit, engaging in dangerous behavior, and inhibiting people from addressing their shortcomings to improve themselves.

The Dunning–Kruger effect is the tendency of people with low ability in a specific area to give overly positive assessments of this ability. This is often seen as a cognitive bias, that is, a systematic tendency to engage in erroneous forms of thinking and judging. In the case of the Dunning–Kruger effect, the systematic error concerns people with low skill in a specific area trying to evaluate their competence within this area and their tendency to greatly overestimate their competence.
The Dunning–Kruger effect is usually defined specifically for the self-assessments of people with a low level of competence. But some theorists do not only restrict it to the bias of people with low skill but also use it to describe the reverse effect, the tendency of highly skilled people to underestimate their abilities relative to the abilities of others. In this case, the source of the error may not be the self-assessment of one's skills, but an overly positive assessment of the skills of others. This phenomenon can be understood as a form of the false-consensus effect, the tendency to "overestimate the extent to which other people share one's beliefs, attitudes, and behaviours".
Some researchers include a metacognitive component in their definition. In this view, the Dunning–Kruger effect is the thesis that those who are incompetent in a given area tend to be ignorant of their incompetence; they lack the metacognitive ability to become aware of their incompetence. As incompetence often includes being unable to tell the difference between competence and incompetence, it is difficult for the incompetent to recognize their incompetence. This is sometimes termed the "dual-burden" account, since low performers are affected by two burdens: they lack a skill and they are unaware of this deficiency. Other definitions focus on the tendency to overestimate one's ability and see the relation to metacognition as a possible explanation that is not part of the definition. This contrast is relevant since the metacognitive explanation is controversial. Many criticisms of the Dunning–Kruger effect target this explanation but accept the empirical findings that low performers tend to overestimate their skills.
Among laypeople, the Dunning–Kruger effect is often misunderstood as the claim that people with low intelligence are more confident in their knowledge and skills than people with high intelligence. According to psychologist Robert D. McIntosh and his colleagues, it is sometimes understood in popular culture as the claim that "stupid people are too stupid to know they are stupid". But the Dunning–Kruger effect applies not to intelligence in general but to skills in specific tasks. Nor does it claim that people lacking a given skill are as confident as high performers. Rather, low performers overestimate themselves but their confidence level is still below that of high performers.

The most common approach to measuring the Dunning–Kruger effect is to compare self-assessment with objective performance. The self-assessment is sometimes called subjective ability in contrast to the objective ability corresponding to the actual performance. The self-assessment may be done before or after the performance. If done afterward, the participants receive no independent clues during the performance as to how well they did. Thus, if the activity involves answering quiz questions, no feedback is given as to whether a given answer was correct. 
The measurement of the subjective and the objective abilities can be in absolute or relative terms. When done in absolute terms, self-assessment and performance are measured according to objective standards, e.g. concerning how many quiz questions were answered correctly. When done in relative terms, the results are compared with a peer group. In this case, participants are asked to assess their performances in relation to the other participants, for example in the form of estimating the percentage of peers they outperformed. The Dunning–Kruger effect is present in both cases, but tends to be significantly more pronounced when done in relative terms; people are usually less accurate when assessing how well they did relative to their peer group than when simply predicting their raw score.
The main point of interest for researchers is usually the correlation between subjective and objective ability. To provide a simplified form of analysis of the measurements, objective performances are often divided into four groups. They start from the bottom quartile of low performers and proceed to the top quartile of high performers. The strongest effect is seen for the participants in the bottom quartile, who tend to see themselves as being part of the top two quartiles when measured in relative terms.
The initial study by David Dunning and Justin Kruger examined the performance and self-assessment of undergraduate students in inductive, deductive, and abductive logical reasoning; English grammar; and appreciation of humor. Across four studies, the research indicates that the participants who scored in the bottom quartile overestimated their test performance and their abilities. Their test scores placed them in the 12th percentile, but they ranked themselves in the 62nd percentile. Other studies focus on how a person's self-view causes inaccurate self-assessments. Some studies indicate that the extent of the inaccuracy depends on the type of task and can be improved by becoming a better performer.
Overall, the Dunning–Kruger effect has been studied across a wide range of tasks, in aviation, business, debating, chess, driving, literacy, medicine, politics, spatial memory, and other fields. Many studies focus on students—for example, how they assess their performance after an exam. In some cases, these studies gather and compare data from different countries. Studies are often done in laboratories; the effect has also been examined in other settings, including assessments of hunters' knowledge of firearms and large Internet surveys.

Various theorists have tried to provide models to explain the Dunning–Kruger effect's underlying causes. The original explanation by Dunning and Kruger holds that a lack of metacognitive abilities is responsible. This interpretation is not universally accepted, and many alternative explanations have been proposed. Some of them focus only on one specific factor, while others see a combination of various factors as the cause.

The metacognitive explanation rests on the idea that part of acquiring a skill consists in learning to distinguish between good and bad performances of the skill. It assumes that people of low skill level are unable to properly assess their performance because they have not yet acquired the discriminatory ability to do so. This leads them to believe that they are better than they actually are because they do not see the qualitative difference between their performance and that of others. In this regard, they lack the metacognitive ability to recognize their incompetence. This model has also been called the "dual-burden account" or the "double-burden of incompetence", since the burden of regular incompetence is paired with the burden of metacognitive incompetence. The metacognitive lack may hinder some people from becoming better by hiding their flaws from them. This can then be used to explain how self-confidence is sometimes higher for unskilled people than for people with an average skill: only the latter are aware of their flaws.
Some attempts have been made to measure metacognitive abilities directly to examine this hypothesis. Some findings suggest that poor performers have reduced metacognitive sensitivity, but it is not clear that its extent is sufficient to explain the Dunning–Kruger effect. Another study concluded that unskilled people lack information but that their metacognitive processes have the same quality as those of skilled people. An indirect argument for the metacognitive model is based on the observation that training people in logical reasoning helps them make more accurate self-assessments. Many criticisms of the metacognitive model hold that it has insufficient empirical evidence and that alternative models offer a better explanation.

A different interpretation is further removed from the psychological level and sees the Dunning–Kruger effect as mainly a statistical artifact. It is based on the idea that the statistical effect known as regression toward the mean explains the empirical findings. This effect happens when two variables are not perfectly correlated: if one picks a sample that has an extreme value for one variable, it tends to show a less extreme value for the other variable. For the Dunning–Kruger effect, the two variables are actual performance and self-assessed performance. If a person with low actual performance is selected, their self-assessed performance tends to be higher.
Most researchers acknowledge that regression toward the mean is a relevant statistical effect that must be taken into account when interpreting the empirical findings. This can be achieved by various methods. Some theorists, like Gilles Gignac and Marcin Zajenkowski, go further and argue that regression toward the mean in combination with other cognitive biases, like the better-than-average effect, can explain most of the empirical findings. This type of explanation is sometimes called "noise plus bias".
According to the better-than-average effect, people generally tend to rate their abilities, attributes, and personality traits as better than average. For example, the average IQ is 100, but people on average think their IQ is 115. The better-than-average effect differs from the Dunning–Kruger effect since it does not track how the overly positive outlook relates to skill. The Dunning–Kruger effect, on the other hand, focuses on how this type of misjudgment happens for poor performers. When the better-than-average effect is paired with regression toward the mean, it shows a similar tendency. This way, it can explain both that unskilled people greatly overestimate their competence and that the reverse effect for highly skilled people is much less pronounced. This can be shown using simulated experiments that have almost the same correlation between objective and self-assessed ability as actual experiments.
Some critics of this model have argued that it can explain the Dunning–Kruger effect only when assessing one's ability relative to one's peer group. But it may not be able to explain self-assessment relative to an objective standard. A further objection claims that seeing the Dunning–Kruger effect as a regression toward the mean is only a form of relabeling the problem and does not explain what mechanism causes the regression.
Based on statistical considerations, Nuhfer et al. arrive at the conclusion that there is no strong tendency to overly positive self-assessment and that the label "unskilled and unaware of it" applies only to few people. Science communicator Jonathan Jarry makes the case that this effect is the only one shown in the original and subsequent papers. Dunning has defended his findings, writing that purely statistical explanations often fail to consider key scholarly findings while adding that self-misjudgements are real regardless of their underlying cause.

The rational model of the Dunning–Kruger effect explains the observed regression toward the mean not as a statistical artifact but as the result of prior beliefs. If low performers expect to perform well, this can cause them to give an overly positive self-assessment. This model uses a psychological interpretation that differs from the metacognitive explanation. It holds that the error is caused by overly positive prior beliefs and not by the inability to correctly assess oneself. For example, after answering a ten-question quiz, a low performer with only four correct answers may believe they got two questions right and five questions wrong, while they are unsure about the remaining three. Because of their positive prior beliefs, they will automatically assume that they got these three remaining questions right and thereby overestimate their performance.

Another model sees the way high and low performers are distributed as the source of erroneous self-assessment. It is based on the assumption that many low performers' skill levels are very similar, i.e., that "many people [are] piled up at the bottom rungs of skill level". This would make it much more difficult for them to accurately assess their skills in relation to their peers. According to this model, the reason for the increased tendency to give false self-assessments is not a lack of metacognitive ability but a more challenging situation in which this ability is applied. One criticism of this interpretation is directed against the assumption that this type of distribution of skill levels can always be used as an explanation. While it can be found in various fields where the Dunning–Kruger effect has been researched, it is not present in all of them. Another criticism holds that this model can explain the Dunning–Kruger effect only when the self-assessment is measured relative to one's peer group, but that it may fail when it is measured relative to absolute standards.

A further explanation, sometimes given by theorists with an economic background, focuses on the fact that participants in the corresponding studies lack incentive to give accurate self-assessments. In such cases, intellectual laziness or a desire to look good to the experimenter may motivate participants to give overly positive self-assessments. For this reason, some studies were conducted with additional incentives to be accurate. One study gave participants a monetary reward based on how accurate their self-assessments were. These studies failed to show any significant increase in accuracy for the incentive group in contrast to the control group.

There are disagreements about the Dunning–Kruger effect's magnitude and practical consequences as compared to other psychological effects. Claims about its significance often focus on how it causes affected people to make decisions that have bad outcomes for them or others. For example, according to Gilles E. Gignac and Marcin Zajenkowski, it can have long-term consequences by leading poor performers into careers for which they are unfit. High performers underestimating their skills, though, may forgo viable career opportunities matching their skills in favor of less promising ones that are below their skill level. In other cases, the wrong decisions can also have short-term effects. For example, Pavel et al. hold that overconfidence can lead pilots to operate a new aircraft for which they lack adequate training or to engage in flight maneuvers that exceed their proficiency.
Emergency medicine is another area where the correct assessment of one's skills and the risks of treatment matters. According to Lisa TenEyck, the tendencies of physicians in training to be overconfident must be considered to ensure the appropriate degree of supervision and feedback. Schlösser et al. hold that the Dunning–Kruger effect can also negatively affect economic activities. This is the case, for example, when the price of a good, such as a used car, is lowered by the buyers' uncertainty about its quality. An overconfident buyer unaware of their lack of knowledge may be willing to pay a much higher price because they do not take into account all the potential flaws and risks relevant to the price.
Another implication concerns fields in which researchers rely on people's self-assessments to evaluate their skills. This is common, for example, in vocational counseling or to estimate students' and professionals' information literacy skills. According to Khalid Mahmood, the Dunning–Kruger effect indicates that such self-assessments often do not correspond to the underlying skills. It implies that they are unreliable as a method for gathering this type of data. Regardless of the field in question, the metacognitive ignorance often linked to the Dunning–Kruger effect may inhibit low performers from improving themselves. Since they are unaware of many of their flaws, they may have little motivation to address and overcome them.
Not all accounts of the Dunning–Kruger effect focus on its negative sides. Some also concentrate on its positive sides, e.g. that ignorance is sometimes bliss. In this sense, optimism can lead people to experience their situation more positively, and overconfidence may help them achieve even unrealistic goals. To distinguish the negative from the positive sides, two important phases have been suggested to be relevant for realizing a goal: preparatory planning and the execution of the plan. According to Dunning, overconfidence may be beneficial in the execution phase by increasing motivation and energy. However it can be detrimental in the planning phase since the agent may ignore bad odds, take unnecessary risks, or fail to prepare for contingencies. For example, being overconfident may be advantageous for a general on the day of battle because of the additional inspiration passed on to his troops. But it can be disadvantageous in the weeks before by ignoring the need for reserve troops or additional protective gear.
Historical precursors of the Dunning–Kruger effect were expressed by theorists such as Charles Darwin ("Ignorance more frequently begets confidence than does knowledge") and Bertrand Russell ("...in the modern world the stupid are cocksure while the intelligent are full of doubt"). In 2000, Kruger and Dunning were awarded the satirical Ig Nobel Prize in recognition of the scientific work recorded in "their modest report".

Hindsight bias, also known as the knew-it-all-along phenomenon or creeping determinism, is the common tendency for people to perceive past events as having been more predictable than they were.
After an event has occurred, people often believe that they could have predicted or perhaps even known with a high degree of certainty what the outcome of the event would be before it occurred. Hindsight bias may cause distortions of memories of what was known or believed before an event occurred and is a significant source of overconfidence in one’s ability to predict the outcomes of future events. Examples of hindsight bias can be seen in the writings of historians describing the outcomes of battles, in physicians’ recall of clinical trials, and in criminal or civil trials as people tend to assign responsibility on the basis of the supposed predictability of accidents.

The hindsight bias, although it was not yet named, was not a new concept when it emerged in psychological research in the 1970s. In fact, it had been indirectly described numerous times by historians, philosophers, and physicians. In 1973, Baruch Fischhoff attended a seminar where Paul E. Meehl stated an observation: clinicians often overestimate their ability to have foreseen the outcome of a particular case, claiming they knew it all along. Baruch, a psychology graduate student at the time, saw an opportunity in psychological research to explain this tendency.
In the early 70s, the investigation of heuristics and biases was a large area of study in psychology, led by Amos Tversky and Daniel Kahneman. Two heuristics identified by Tversky and Kahneman were of immediate importance in the development of the hindsight bias; these were the availability heuristic and the representativeness heuristic.
In an elaboration of these heuristics, Beyth and Fischhoff devised the first experiment directly testing the hindsight bias. They asked participants to judge the likelihood of several outcomes of US president Richard Nixon's upcoming visit to Beijing and Moscow. Some time after president Nixon's return, participants were asked to recall (or reconstruct) the probabilities they had assigned to each possible outcome, and their perceptions of the likelihood of each outcome were greater or overestimated for events that had occurred. This study is frequently referred to in definitions of the hindsight bias, and the title of the paper, "I knew it would happen," may have contributed to the hindsight bias being interchangeable with the phrase, "knew-it-all-along phenomenon".
In 1975, Fischhoff developed another method for investigating the hindsight bias, which was, at the time, referred to as the "creeping determinism hypothesis". This method involves giving participants a short story with four possible outcomes, one of which they are told is true, and are then asked to assign the likelihood of each particular outcome. Participants frequently assign a higher likelihood of occurrence to whichever outcome they have been told is true. Remaining relatively unmodified, this method is still used in psychological and behavioural experiments investigating aspects of the hindsight bias. Having evolved from the heuristics of Tversky and Kahneman into the creeping determinism hypothesis and finally into the hindsight bias as we now know it, the concept has many practical applications and is still at the forefront of research today. Recent studies involving the hindsight bias have investigated the effect age has on the bias, how hindsight may impact interference and confusion, and how it may affect banking and investment strategies.

Hindsight bias is more likely to occur when the outcome of an event is negative rather than positive. This is a phenomenon consistent with the general tendency for people to pay more attention to negative outcomes of events than positive outcomes.
In addition, hindsight bias is affected by the severity of the negative outcome. In malpractice lawsuits, it has been found that the more severe a negative outcome is, the juror's hindsight bias is more dramatic. In a perfectly objective case, the verdict would be based on the physician's standard of care instead of the outcome of the treatment; however, studies show that cases ending in severe negative outcomes (such as death) result in a higher level of hindsight bias.
For example, in 1996, LaBine proposed a scenario where a psychiatric patient told a therapist that he was contemplating harming another individual. The therapist did not warn the other individual of the possible danger. Participants were each given one of three possible outcomes; the threatened individual either received no injuries, minor injuries, or serious injuries. Participants were then asked to determine if the physician should be considered negligent. Participants in the "serious injuries" condition were not only more likely to rate the therapist as negligent but also rated the attack as more foreseeable. Participants in the no injuries and minor injury categories were more likely to see the therapist's actions as reasonable.

The role of surprise can help explain the malleability of hindsight bias. Surprise influences how the mind reconstructs pre-outcome predictions in three ways:
1. Surprise is a direct metacognitive heuristic to estimate the distance between outcome and prediction.
2. Surprise triggers a deliberate sense-making process.
3. Surprise biases this process ( the malleability of hindsight bias) by enhancing the recall of surprise-congruent information and expectancy-based hypothesis testing.
Pezzo's sense-making model supports two contradicting ideas of a surprising outcome. The results can show a lesser hindsight bias or possibly a reversed effect, where the individual believes the outcome was not a possibility at all.  The outcome can also lead to the hindsight bias being magnified to have a stronger effect. The sense-making process is triggered by an initial surprise. If the sense-making process is not complete and the sensory information is not detected or coded [by the individual], the sensation is experienced as a surprise and the hindsight bias has a gradual reduction. When the sense-making process is lacking, the phenomena of reversed hindsight bias is created. Without the sense-making process being present, there is no remnant of thought about the surprise.  This can lead to a sensation of not believing the outcome as a possibility.

Along with the emotion of surprise, the personality traits of an individual affect hindsight bias. A new C model is an approach to figure out the bias and accuracy in human inferences because of their individual personality traits. This model integrates accurate personality judgments and hindsight effects as a by-product of knowledge updating.
During the study, three processes showed potential to explain the occurrence of hindsight effects in personality judgments:
1. Changes in an individual's cue perceptions,
2. Changes in the use of more valid cues, and
3. Changes in the consistency with which an individual applies cue knowledge.
After two studies, it was clear that there were hindsight effects for each of the Big Five personality dimensions. Evidence was found that both the utilization of more valid cues and changes in cue perceptions of the individual, but not changes in the consistency with which cue knowledge is applied, account for the hindsight effects. During both of these studies, participants were presented with target pictures and were asked to judge each target's levels of the Big Five personality traits.
In a study of 75 participants, researchers tested 10 personalities about hindsight bias.  This study conducted three comparisons of hindsight estimation with foresight estimation (memory conditioning), hindsight estimation with forward estimation with other participants, and hindsight estimation with foresight estimation. The participants in these comparisons all Demonstrated hindsight bias.  Personality measures cannot account for memory hindsight in multiple regression analysis. Hindsight in individual differences is present but must be accounted for in the full effect model.

It is more difficult to test for hindsight bias in children than adults because the verbal methods used in experiments on adults are too complex for children to understand, let alone measure bias. Some experimental procedures have been created with visual identification to test children about their hindsight bias in a way they can grasp. Methods with visual images start by presenting a blurry image to the child that becomes clearer over time. In some conditions, the subjects know what the final object is and in others they do not. In cases where the subject knows what the object shape will become when the image is clear, they are asked to estimate the amount of time other participants of similar age will take to guess what the object is. Due to hindsight bias, the estimated times are often much lower than the actual times.  This is because the participant is using their personal knowledge while making their estimate.
These types of studies show that children are also affected by hindsight bias. Adults and children with hindsight bias share the core cognitive constraint of being biased to one's current knowledge while, at the same time, attempting to recall or reason about a more naïve cognitive state—regardless of whether the more naïve state is one's earlier naïve state or someone else's.

Hindsight bias also affects human communications. To test auditory hindsight bias, four experiments were completed. 
Experiment one included plain words, in which low-pass filters were used to reduce the amplitude for sounds of consonants; thus making the words more degraded. In the naïve-identification task, participants were presented a warning tone before hearing the degraded words. In the hindsight estimation task, a warning tone was presented before the clear word followed by the degraded version of the word. 
Experiment two included words with explicit warnings of hindsight bias. It followed the same procedure as experiment one.  However, the participants were informed and asked not to complete the same error. 
Experiment three included full sentences of degraded words rather than individual words. Experiment four included less-degraded words in order to make the words easier to understand and identify to the participants.
By using these different techniques, this offered a different range of detection and also evaluated the ecological validity of the experiment's effect. In each experiment, the hindsight estimates of the percentage that their naïve peers can correctly identify the words, all exceed the actual percentages. Therefore, knowing the identities of words caused people to overestimate others' naïve ability to identify moderately to highly degraded spoken versions of those words. People who know the outcome of an event tend to overestimate their prior knowledge or others' naïve knowledge of the event. As a result, speakers—knowing what is being communicated—tend to overestimate the clarity of their message while listeners—hearing what they want to hear—tend to overestimate their understanding of ambiguous messages. This miscommunication stems from hindsight bias which then creates a feeling of inevitability. Overall, this auditory hindsight bias occurs despite people's effort to avoid it.

To understand how a person can so easily change the foundation of knowledge and belief for events after receiving new information, three cognitive models of hindsight bias have been reviewed. 
The three models are:
SARA (Selective Activation and Reconstructive Anchoring),
RAFT (reconstruction after feedback with take the best), and
CMT (causal model theory).
SARA and RAFT focus on distortions or changes in a memory process, while CMT focuses on probability judgments of hindsight bias.
The SARA model, created by Rüdiger Pohl and associates, explains hindsight bias for descriptive information in memory and hypothetical situations. Memory design is when the participants make foresight judgements, and then recall them in hindsight.  Hypothetical design is when participants make hindsight judgements as if they had not known the outcome. SARA assumes that people have a set of images to draw their memories from. They suffer from the hindsight bias due to selective activation or biased sampling of that set of images.
Basically, people only remember small, select amounts of information—and when asked to recall it later, use that biased image to support their opinions about the situation. The set of images is originally processed in the brain when first experienced. When remembered, this image reactivates, and the mind can edit and alter the memory, which takes place in hindsight bias when new and correct information is presented, leading one to believe that this new information, when remembered at a later time, is the person's original memory. Due to this reactivation in the brain, there may be permanent changes to the existing memory trace. The new information acts as a memory anchor causing retrieval impairment.
The RAFT model explains hindsight bias with comparisons of objects. It uses knowledge-based probability and then applies interpretations to those probabilities. When given two choices, a person recalls the information on both topics and makes assumptions based on how reasonable they find the information. An example case is someone comparing the size of two cities. If they know one city well (e.g. because it has a popular sporting team or through personal history) and know much less about the other, their mental cues for the more popular city increase. They then "take the best" option in their assessment of their probabilities. For example, they recognize a city due to knowing its sports team, and thus they assume that that city has the highest population. "Take the best" refers to a cue that is viewed as most valid and becomes support for the person's interpretations. RAFT is a by-product of adaptive learning. Feedback information updates a person's knowledge base. This can lead a person to be unable to retrieve the initial information because the information cue has been replaced by a cue that they thought was more fitting. The "best" cue has been replaced, and the person only remembers the answer that is most likely and believes they thought this was the best point the whole time.
Both SARA and RAFT descriptions include a memory trace impairment or cognitive distortion that is caused by the feedback of information and reconstruction of memory.
CMT is a non-formal theory based on work by many researchers to create a collaborative process model for hindsight bias that involves event outcomes. People try to make sense of an event that has not turned out how they expected by creating causal reasoning for the starting event conditions. This can give that person the idea that the event outcome was inevitable and there was nothing that could take place to prevent it from happening. CMT can be caused by a discrepancy between a person's expectation of the event and the reality of an outcome. They consciously want to make sense of what has happened and selectively retrieve memory that supports the current outcome. This causal attribution can be motivated by wanting to feel more positive about the outcome and possibly themselves.

Hindsight bias has similarities to other memory distortions, such as misinformation effect and false autobiographical memory. Misinformation effect occurs after an event is witnessed; new information received after the fact influences how the person remembers the event, and can be called post-event misinformation. This is an important issue with eyewitness testimony. False autobiographical memory takes place when suggestions or additional outside information is provided to distort and change memory of events; this can also lead to false memory syndrome. At times this can lead to the creation of new memories that are completely false and have not taken place.
All three of these memory distortions contain a three-stage procedure. The details of each procedure are different, but all three can result in some form of psychological manipulation and alteration of memory. Stage one is different between the three paradigms, although all involve an event, an event that has taken place (misinformation effect), an event that has not taken place (false autobiographical memory), and a judgement made by a person about an event that must be remembered (hindsight bias). Stage two consists of more information that is received by the person after the event has taken place. The new information given in hindsight bias is correct and presented upfront to the person, while the extra information for the other two memory distortions is wrong and presented in an indirect and possibly manipulative way. The third stage consists of recalling the starting information. The person must recall the original information with hindsight bias and misinformation effect, while a person that has a false autobiographical memory is expected to remember the incorrect information as a true memory.
Cavillo (2013) tested whether there is a relationship between the amount of time the people performing the experiment gave the participants to respond and the participant's level of bias when recalling their initial judgements. The results showed that there is in fact a relationship; the hindsight bias index was greater among the participants who were asked to respond more rapidly than among the participants who were allowed more time to respond.
Distortions of autobiographical memory produced by hindsight bias have also been used as a tool to study changes in students' beliefs about paranormal phenomena after taking a university level skepticism course.  In a study by Kane (2010), students in Kane's skepticism class rated their level of belief in a variety of paranormal phenomena at both the beginning and at the end of the course.  At the end of the course, they also rated what they remembered their level of belief had been at the beginning of the course.  The critical finding was that not only did students reduce their average level of belief in paranormal phenomena by the end of the course, but they also falsely remembered the level of belief they held at the beginning of the course, recalling a much lower level of belief than what they had initially rated. It is the latter finding that is a reflection of the operation of hindsight bias.
To create a false autobiographical memory, the person must believe a memory that is not real. To seem real, the information must be influenced by their personal judgments. There is no real episode of an event to remember, so this memory construction must be logical to that person's knowledge base. Hindsight bias and the misinformation effect recall a specific time and event; this is called an episodic memory process. These two memory distortions both use memory-based mechanisms that involve a memory trace that has been changed. Hippocampus activation takes place when an episodic memory is recalled. The memory is then available for alteration by new information. The person believes that the recalled information is the original memory trace, not an altered memory. This new memory is made from accurate information, and, therefore, the person does not have much motivation to admit that they were wrong originally by remembering the original memory. This can lead to motivated forgetting.

Following a negative outcome of a situation, people do not want to accept responsibility. Instead of accepting their role in the event, they might either view themselves as caught up in a situation that was unforeseeable with them therefore not being the culprits (this is referred to as defensive processing) or view the situation as inevitable with there therefore being nothing that could have been done to prevent it (this is retroactive pessimism). Defensive processing involves less hindsight bias, as they are playing ignorant of the event. Retroactive pessimism makes use of hindsight bias after a negative, unwanted outcome. Events in life can be hard to control or predict. It is no surprise that people want to view themselves in a more positive light and do not want to take responsibility for situations they could have altered. This leads to hindsight bias in the form of retroactive pessimism to inhibit upward counterfactual thinking, instead interpreting the outcome as succumbing to an inevitable fate.
This memory inhibition that prevents a person from recalling what happened may lead to failure to accept mistakes, and therefore may make someone unable to learn and grow to prevent repeating the mistake. Hindsight bias can also lead to overconfidence in decisions without considering other options. Such people see themselves as persons who remember correctly, even though they are just forgetting that they were wrong. Avoiding responsibility is common among the human population. Examples are discussed below to show the regularity and severity of hindsight bias in society.

Hindsight bias has both positive and negative consequences. The bias also plays a role in the process of decision-making within the medical field.

Positive consequences of hindsight bias is an increase in one's confidence and performance, as long as the bias distortion is reasonable and does not create overconfidence. Another positive consequence is that one's self-assurance of their knowledge and decision-making, even if it ends up being a poor decision, can be beneficial to others; allowing others to experience new things or to learn from those who made the poor decisions.

Hindsight bias causes overconfidence in one's performance relative to others. Hindsight bias decreases one's rational thinking because of when a person experiences strong emotions, which in turn decreases rational thinking. Another negative consequence of hindsight bias is the interference of one's ability to learn from experience, as a person is unable to look back on past decisions and learn from mistakes. A third consequence is a decrease in sensitivity toward a victim by the person who caused the wrongdoing. The person demoralizes the victim and does not allow for a correction of behaviors and actions.

Hindsight bias may lead to overconfidence and malpractice in regards to physicians. Hindsight bias and overconfidence is often attributed to the number of years of experience the physician has. After a procedure, physicians may have a "knew it the whole time" attitude, when in reality they may not have known it. Medical decision support systems are designed to assist physicians in diagnosis and treatment, and have been suggested as a way to counteract hindsight bias. However, these decision support systems come with drawbacks, as going against a recommended decision resulted in more punitive jury outcomes when physicians were found liable for causing harm.

Hindsight bias has also been found to affect judgments regarding the perception of visual stimuli, an effect referred to as the "I saw it all along" phenomenon.  This effect  has been demonstrated experimentally  by presenting participants with initially very blurry images of celebrities.  Participants then viewed the images as the images resolved to full clarity (Phase 1). Following Phase 1, participants predicted the level of blur at which a peer would be able to make an accurate identification of each celebrity. It was found that, now that the identity of the celebrities in each image was known, participants significantly overestimated the ease with which others would be able to identify the celebrities when the images were blurry.
The phenomenon of visual hindsight bias has important implications for a form of malpractice litigation that occurs in the field of radiology. Typically, in these cases, a radiologist is charged with having failed to detect the presence of an abnormality that was present in a radiology image.  During litigation, a different radiologist – who now knows that the image contains an abnormality – is asked to judge how likely it would be for a naive radiologist to have detected the abnormality during the initial reading of the image.  This kind of judgement directly parallels the judgments made in hindsight bias studies.  Consistent with the hindsight bias literature, it has been found that abnormalities are, in fact, more easily detected in hindsight than foresight.  In the absence of controls for hindsight bias, testifying radiologists may overestimate the ease with which the abnormality would have been detected in foresight.

Research suggests that people still exhibit the hindsight bias even when they are aware of it or possess the intention of eradicating it. There is no solution to eliminate hindsight bias in its totality, but only ways to reduce it. Some of these include considering alternative explanations or opening one's mind to different perspectives. The only observable way to decrease hindsight bias in testing is to have the participant think about how alternative hypotheses could be correct. As a result, the participant would doubt the correct hypothesis and report not having chosen it.
Given that researchers' attempts to eliminate hindsight bias have failed, some believe there is a possible combination of motivational and automatic processes in cognitive reconstruction. Incentive prompts participants to use more effort to recover even the weak memory traces. This idea supports the causal model theory and the use of sense-making to understand event outcomes.

A multinomial processing tree (MPT) model was used to identify processes underlying the phenomenon of hindsight bias (HB). A 2015 study extended HB by incorporating individual differences in cognitive function into estimates of core parameters of the model for older and younger adults, the MPT model.  The findings suggest that (1) in the absence of outcome knowledge, better episodic memory is associated with higher recall, (2) Better episodic memory and inhibitory control and higher working memory abilities were associated with higher recall abilities in the presence of knowledge of the outcome, (3) Better inhibitory control is associated with less reconstruction bias. Despite a similar pattern of effects in young adults, cognitive covariates did not significantly predict the underlying HB process in this age group.  Overall, the findings of this study suggest that working memory capacity and inhibitory control contribute to individual differences in recall bias and reconstruction bias, respectively, especially in older adults.

Schizophrenia is an example of a disorder that directly affects the hindsight bias. Individuals with schizophrenia are more strongly affected by the hindsight bias than are individuals from the general public.
The hindsight bias effect is a paradigm that demonstrates how recently acquired knowledge influences the recollection of past information. Recently acquired knowledge has a strange but strong influence on schizophrenic individuals in relation to information previously learned. New information combined with rejection of memories can disconfirm behavior and delusional belief, which is typically found in patients with schizophrenia. This can cause faulty memory, which can lead to hindsight thinking and believing in knowing something they do not. Delusion-prone individuals with schizophrenia can falsely jump to conclusions. Jumping to conclusions can lead to hindsight, which strongly influences the delusional conviction in individuals with schizophrenia. In numerous studies, cognitive functional deficits in schizophrenic individuals impair their ability to represent and uphold contextual processing.

Post-traumatic stress disorder (PTSD) is the re-experiencing and avoidance of trauma-related stressors, emotions, and memories from a past event or events that has cognitive dramatizing impact on an individual. PTSD can be attributed to the functional impairment of the prefrontal cortex (PFC) structure. Dysfunctions of cognitive processing of context and abnormalities that PTSD patients often have can affect hindsight thinking, such as in combat soldiers perceiving they could have altered outcomes of events in war. The PFC and dopamine systems are parts of the brain that can be responsible for the impairment in cognitive control processing of context information. The PFC is well known for controlling the thought process in hindsight bias that something will happen when it evidently does not. Brain impairment in certain brain regions can also affect the thought process of an individual who may engage in hindsight thinking.
Cognitive flashbacks and other associated features from a traumatic event can trigger severe stress and negative emotions such as unpardonable guilt. For example, studies were done on trauma-related guilt characteristics of war veterans with chronic PTSD. Although there has been limited research, significant data suggests that hindsight bias has an effect on war veterans' personal perception of wrongdoing, in terms of guilt and responsibility from traumatic events of war. They blame themselves, and, in hindsight, perceive that they could have prevented what happened.

Accidents are prone to happen in any human undertaking, but accidents occurring within the healthcare system seem more salient and severe because of their profound effect on the lives of those involved and sometimes result in the death of a patient. In the healthcare system, there are a number of methods in which specific cases of accidents that happened are being reviewed by others who already know the outcome of the case. Those methods include morbidity and mortality conferences, autopsies, case analysis, medical malpractice claims analysis, staff interviews, and even patient observation. Hindsight bias has been shown to cause difficulties in measuring errors in these cases. Many of the errors are considered preventable after the fact, which clearly indicates the presence and the importance of a hindsight bias in this field. There are two sides in the debate in how these case reviews should be approached to best evaluate past cases: the error elimination strategy and the safety management strategy. The error elimination strategy aims to find the cause of errors, relying heavily on hindsight (therefore more subject to the hindsight bias). The safety management strategy relies less on hindsight (less subject to hindsight bias) and identifies possible constraints during the decision-making process of that case. However, it is not immune to error.

Hindsight bias results in being held to a higher standard in court. The defense is particularly susceptible to these effects since their actions are the ones being scrutinized by the jury. The hindsight bias causes defendants to be judged as capable of preventing the bad outcome. Although much stronger for the defendants, hindsight bias also affects the plaintiffs. In cases that there is an assumption of risk, hindsight bias may contribute to the jurors perceiving the event as riskier because of the poor outcome. That may lead the jury to feel that the plaintiff should have exercised greater caution in the situation. Both effects can be minimized if attorneys put the jury in a position of foresight, rather than hindsight, through the use of language and timelines. Judges and juries are likely to mistakenly view negative events as being more foreseeable than what it was in the moment when they look at the situation after the fact in court. Encouraging people to explicitly think about the counterfactuals was an effective means of reducing the hindsight bias. In other words, people became less attached to the actual outcome and were more open to consider alternative lines of reasoning prior to the event. Judges involved in fraudulent transfer litigation cases were subject to the hindsight bias as well and resulted in an unfair advantage for the plaintiff, showing that jurors are not the only ones sensitive to the effects of the hindsight bias in the courtroom.

Since hindsight leads people to focus on information that is consistent with what happened while inconsistent information is ignored or regarded as less relevant, it is likely included in representations about the past as well. In a study of Wikipedia articles the latest article versions before the event (foresight article versions) were compared to two hindsight article versions: the first online after the event took place and another one eight weeks later. To be able to investigate various types of events, even including disasters (such as the nuclear disaster at Fukushima), for which foresight articles do not exist, the authors made use of articles about the structure that suffered damage in those instances (such as the article about the nuclear power plant of Fukushima). When analyzing to what extent the articles were suggestive of a particular event, they found only articles about disasters to be much more suggestive of the disaster in hindsight than in foresight, which indicated hindsight bias. For the remaining event categories, however, Wikipedia articles did not show any hindsight bias. In an attempt to compare individuals' and Wikipedia's hindsight bias more directly, another study came to the conclusion that Wikipedia articles are less susceptible to hindsight bias than individuals' representations.

Financial bubbles are often heavily biased with hindsight after they burst. After the worldwide dot-com bubble of the late 1990s and the Great Recession of 2008, many economists have suggested that conditions that seemed insignificant at the time were harbingers of future financial collapse. According to economist Richard Thaler, executives and entrepreneurs are particularly prone to hindsight bias. For example, in one study, more than 75% of entrepreneurs whose startups eventually failed predicted that their businesses would succeed. However, when asked again after their startup failed, only 58% said they had originally believed their startup would be a success. Hindsight bias can also contribute to startup failure through biased performance evaluations and overentry into competition. Hindsight-biased performance evaluation is also related to inefficient delegation. Additionally, hindsight bias affects judgment of business cases in court. A study by Strohmeier et al. (2020) found, that professional legal investigators (such as judges, lawyers and bankers with legal background) are perceptible to hindsight bias when judging the foreseeability of bankruptcy and assessing the legal responsibility of the CEO. Participants were presented with one of three hypothetical bankruptcy cases, which contained either no statement about the ending of the case, a positive outcome or a negative outcome. Depending on the presented ending, judgements about whether the CEO was to blame and about the possibility to predict the outcome, were significantly different. This can possibly result in an unjustifiably stricter judgement of bankruptcy cases in real life scenarios.

Hindsight bias influences the decisions of investors in the investment sector. Investors tend to be overconfident in predicting the future because we mistakenly believe that we have predicted the present in the past, so we assume that the future will follow our predictions. Overconfidence is the killer for investment returns. Biais et al. show that hindsight bias allows people to underestimate the magnitude of volatility and that investment agents with hindsight bias have worse investment return performance. Research suggests that the main cause of hindsight bias is that no investor can remember how they made their decisions at the time. Therefore, in order to invest more rationally and safely, investors should keep a diary of the influences, outcomes and show why those outcomes were achieved when making their investment decisions. However, the diary approach largely prevents investors from forgetting conditions that have led to insight bias and overconfidence. The process of writing down the investment approach still leads to overconfidence, but the study found that it does not have an overall negative effect on current returns.

In economics and business decision-making, a sunk cost (also known as retrospective cost) is a cost that has already been incurred and cannot be recovered. Sunk costs are contrasted with prospective costs, which are future costs that may be avoided if action is taken. In other words, a sunk cost is a sum paid in the past that is no longer relevant to decisions about the future. Even though economists argue that sunk costs are no longer relevant to future rational decision-making, people in everyday life often take previous expenditures in situations, such as repairing a car or house, into their future decisions regarding those properties.

According to classical economics and standard microeconomic theory, only prospective (future) costs are relevant to a rational decision. At any moment in time, the best thing to do depends only on current alternatives. The only things that matter are the future consequences. Past mistakes are irrelevant. Any costs incurred prior to making the decision have already been incurred no matter what decision is made. They may be described as "water under the bridge", and making decisions on their basis may be described as "crying over spilt milk". In other words, people should not let sunk costs influence their decisions; sunk costs are irrelevant to rational decisions. Thus, if a new factory was originally projected to cost $100 million, and yield $120 million in value, and after $30 million is spent on it the value projection falls to $65 million, the company should abandon the project rather than spending an additional $70 million to complete it. Conversely, if the value projection falls to $75 million, the company, as a rational actor, should continue the project.  This is known as the bygones principle or the marginal principle.
The bygones principle is grounded in the branch of normative decision theory known as rational choice theory, particularly in expected utility hypothesis. Expected utility theory relies on a property known as cancellation, which says that it is rational in decision-making to disregard (cancel) any state of the world that yields the same outcome regardless of one's choice. Past decisions—including sunk costs—meet that criterion.
The bygones principle can also be formalised as the notion of "separability". Separability requires agents to take decisions by comparing the available options in eventualities that can still occur, uninfluenced by how the current situation was reached or by eventualities that are precluded by that history. In the language of decision trees, it requires the agent's choice at a particular choice node to be independent of unreachable parts of the tree. This formulation makes clear how central the principle is to standard economic theory by, for example, founding the folding-back algorithm for individual sequential decisions and game-theoretical concepts such as sub-game perfection.
Until a decision-maker irreversibly commits resources, the prospective cost is an avoidable future cost and is properly included in any decision-making process. For instance, if someone is considering pre-ordering movie tickets, but has not actually purchased them yet, the cost remains avoidable.
Both retrospective and prospective costs could be either fixed costs (continuous for as long as the business is operating and unaffected by output volume) or variable costs (dependent on volume). However, many economists consider it a mistake to classify sunk costs as "fixed" or "variable". For example, if a firm sinks $400 million on an enterprise software installation, that cost is "sunk" because it was a one-time expense and cannot be recovered once spent. A "fixed" cost would be monthly payments made as part of a service contract or licensing deal with the company that set up the software. The upfront irretrievable payment for the installation should not be deemed a "fixed" cost, with its cost spread out over time. Sunk costs should be kept separate. The "variable costs" for this project might include data centre power usage, for example.
There are cases in which taking sunk costs into account in decision-making, violating the bygones principle, is rational. For example, for a manager who wishes to be perceived as persevering in the face of adversity, or to avoid blame for earlier mistakes, it may be rational to persist with a project for personal reasons even if it is not the benefit of their company. Or, if they hold private information about the undesirability of abandoning a project, it is fully rational to persist with a project that outsiders think displays the fallacy of sunk cost.

The bygones principle does not always accord with real-world behavior. Sunk costs often influence people's decisions, with people believing that investments (i.e., sunk costs) justify further expenditures. People demonstrate "a greater tendency to continue an endeavor once an investment in money, effort, or time has been made". This is the sunk cost fallacy, and such behavior may be described as "throwing good money after bad", while refusing to succumb to what may be described as "cutting one's losses". People can remain in failing relationships because they "have already invested too much to leave". Other people are swayed by arguments that a war must continue because lives will have been sacrificed in vain unless victory is achieved. Individuals caught up in psychologically manipulative scams will continue investing time, money and emotional energy into the project, despite doubts or suspicions that something is not right. These types of behaviour do not seem to accord with rational choice theory and are often classified as behavioural errors.
Rego, Arantes, and Magalhães point out that the sunk cost effect exists in committed relationships. They devised two experiments, one of which showed that people in a relationship which they had invested money and effort in were more likely to keep that relationship going than end it; and in the second experiment, while people are in a relationship which they had invested enough time in, they tended to devote more time to the relationship. It also means people fall into the sunk cost fallacy. However, people should ignore sunk costs and make rational decisions when planning for the future; time, money, and effort often make people continue to maintain this relationship, which is equivalent to investing in failed projects.
According to evidence reported by De Bondt and Makhija (1988), managers of many utility companies in the United States have been overly reluctant to terminate economically unviable nuclear plant projects. In the 1960s, the nuclear power industry promised "energy too cheap to meter". Nuclear power lost public support in the 1970s and 1980s, when public service commissions nationwide ordered prudency reviews. From these reviews, De Bondt and Makhija find evidence that the commissions denied many utility companies even partial recovery of nuclear construction costs on the grounds that they had been mismanaging the nuclear construction projects in ways consistent with throwing good money after bad.
There is also evidence of government representatives failing to ignore sunk costs. The term "Concorde fallacy" derives from the fact that the British and French governments continued to fund the joint development of the costly Concorde supersonic airplane even after it became apparent that there was no longer an economic case for the aircraft. The British government privately regarded the project as a commercial disaster that should never have been started. Political and legal issues made it impossible for either government to pull out.
Dijkstra and Hong proposed that part of a person's behavior is influenced by a person's current emotions. Their experiments showed that emotional responses benefit from the sunk cost fallacy. Negative influences lead to the sunk cost fallacy. For example, anxious people face the stress brought about by the sunk cost fallacy. When stressed, they are more motivated to invest in failed projects rather than take additional approaches. Their report shows that the sunk cost fallacy will have a greater impact on people under high load conditions, and people's psychological state and external environment will be the key influencing factors.
The sunk cost effect may cause cost overrun. In business, an example of sunk costs may be an investment into a factory or research that now has a lower value or none. For example, $20 million has been spent on building a power plant; the value now is zero because it is incomplete (and no sale or recovery is feasible). The plant can be completed for an additional $10 million or abandoned, and a different but equally valuable facility built for $5 million. Abandonment and construction of the alternative facility is the more rational decision, even though it represents a total loss of the original expenditure—the original sum invested is a sunk cost. If decision-makers are irrational or have the "wrong" (different) incentives, the completion of the project may be chosen. For example, politicians or managers may have more incentive to avoid the appearance of a total loss. In practice, there is considerable ambiguity and uncertainty in such cases, and decisions may, in retrospect, appear irrational that were, at the time, reasonable to the economic actors involved and in the context of their incentives. A decision-maker might make rational decisions according to their incentives, outside of efficiency or profitability.  This is considered an incentive problem and is distinct from a sunk cost problem. Some research has also noted circumstances where the sunk cost effect is reversed, where individuals appear irrationally eager to write off earlier investments to take up a new endeavor.

A related phenomenon is plan continuation bias, which is recognised as a subtle cognitive bias that tends to force the continuation of a plan or course of action even in the face of changing conditions. In the field of aerospace, it has been recognised as a significant causal factor in accidents, with a 2004 NASA study finding that in 9 out of the 19 accidents studied, aircrew exhibited this behavioural bias.
This is a hazard for ships' captains or aircraft pilots who may stick to a planned course even when it is leading to a fatal disaster, and they should abort instead. A famous example is the Torrey Canyon oil spill in which a tanker ran aground when its captain persisted with a risky course rather than accepting a delay. It has been a factor in numerous air crashes and an analysis of 279 approach and landing accidents (ALAs) found that it was the fourth most common cause, occurring in 11% of cases. Another analysis of 76 accidents found that it was a contributory factor in 42% of cases.
There are also two predominant factors that characterise the bias. The first is an overly optimistic estimate of the probability of success, possibly to reduce cognitive dissonance making a decision. The second is that of personal responsibility: when you are personally accountable, it is difficult for you to admit that you were wrong.
Projects often suffer cost overruns and delays due to the planning fallacy and related factors, including excessive optimism, an unwillingness to admit failure, groupthink, and aversion to loss of sunk costs.

Evidence from behavioral economics suggests that there are at least four specific psychological factors underlying the sunk cost effect:
Framing effects, a cognitive bias where people decide on options based on whether the options are presented with positive or negative connotations; e.g. as a loss or as a gain. People tend to avoid risk when a positive frame is presented but seek risks when a negative frame is presented.
An overoptimistic probability bias, whereby after an investment the evaluation of one's investment-reaping dividends is increased.
The requisite of personal responsibility. Sunk cost appears to operate chiefly in those who feel a personal responsibility for the investments that are to be viewed as a sunk cost.
The desire not to appear wasteful—"One reason why people may wish to throw good money after bad is that to stop investing would constitute an admission that the prior money was wasted."
Taken together, these results suggest that the sunk cost effect may reflect non-standard measures of utility, which is ultimately subjective and unique to the individual.

The framing effect which underlies the sunk cost effect builds upon the concept of extensionality where the outcome is the same regardless of how the information is framed. This is in contradiction to the concept of intentionality, which is concerned with whether the presentation of information changes the situation in question.
Take two mathematical functions:
While these functions are framed differently, regardless of the input "x", the outcome is analytically equivalent. Therefore, if a rational decision maker were to choose between these two functions, the likelihood of each function being chosen should be the same. However, a framing effect places unequal biases towards preferences that are otherwise equal.
Ellingsen, Johannesson, Möllerström and Munkammar have categorised framing effects in a social and economic orientation into three broad classes of theories. Firstly, the framing of options presented can affect internalised social norms or social preferences - this is called variable sociality hypothesis. Secondly, the social image hypothesis suggests that the frame in which the options are presented will affect the way the decision maker is viewed and will in turn affect their behaviour. Lastly, the frame may affect the expectations that people have about each other's behaviour and will in turn affect their own behaviour.

In 1968, Knox and Inkster approached 141 horse bettors: 72 of the people had just finished placing a $2.00 bet within the past 30 seconds, and 69 people were about to place a $2.00 bet in the next 30 seconds. Their hypothesis was that people who had just committed themselves to a course of action (betting $2.00) would reduce post-decision dissonance by believing more strongly than ever that they had picked a winner. Knox and Inkster asked the bettors to rate their horse's chances of winning on a 7-point scale. What they found was that people who were about to place a bet rated the chance that their horse would win at an average of 3.48 which corresponded to a "fair chance of winning" whereas people who had just finished betting gave an average rating of 4.81 which corresponded to a "good chance of winning". Their hypothesis was confirmed: after making a $2.00 commitment, people became more confident their bet would pay off. Knox and Inkster performed an ancillary test on the patrons of the horses themselves and managed (after normalization) to repeat their finding almost identically. Other researchers have also found evidence of inflated probability estimations.

A ticket buyer who purchases a ticket in advance to an event they eventually turn out not to enjoy makes a semi-public commitment to watching it. To leave early is to make this lapse of judgment manifest to strangers, an appearance they might otherwise choose to avoid. As well, the person may not want to leave the event because they have already paid, so they may feel that leaving would waste their expenditure. Alternatively, they may take a sense of pride in having recognised the opportunity cost of the alternative use of time.

In recent years, there has been a resurgence in studies of how the brain processes information with respect to sunk costs. Measuring sensitivity to sunk costs in laboratory studies can be challenging, as it is often difficult to disentangle the influence of sunk costs from future returns on investment. In a cross-species study in humans, rats, and mice, Sweis et al discovered a conserved evolutionary history to sensitivity to sunk costs across species.
This has opened up more questions as to what might the evolutionary drivers be behind why the brain is capable of processing information in this way, what utility, if any, sensitivity to sunk costs may confer, and how might distinct circuits in the brain give rise to this sort of valuation depending on the framing of the question, circumstances of the environment, or state of the individual. Ongoing work is characterizing how neurons encode sensitivity to sunk costs, how sunk costs appear only after certain types of choices, and how sunk costs could contribute to mood burden.

Framing effect is a cognitive bias where people's decisions change depending on how options are framed, even when the options are logically identical. Studies show that when both choices are framed positively as gains, the majority of people prefer a certain gain over a probable gain. On the other hand, when both choices are framed negatively as losses, people tend to choose an uncertain loss over an inevitable loss. Though the choices across the positive and negative framing conditions are logically equivalent, people in different conditions make different decisions. Gain and loss are defined within the scenario as outcomes, for example, lives lost or saved, patients treated or not treated, monetary gains or losses.
Prospect theory posits that a loss is more significant than the equivalent gain, that a sure gain (certainty effect and pseudocertainty effect) is favored over a probabilistic gain, and that a probabilistic loss is preferred to a definite loss. One of the dangers of framing effects is that people are often provided with options within the context of only one of the two frames.
The concept helps to develop an understanding of frame analysis within social movements, and also in the formation of political opinion where spin plays a large role in political opinion polls that are framed to encourage a response beneficial to the organization that has commissioned the poll. It has been suggested that the use of the technique is discrediting political polls themselves. The effect is reduced, or even eliminated, if ample credible information is provided to people.

The rational actor model is a theoretical framework used by economists to understand human decision-making at the individual level. It assumes that people act in rational ways that optimize their interests. The model includes several core assumptions: rational actors have stable preferences, access to all relevant information, the capability to analyze all possible options and outcomes, and are unbiased and unemotional. This model has been influential in many social science disciplines, including economics and political science 
Psychologists have criticized the rational actor model by arguing that its assumptions do not accurately reflect real-world human decision-making behavior. They point out that the time and cognitive resources available to individuals are insufficient for deliberate analysis of every situation. In practice, decision-making is often affected by emotion, social context, and Heuristic Since then, psychologists have conducted extensive research on the systematic biases and errors that influence human judgment and decision-making.

Tversky and Kahneman proposed a new theoretical frame to understand decision-making in their 1981 paper, "The Framing of Decisions and the Psychology of Choice."  They are major contributors in the domain of the psychology of decision-making. They proposed prospect theory, which, unlike traditional economic theories that assume people make rational choices by calculating risks and benefits, recognizes that people make biased or inconsistent decisions in real life based on how options are presented—or, in other words, framed.
In the same paper, they conducted and experiment to test participants with the classic question, the Asian disease problem: participants were asked to choose between two treatments. The researchers manipulated two independent variables: framing of options (positive vs. negative) and certainty of outcomes (certain vs. risky)
Participants were asked to choose between two treatments for 600 people affected by a deadly disease. Treatment A was predicted to result in 400 deaths, whereas treatment B had a 33% chance that no one would die but a 66% chance that everyone would die. This choice was then presented to participants either with positive framing, i.e. how many people would live, or with negative framing, i.e. how many people would die.
Treatment A was chosen by 72% of participants when it was presented with positive framing ("saves 200 lives"). When the question was negatively framed, the result reversed, and 78% of participants chose Treatment B.
In the paper they coauthored later in 1984, Kahneman and Tversky made two addition to their original theory. First, they discussed how people reason about probability. They pointed out that people do not treat probability logically, instead tending to overestimate the likelihood of low-probability events (like winning the lottery) while underestimating events with moderate to high probability. They also introduced the concept of mental accounting—for example, people treat losing a $10 bill differently from losing a $10 movie ticket, even though the monetary loss is the same.
In his later work, Kahneman further developed the theoretical framework of decision-making by introducing the concept of dual systems. In his 2011 book Thinking, Fast and Slow, he described two modes of thinking: one system is automatic, intuitive, and emotional, operating effortlessly and responsible for generating quick judgments and impressions. The other system is logical, deliberate, and effortful, responsible for complex tasks and self-control. People are more vulnerable to framing effects when relying on the intuitive system, as it is more susceptible to contextual cues and surface-level presentation.

In logic, extensionality requires "two formulas which have the same truth-value under any truth-assignments to be mutually substitutable salva veritate in a sentence that contains one of these formulas." Put simply, objects that have the same external properties are equal. This principle, applied to decision making, suggests that making a decision in a problem should not be affected by how the problem is described. For example, varied descriptions of the same decision problem should not give rise to different decisions, due to the extensionality principle. If judgments are made on the basis of irrelevant information as described, that is called an extensionality violation. Addressing extensionality violations entails cultivating awareness of how different descriptions of a problem may inadvertently influence decisions, and as a result developing strategies to mitigate such deviations. In doing so, decision-makers can aim to uphold the extensionality principle as a guide in navigating the complexity of choice, focusing on decisions that are more in tune to the inherent properties of the problem rather than its descriptions.

Framing effects have powerful real-world influences. They shape decision-making across a broad range of domains and contexts—for example, in public health, education, politics, law, and economics. Even when options are logically identical, subtle shifts in wording—such as presenting outcomes as gains or losses—can reverse preferences.
In a 2021 study on the framing effect in the context of the COVID-19 pandemic, researchers found that the framing effect was larger in this context than under normal circumstances, indicating that individuals were more influenced by how options were presented during the pandemic. There was a positive association between the framing effect and perceived stress and concerns related to coronavirus, indicating that these factors are influential when it comes to decision-making. However, they were not related to risk aversion.
This effect has been shown in other contexts:
93% of PhD students registered early when a penalty fee for late registration was emphasized, but only 67% did so when this was presented as a discount for earlier registration.
62% of people disagreed with allowing "public condemnation of democracy", but only 46% of people agreed that it was right to "forbid public condemnation of democracy".
More people will support an economic policy if the employment rate is emphasized than when the associated unemployment rate is highlighted.
It has been argued that pretrial detention may increase a defendant's willingness to accept a plea bargain, since imprisonment, rather than freedom, will be their baseline, and pleading guilty will be viewed as an event that will cause their earlier release rather than as an event that will put them in prison.

The framing effect has consistently been shown to be one of the largest biases in decision making. In general, susceptibility to framing effects increases with age. Age difference factors are particularly important when considering health care and financial decisions. The susceptibility to framing can influence how older individuals perceive and in turn respond to information, potentially leading to less optimal choices that can have lasting consequences. In healthcare, for instance, where decisions profoundly affect well-being, the framing effect can sway older individuals towards or away from certain treatment options based on the way in which the medical information is presented. Likewise, in financial decision-making, the framing of retirement planning or investment risks may have significant impacts on the choices individuals make, potentially impacting their financial security and state in the later stages of their lives.
However, the framing effect seems to diminish when encountered in a foreign (non-native) language. One explanation of this disappearance is that a non-native language provides greater cognitive and emotional distance than one's native tongue. A foreign language is also processed less automatically than a native tongue. This leads to more deliberation, which can affect decision making, resulting in decisions that are more systematic.

Framing effects in decision-making become stronger as children age. This is partially because qualitative reasoning increases with age. While preschoolers are more likely to make decisions based on quantitative properties, such as probability of an outcome, elementary schoolers and adolescents become progressively more likely to reason qualitatively, opting for a sure option in a gain frame and a risky option in a loss frame regardless of probabilities. The increase in qualitative thinking is related to an increase in "gist based" thinking that occurs over a lifetime.
However, qualitative reasoning, and thus susceptibility to framing effects, is still not as strong in adolescents as in adults, and adolescents are more likely than adults to choose the risky option under both the gain and loss frames of a given scenario. One explanation for adolescent tendencies toward risky choices is that they lack real-world experience with negative consequences, and thus over-rely on conscious evaluation of risks and benefits, focusing on specific information and details or quantitative analysis. This reduces influence of framing effects and leads to greater consistency across frames of a given scenario. Children between the ages of 10 and 12 are more likely to take risks and show framing effects, while younger children only considered the quantitative differences between the two options presented.

Younger adults are more likely than older adults to be enticed by risk-taking when presented with loss frame trials. This is a noteworthy phenomenon that underscores the complex interplay between age and decision-making tendencies.
In multiple studies of undergraduate students, researchers have found that students are more likely to prefer options framed positively. This could be attributed to a variety of factors such as an inclination for novelty-seeking, a more optimistic outlook on outcomes, or even a reduced aversion to risk which is inherent in youth. For example, they are more likely to enjoy meat labeled 75% lean meat as opposed to 25% fat, or use condoms advertised as being 95% effective as opposed to having a 5% risk of failure.
Young adults are especially susceptible to framing effects when presented with an ill-defined problem in which there is no correct answer and individuals must arbitrarily determine what information they consider relevant. For example, undergraduate students are more willing to purchase an item such as a movie ticket after losing an amount equivalent to the item's cost than after losing the item itself. This susceptibility underscores the importance of considering psychological factors in the context of decision-making. Recognizing this vulnerability emphasizes the need for decision-makers to be aware of cognitive biases when navigating decision-making in which there isn't a clear answer so that they can take a more informed approach.

The framing effect is claimed to be greater in older adults than in younger adults or adolescents. This claim may be a result of enhanced negativity bias, though some sources claim that the negativity bias actually decreases with age.
In particular, this increased susceptibility to the framing effect manifested itself mainly in response to negative frames. Positive framings were not found to have a significant effect on the framing effect in older adults. This may be due in part to socioemotional selectivity theory, where the increased age shifts the focus of adults from risk taking to maximizing their emotional experiences in the present, hence the increased framing in the negative frame. The dual process theory may also play a role as negative framings evoke less heightened responses, leading to the deployment of the implicit processes. The implicit process is found to be frame-sensitive, and thus may be the reason why framing is pronounced in negative frames for older adults.
Another possible cause is that older adults have fewer cognitive resources available to them and are more likely to default to less cognitively demanding strategies when faced with a decision. They tend to rely on easily accessible information, or frames, regardless of whether that information is relevant to making the decision in question. Several studies have shown that younger adults will make less biased decisions than older adults because they base their choices on interpretations of patterns of events and can better employ decision making strategies that require cognitive resources like working-memory skills. Older adults, on the other hand, make choices based on immediate reactions to gains and losses.
Older adults' lack of cognitive resources, such as flexibility in decision making strategies, may cause older adults to be influenced by emotional frames more so than younger adults or adolescents. In addition, as individuals age, they make decisions more quickly than their younger counterparts. It is significant that, when prompted to do so, older adults will often make a less biased decision with reevaluation of their original choice.
The increase in framing effects among older adults has important implications, especially in medical contexts. Older adults are influenced heavily by the inclusion or exclusion of extraneous details, meaning they are likely to make serious medical decisions based on how doctors frame the two options rather than the qualitative differences between the options, causing older adults to inappropriately form their choices.
When considering cancer treatments, framing can shift older adults' focus from short- to long-term survival under a negative and positive frame, respectively. When presented with treatment descriptions described in positive, negative, or neutral terms, older adults are significantly more likely to agree to a treatment when it is positively described than they are to agree to the same treatment when it is described neutrally or negatively. Additionally, framing often leads to inconsistency in choice: a change in description qualities after an initial choice is made can cause older adults to revoke their initial decision in favor of an alternative option. Older adults also remember positively framed statements more accurately than negatively framed statements. This has been demonstrated by evaluating older adults' recall of statements in pamphlets about health care issues.

The gambler's fallacy, also known as the Monte Carlo fallacy or the fallacy of the maturity of chances, is the belief that, if an event (whose occurrences are independent and identically distributed) has occurred less frequently than expected, it is more likely to happen again in the future (or vice versa). The fallacy is commonly associated with gambling, where it may be believed, for example, that the next dice roll is more likely to be six than is usually the case because there have recently been fewer than the expected number of sixes.
The term  "Monte Carlo fallacy" originates from an example of the phenomenon, in which the roulette wheel spun black 26 times in succession at the Monte Carlo Casino in 1913.

The gambler's fallacy can be illustrated by considering the repeated toss of a fair coin. The outcomes in different tosses are statistically independent and the probability of getting heads on a single toss is ⁠1/2⁠ (one in two). The probability of getting two heads in two tosses is ⁠1/4⁠ (one in four) and the probability of getting three heads in three tosses is ⁠1/8⁠ (one in eight). In general, if Ai is the event where toss i of a fair coin comes up heads, then:
If after tossing four heads in a row, the next coin toss also came up heads, it would complete a run of five successive heads. Since the probability of a run of five successive heads is ⁠1/32⁠ (one in thirty-two), a person might believe that the next flip would be more likely to come up tails rather than heads again. This is incorrect and is an example of the gambler's fallacy. The event "5 heads in a row" and the event "first 4 heads, then a tails" are equally likely, each having probability ⁠1/32⁠. Since the first four tosses turn up heads, the probability that the next toss is a head is:

If a fair coin is flipped 21 times, the probability of 21 heads is 1 in 2,097,152. The probability of flipping a head after having already flipped 20 heads in a row is ⁠1/2⁠. Assuming a fair coin:
The probability of getting 20 heads then 1 tail, and the probability of getting 20 heads then another head are both 1 in 2,097,152. When flipping a fair coin 21 times, the outcome is equally likely to be 21 heads as 20 heads and then 1 tail. These two outcomes are equally as likely as any of the other combinations that can be obtained from 21 flips of a coin. All of the 21-flip combinations will have probabilities equal to 0.521, or 1 in 2,097,152. Assuming that a change in the probability will occur as a result of the outcome of prior flips is incorrect because every outcome of a 21-flip sequence is as likely as the other outcomes. In accordance with Bayes' theorem, the likely outcome of each flip is the probability of the fair coin, which is ⁠1/2⁠.

The fallacy leads to the incorrect notion that previous failures will create an increased probability of success on subsequent attempts. For a fair 16-sided die, the probability of each outcome occurring is ⁠1/16⁠ (6.25%). If a win is defined as rolling a 1, the probability of a 1 occurring at least once in 16 rolls is:
The probability of a loss on the first roll is ⁠15/16⁠ (93.75%). According to the fallacy, the player should have a higher chance of winning after one loss has occurred. The probability of at least one win is now:
By losing one toss, the player's probability of winning drops by two percentage points. With 5 losses and 11 rolls remaining, the probability of winning drops to around 0.5 (50%). The probability of at least one win does not increase after a series of losses; indeed, the probability of success actually decreases, because there are fewer trials left in which to win. The probability of winning will eventually be equal to the probability of winning a single toss, which is ⁠1/16⁠ (6.25%) and occurs when only one toss is left.

After a consistent tendency towards tails, a gambler may also decide that tails has become a more likely outcome. This is a rational and Bayesian conclusion, bearing in mind the possibility that the coin may not be fair; it is not a fallacy. Believing the odds to favor tails, the gambler sees no reason to change to heads. However, it is a fallacy that a sequence of trials carries a memory of past results which tend to favor or disfavor future outcomes.
The inverse gambler's fallacy described by Ian Hacking is a situation where a gambler entering a room and seeing a person rolling a double six on a pair of dice may erroneously conclude that the person must have been rolling the dice for quite a while, as they would be unlikely to get a double six on their first attempt.

Researchers have examined whether a similar bias exists for inferences about unknown past events based upon known subsequent events, calling this the "retrospective gambler's fallacy".
An example of a retrospective gambler's fallacy would be to observe multiple successive "heads" on a coin toss and conclude from this that the previously unknown flip was "tails". Real world examples of retrospective gambler's fallacy have been argued to exist in events such as the origin of the Universe. In his book Universes, John Leslie argues that "the presence of vastly many universes very different in their characters might be our best explanation for why at least one universe has a life-permitting character". Daniel M. Oppenheimer and Benoît Monin argue that "In other words, the 'best explanation' for a low-probability event is that it is only one in a multiple of trials, which is the core intuition of the reverse gambler's fallacy." Philosophical arguments are ongoing about whether such arguments are or are not a fallacy, arguing that the occurrence of our universe says nothing about the existence of other universes or trials of universes. Three studies involving Stanford University students tested the existence of a retrospective gamblers' fallacy. All three studies concluded that people have a gamblers' fallacy retrospectively as well as to future events. The authors of all three studies concluded their findings have significant "methodological implications" but may also have "important theoretical implications" that need investigation and research, saying "[a] thorough understanding of such reasoning processes requires that we not only examine how they influence our predictions of the future, but also our perceptions of the past."

In 1796, Pierre-Simon Laplace described in A Philosophical Essay on Probabilities the ways in which men calculated their probability of having sons: "I have seen men, ardently desirous of having a son, who could learn only with anxiety of the births of boys in the month when they expected to become fathers. Imagining that the ratio of these births to those of girls ought to be the same at the end of each month, they judged that the boys already born would render more probable the births next of girls." The expectant fathers feared that if more sons were born in the surrounding community, then they themselves would be more likely to have a daughter. This essay by Laplace is regarded as one of the earliest descriptions of the fallacy. Likewise, after having multiple children of the same sex, some parents may erroneously believe that they are due to have a child of the opposite sex.

An example of the gambler's fallacy occurred in a game of roulette at the Monte Carlo Casino on August 18, 1913, when the ball fell in black 26 times in a row. This was an extremely unlikely occurrence: for any given sequence of 26 spins, the probability of either red or black occurring 26 times in a row on a single zero roulette wheel is 2×(⁠18/37⁠)26 or around 1 in 68.4 million, assuming the mechanism is unbiased. Gamblers lost millions of francs betting against black, reasoning incorrectly that the streak was causing an imbalance in the randomness of the wheel, and that it had to be followed by a long streak of red.

The gambler's fallacy does not apply when the probability of different events is not independent. In such cases, the probability of future events can change based on the outcome of past events, such as the statistical permutation of events. An example is when cards are drawn from a deck without replacement. If an ace is drawn from a deck and not reinserted, the next card drawn is less likely to be an ace and more likely to be of another rank. The probability of drawing another ace, assuming that it was the first card drawn and that there are no jokers, has decreased from ⁠4/52⁠ (7.69%) to ⁠3/51⁠ (5.88%), while the probability for each other rank has increased from ⁠4/52⁠ (7.69%) to ⁠4/51⁠ (7.84%). This effect allows card counting systems to work in games such as blackjack.

In most illustrations of the gambler's fallacy and the reverse gambler's fallacy, the trial (e.g. flipping a coin) is assumed to be fair. In practice, this assumption may not hold. For example, if a coin is flipped 21 times, the probability of 21 heads with a fair coin is 1 in 2,097,152. Since this probability is so small, if it happens, it may well be that the coin is somehow biased towards landing on heads, or that it is being controlled by hidden magnets, or similar. In this case, the smart bet is "heads" because Bayesian inference from the empirical evidence — 21 heads in a row — suggests that the coin is likely to be biased toward heads. Bayesian inference can be used to show that when the long-run proportion of different outcomes is unknown but exchangeable (meaning that the random process from which the outcomes are generated may be biased but is equally likely to be biased in any direction) and that previous observations demonstrate the likely direction of the bias, the outcome which has occurred the most in the observed data is the most likely to occur again.
For example, if the a priori probability of a biased coin is say 1%, and assuming that such a biased coin would come down heads say 60% of the time, then after 21 heads the probability of a biased coin has increased to about 32%.
The opening scene of the play Rosencrantz and Guildenstern Are Dead by Tom Stoppard discusses these issues as one man continually flips heads and the other considers various possible explanations.

If external factors are allowed to change the probability of the events, the gambler's fallacy may not hold. For example, a change in the game rules might favour one player over the other, improving their win percentage. Similarly, an inexperienced player's success may decrease after opposing teams learn about and play against their weaknesses. This is another example of bias.

The gambler's fallacy arises out of a belief in a law of small numbers, leading to the erroneous belief that small samples must be representative of the larger population. According to the fallacy, streaks must eventually even out in order to be representative. Amos Tversky and Daniel Kahneman first proposed that the gambler's fallacy is a cognitive bias produced by a psychological heuristic called the representativeness heuristic, which states that people evaluate the probability of a certain event by assessing how similar it is to events they have experienced before, and how similar the events surrounding those two processes are. According to this view, "after observing a long run of red on the roulette wheel, for example, most people erroneously believe that black will result in a more representative sequence than the occurrence of an additional red", so people expect that a short run of random outcomes should share properties of a longer run, specifically in that deviations from average should balance out. When people are asked to make up a random-looking sequence of coin tosses, they tend to make sequences where the proportion of heads to tails stays closer to 0.5 in any short segment than would be predicted by chance, a phenomenon known as insensitivity to sample size. Kahneman and Tversky interpret this to mean that people believe short sequences of random events should be representative of longer ones. The representativeness heuristic is also cited behind the related phenomenon of the clustering illusion, according to which people see streaks of random events as being non-random when such streaks are actually much more likely to occur in small samples than people expect.
The gambler's fallacy can also be attributed to the mistaken belief that gambling, or even chance itself, is a fair process that can correct itself in the event of streaks, known as the just-world fallacy. Other researchers believe that belief in the fallacy may be the result of a mistaken belief in an internal locus of control. When a person believes that gambling outcomes are the result of their own skill, they may be more susceptible to the gambler's fallacy because they reject the idea that chance could overcome skill or talent.

Some researchers believe that it is possible to define two types of gambler's fallacy: type one and type two. Type one is the classic gambler's fallacy, where individuals believe that a particular outcome is due after a long streak of another outcome. Type two gambler's fallacy, as defined by Gideon Keren and Charles Lewis, occurs when a gambler underestimates how many observations are needed to detect a favorable outcome, such as watching a roulette wheel for a length of time and then betting on the numbers that appear most often. For events with a high degree of randomness, detecting a bias that will lead to a favorable outcome takes an impractically large amount of time and is very difficult, if not impossible, to do. The two types differ in that type one wrongly assumes that gambling conditions are fair and perfect, while type two assumes that the conditions are biased, and that this bias can be detected after a certain amount of time.
Another variety, known as the retrospective gambler's fallacy, occurs when individuals judge that a seemingly rare event must come from a longer sequence than a more common event does. The belief that an imaginary sequence of die rolls is more than three times as long when a set of three sixes is observed as opposed to when there are only two sixes. This effect can be observed in isolated instances, or even sequentially. Another example would involve hearing that a teenager has unprotected sex and becomes pregnant on a given night, and concluding that she has been engaging in unprotected sex for longer than if we hear she had unprotected sex but did not become pregnant, when the probability of becoming pregnant as a result of each intercourse is independent of the amount of prior intercourse.

Another psychological perspective states that gambler's fallacy can be seen as the counterpart to basketball's hot-hand fallacy, in which people tend to predict the same outcome as the previous event - known as positive recency - resulting in a belief that a high scorer will continue to score. In the gambler's fallacy, people predict the opposite outcome of the previous event - negative recency - believing that since the roulette wheel has landed on black on the previous six occasions, it is due to land on red the next. Ayton and Fischer have theorized that people display positive recency for the hot-hand fallacy because the fallacy deals with human performance, and that people do not believe that an inanimate object can become "hot." Human performance is not perceived as random, and people are more likely to continue streaks when they believe that the process generating the results is nonrandom. When a person exhibits the gambler's fallacy, they are more likely to exhibit the hot-hand fallacy as well, suggesting that one construct is responsible for the two fallacies.
The difference between the two fallacies is also found in economic decision-making. A study by Huber, Kirchler, and Stockl in 2010 examined how the hot hand and the gambler's fallacy are exhibited in the financial market. The researchers gave their participants a choice: they could either bet on the outcome of a series of coin tosses, use an expert opinion to sway their decision, or choose a risk-free alternative instead for a smaller financial reward. Participants turned to the expert opinion to make their decision 24% of the time based on their past experience of success, which exemplifies the hot-hand. If the expert was correct, 78% of the participants chose the expert's opinion again, as opposed to 57% doing so when the expert was wrong. The participants also exhibited the gambler's fallacy, with their selection of either heads or tails decreasing after noticing a streak of either outcome. This experiment helped bolster Ayton and Fischer's theory that people put more faith in human performance than they do in seemingly random processes.

While the representativeness heuristic and other cognitive biases are the most commonly cited cause of the gambler's fallacy, research suggests that there may also be a neurological component. Functional magnetic resonance imaging has shown that after losing a bet or gamble, known as riskloss, the frontoparietal network of the brain is activated, resulting in more risk-taking behavior. In contrast, there is decreased activity in the amygdala, caudate, and ventral striatum after a riskloss. Activation in the amygdala is negatively correlated with gambler's fallacy, so that the more activity exhibited in the amygdala, the less likely an individual is to fall prey to the gambler's fallacy. These results suggest that gambler's fallacy relies more on the prefrontal cortex, which is responsible for executive, goal-directed processes, and less on the brain areas that control affective decision-making.
The desire to continue gambling or betting is controlled by the striatum, which supports a choice-outcome contingency learning method. The striatum processes the errors in prediction and the behavior changes accordingly. After a win, the positive behavior is reinforced and after a loss, the behavior is conditioned to be avoided. In individuals exhibiting the gambler's fallacy, this choice-outcome contingency method is impaired, and they continue to make risks after a series of losses.

The gambler's fallacy is a deep-seated cognitive bias and can be very hard to overcome. Educating individuals about the nature of randomness has not always proven effective in reducing or eliminating any manifestation of the fallacy. Participants in a study by Beach and Swensson in 1967 were shown a shuffled deck of index cards with shapes on them, and were instructed to guess which shape would come next in a sequence. The experimental group of participants was informed about the nature and existence of the gambler's fallacy, and were explicitly instructed not to rely on run dependency to make their guesses. The control group was not given this information. The response styles of the two groups were similar, indicating that the experimental group still based their choices on the length of the run sequence. This led to the conclusion that instructing individuals about randomness is not sufficient in lessening the gambler's fallacy.
An individual's susceptibility to the gambler's fallacy may decrease with age. A study by Fischbein and Schnarch in 1997 administered a questionnaire to five groups: students in grades 5, 7, 9, 11, and college students specializing in teaching mathematics. None of the participants had received any prior education regarding probability. The question asked was: "Ronni flipped a coin three times and in all cases heads came up. Ronni intends to flip the coin again. What is the chance of getting heads the fourth time?" The results indicated that as the students got older, the less likely they were to answer with "smaller than the chance of getting tails", which would indicate a negative recency effect. 35% of the 5th graders, 35% of the 7th graders, and 20% of the 9th graders exhibited the negative recency effect. Only 10% of the 11th graders answered this way, and none of the college students did. Fischbein and Schnarch theorized that an individual's tendency to rely on the representativeness heuristic and other cognitive biases can be overcome with age.
Another possible solution comes from Roney and Trick, Gestalt psychologists who suggest that the fallacy may be eliminated as a result of grouping. When a future event such as a coin toss is described as part of a sequence, no matter how arbitrarily, a person will automatically consider the event as it relates to the past events, resulting in the gambler's fallacy. When a person considers every event as independent, the fallacy can be greatly reduced.
Roney and Trick told participants in their experiment that they were betting on either two blocks of six coin tosses, or on two blocks of seven coin tosses. The fourth, fifth, and sixth tosses all had the same outcome, either three heads or three tails. The seventh toss was grouped with either the end of one block, or the beginning of the next block. Participants exhibited the strongest gambler's fallacy when the seventh trial was part of the first block, directly after the sequence of three heads or tails. The researchers pointed out that the participants that did not show the gambler's fallacy showed less confidence in their bets and bet fewer times than the participants who picked with the gambler's fallacy. When the seventh trial was grouped with the second block, and was perceived as not being part of a streak, the gambler's fallacy did not occur.
Roney and Trick argued that instead of teaching individuals about the nature of randomness, the fallacy could be avoided by training people to treat each event as if it is a beginning and not a continuation of previous events. They suggested that this would prevent people from gambling when they are losing, in the mistaken hope that their chances of winning are due to increase based on an interaction with previous events.

Within a real-world setting, numerous studies have uncovered that for various decision makers placed in high stakes scenarios, it is likely they will reflect some degree of strong negative autocorrelation in their judgement.

In a study aimed at discovering if the negative autocorrelation that exists with the gambler's fallacy existed in the decision made by U.S. asylum judges, results showed that after two successive asylum grants, a judge would be 5.5% less likely to approve a third grant.

In the game of baseball, decisions are made every minute. One particular decision made by umpires which is often subject to scrutiny is the 'strike zone' decision. Whenever a batter does not swing, the umpire must decide if the ball was within a fair region for the batter, known as the strike zone. If outside of this zone, the ball does not count towards outing the batter. In a study of over 12,000 games, results showed that umpires are 1.3% less likely to call a strike if the previous two balls were also strikes.

In the decision making of loan officers, it can be argued that monetary incentives are a key factor in biased decision making, rendering it harder to examine the gambler's fallacy effect. However, research shows that loan officers who are not incentivised by monetary gain are 8% less likely to approve a loan if they approved one for the previous client.

Lottery play and jackpots entice gamblers around the globe, with the biggest decision for hopeful winners being what numbers to pick. While most people will have their own strategy, evidence shows that after a number is selected as a winner in the current draw, the same number will experience a significant drop in selections in the following lottery. A popular study by Charles Clotfelter and Philip Cook investigated this effect in 1991, where they concluded bettors would cease to select numbers immediately after they were selected, ultimately recovering selection popularity within three months. Soon after, a 1994 study was constructed by Dek Terrell to test the findings of Clotfelter and Cook. The key change in Terrell's study was the examination of a pari-mutuel lottery in which, a number selected with lower total wagers placed on it will result in a higher pay-out. While this examination did conclude that players in both types of lotteries exhibited behaviour in-line with the gambler's fallacy theory, those who took part in pari-mutuel betting seemed to be less influenced.
The effect of the gambler's fallacy can be observed as numbers are chosen far less frequently soon after they are selected as winners, recovering slowly over a two-month period. For example, on the 11th of April 1988, 41 players selected 244 as the winning combination. Three days later only 24 individuals selected 244, a 41.5% decrease. This is the gambler's fallacy in motion, as lottery players believe that the occurrence of a winning combination in previous days will decrease its likelihood of occurring today.

Several video games feature the use of loot boxes, a collection of in-game items awarded on opening with random contents set by rarity metrics, as a monetization scheme. Since around 2018, loot boxes have come under scrutiny from governments and advocates on the basis they are akin to gambling, particularly for games aimed at youth. Some games use a special "pity-timer" mechanism, that if the player has opened several loot boxes in a row without obtaining a high-rarity item, subsequent loot boxes will improve the odds of a higher-rate item drop. This is considered to feed into the gambler's fallacy since it reinforces the idea that a player will eventually obtain a high-rarity item (a win) after only receiving common items from a string of previous loot boxes.

Groupthink is a psychological phenomenon that occurs within a group of people in which the desire for harmony or conformity in the group results in an irrational or dysfunctional decision-making outcome. Cohesiveness, or the desire for cohesiveness, in a group may produce a tendency among its members to agree at all costs. This causes the group to minimize conflict and reach a consensus decision without critical evaluation.
Groupthink is a construct of social psychology but has an extensive reach and influences literature in the fields of communication studies, political science, management, and organizational theory, as well as important aspects of deviant religious cult behaviour.

Groupthink is sometimes stated to occur (more broadly) within natural groups within the community, for example to explain the lifelong different mindsets of those with differing political views (such as "conservatism" and "liberalism" in the U.S. political context or the purported benefits of team work vs. work conducted in solitude). However, this conformity of viewpoints within a group does not mainly involve deliberate group decision-making, and might be better explained by the collective confirmation bias of the individual members of the group. 
The term was coined in 1952 by William H. Whyte Jr. Most of the initial research on groupthink was conducted by Irving Janis, a research psychologist from Yale University. Janis published an influential book in 1972, which was revised in 1982. Janis used the Bay of Pigs disaster (the failed invasion of Castro's Cuba in 1961) and the Japanese attack on Pearl Harbor in 1941 as his two prime case studies. Later studies have evaluated and reformulated his groupthink model.
Groupthink requires individuals to avoid raising controversial issues or alternative solutions, and there is loss of individual creativity, uniqueness and independent thinking. The dysfunctional group dynamics of the "ingroup" produces an "illusion of invulnerability" (an inflated certainty that the right decision has been made). Thus the "ingroup" significantly overrates its own abilities in decision-making and significantly underrates the abilities of its opponents (the "outgroup"). Furthermore, groupthink can produce dehumanizing actions against the "outgroup". Members of a group can often feel under peer pressure to "go along with the crowd" for fear of "rocking the boat" or of how their speaking out will be perceived by the rest of the group. Group interactions tend to favor clear and harmonious agreements and it can be a cause for concern when little to no new innovations or arguments for better policies, outcomes and structures are called to question. (McLeod). Groupthink can often lead to the creation of "yes men", because group activities and group projects in general make it extremely easy to pass on not offering constructive opinions.
Some methods that have been used to counteract group think in the past are selecting teams from more diverse backgrounds, and even mixing men and women for groups (Kamalnath). Groupthink can be considered by many to be a detriment to companies, organizations and in any work situations. Most positions that are senior level need individuals to be independent in their thinking. There is a positive correlation found between outstanding executives and decisiveness (Kelman). Groupthink also prohibits an organization from moving forward and innovating if no one ever speaks up and says something could be done differently.
Antecedent factors such as group cohesiveness, faulty group structure, and situational context (e.g., community panic) play into the likelihood of whether or not groupthink will impact the decision-making process.

William H. Whyte Jr. derived the term from George Orwell's Nineteen Eighty-Four, and popularized it in 1952 in Fortune magazine:
Groupthink being a coinage – and, admittedly, a loaded one – a working definition is in order. We are not talking about mere instinctive conformity – it is, after all, a perennial failing of mankind. What we are talking about is a rationalized conformity – an open, articulate philosophy which holds that group values are not only expedient but right and good as well.
Groupthink was Whyte's diagnosis of the malaise affecting both the study and practice of management (and, by association, America) in the 1950s. Whyte was dismayed that employees had subjugated themselves to the tyranny of groups, which crushed individuality and were instinctively hostile to anything or anyone that challenged the collective view.
American psychologist Irving Janis (Yale University) pioneered the initial research on the groupthink theory. He does not cite Whyte, but coined the term again by analogy with "doublethink" and similar terms that were part of the newspeak vocabulary in the novel Nineteen Eighty-Four by George Orwell. He initially defined groupthink as follows:
I use the term groupthink as a quick and easy way to refer to the mode of thinking that persons engage in when concurrence-seeking becomes so dominant in a cohesive ingroup that it tends to override realistic appraisal of alternative courses of action. Groupthink is a term of the same order as the words in the newspeak vocabulary George Orwell used in his dismaying world of 1984. In that context, groupthink takes on an invidious connotation. Exactly such a connotation is intended, since the term refers to a deterioration in mental efficiency, reality testing and moral judgments as a result of group pressures.
He went on to write:
The main principle of groupthink, which I offer in the spirit of Parkinson's Law, is this: "The more amiability and esprit de corps there is among the members of a policy-making ingroup, the greater the danger that independent critical thinking will be replaced by groupthink, which is likely to result in irrational and dehumanizing actions directed against outgroups".
Janis set the foundation for the study of groupthink starting with his research in the American Soldier Project where he studied the effect of extreme stress on group cohesiveness. After this study he remained interested in the ways in which people make decisions under external threats. This interest led Janis to study a number of "disasters" in American foreign policy, such as failure to anticipate the Japanese attack on Pearl Harbor (1941); the Bay of Pigs Invasion fiasco (1961); and the prosecution of the Vietnam War (1964–67) by President Lyndon Johnson. He concluded that in each of these cases, the decisions occurred largely because of groupthink, which prevented contradictory views from being expressed and subsequently evaluated.
After the publication of Janis' book Victims of Groupthink in 1972, and a revised edition with the title Groupthink: Psychological Studies of Policy Decisions and Fiascoes in 1982, the concept of groupthink was used to explain many other faulty decisions in history. These events included Nazi Germany's decision to invade the Soviet Union in 1941, the Watergate scandal and others. Despite the popularity of the concept of groupthink, fewer than two dozen studies addressed the phenomenon itself following the publication of Victims of Groupthink, between the years 1972 and 1998. This was surprising considering how many fields of interests it spans, which include political science, communications, organizational studies, social psychology, management, strategy, counseling, and marketing. One can most likely explain this lack of follow-up in that group research is difficult to conduct, groupthink has many independent and dependent variables, and it is unclear "how to translate [groupthink's] theoretical concepts into observable and quantitative constructs".
Nevertheless, outside research psychology and sociology, wider culture has come to detect groupthink in observable situations, for example:
" [...] critics of Twitter point to the predominance of the hive mind in such social media, the kind of groupthink that submerges independent thinking in favor of conformity to the group, the collective"
"[...] leaders often have beliefs which are very far from matching reality and which can become more extreme as they are encouraged by their followers. The predilection of many cult leaders for abstract, ambiguous, and therefore unchallengeable ideas can further reduce the likelihood of reality testing, while the intense milieu control exerted by cults over their members means that most of the reality available for testing is supplied by the group environment. This is seen in the phenomenon of 'groupthink', alleged to have occurred, notoriously, during the Bay of Pigs fiasco."
"Groupthink by Compulsion [...] [G]roupthink at least implies voluntarism. When this fails, the organization is not above outright intimidation. [...] In [a nationwide telecommunications company], refusal by the new hires to cheer on command incurred consequences not unlike the indoctrination and brainwashing techniques associated with a Soviet-era gulag."

To make groupthink testable, Irving Janis devised eight symptoms indicative of groupthink:
Type I: Overestimations of the group — its power and morality
Illusions of invulnerability creating excessive optimism and encouraging risk taking.
Unquestioned belief  in the morality of the group, causing members to ignore the consequences of their actions.
Type II: Closed-mindedness
Rationalizing warnings that might challenge the group's assumptions.
Stereotyping those who are opposed to the group as weak, evil, biased, spiteful, impotent, or stupid.
Type III: Pressures toward uniformity
Self-censorship of ideas that deviate from the apparent group consensus.
Illusions of unanimity among group members, silence is viewed as agreement.
Direct pressure to conform placed on any member who questions the group, couched in terms of "disloyalty".
Mindguards— self-appointed members who shield the group from dissenting information.
When a group exhibits most of the symptoms of groupthink, the consequences of a failing decision process can be expected: incomplete analysis of the other options, incomplete analysis of the objectives, failure to examine the risks associated with the favored choice, failure to reevaluate the options initially rejected, poor information research, selection bias in available information processing, failure to prepare for a back-up plan.

Janis identified three antecedent conditions to groupthink:
High group cohesiveness: Cohesiveness is the main factor that leads to groupthink. Groups that lack cohesiveness can of course make bad decisions, but they do not experience groupthink. In a cohesive group, members avoid speaking out against decisions, avoid arguing with others, and work towards maintaining friendly relationships in the group. If cohesiveness gets to such a level that there are no longer disagreements between members, then the group is ripe for groupthink.
Deindividuation: Group cohesiveness becomes more important than individual freedom of expression.
Illusions of unanimity: Members perceive falsely that everyone agrees with the group's decision; silence is seen as consent. Janis noted that the unity of group members was mere illusion. Members may disagree with the organizations' decision, but go along with the group for many reasons, such as maintaining their group status and avoiding conflict with managers or workmates. Such members think that suggesting opinions contrary to others may lead to isolation from the group.
Structural faults: The group is organized in ways that disrupt the communication of information, or the group carelessly makes decisions.
Insulation of the group: This can promote the development of unique, inaccurate perspectives on issues the group is dealing with, which can then lead to faulty solutions to the problem.
Lack of impartial leadership: Leaders control the group discussion, by planning what will be discussed, allowing only certain questions to be asked, and asking for opinions of only certain people in the group. Closed-style leadership is when leaders announce their opinions on the issue before the group discusses the issue together. Open-style leadership is when leaders withhold their opinion until a later time in the discussion. Groups with a closed-style leader are more biased in their judgments, especially when members had a high degree of certainty.
Lack of norms requiring methodological procedures.
Homogeneity of members' social backgrounds and ideology.
Situational context:
Highly stressful external threats: High-stake decisions can create tension and anxiety; group members may cope with this stress in irrational ways. Group members may rationalize their decision by exaggerating the positive consequences and minimizing the possible negative consequences. In attempt to minimize the stressful situation, the group decides quickly and allows little to no discussion or disagreement. Groups under high stress are more likely to make errors, lose focus of the ultimate goal, and use procedures that members know have not been effective in the past.
Recent failures: These can lead to low self-esteem, resulting in agreement with the group for fear of being seen as wrong.
Excessive difficulties in decision-making tasks.
Moral dilemmas
Although it is possible for a situation to contain all three of these factors, all three are not always present even when groupthink is occurring. Janis considered a high degree of cohesiveness to be the most important antecedent to producing groupthink, and always present when groupthink was occurring; however, he believed high cohesiveness would not always produce groupthink. A very cohesive group abides with all group norms; but whether or not groupthink arises is dependent on what the group norms are. If the group encourages individual dissent and alternative strategies to problem solving, it is likely that groupthink will be avoided even in a highly cohesive group. This means that high cohesion will lead to groupthink only if one or both of the other antecedents is present, situational context being slightly more likely than structural faults to produce groupthink.
A 2018 study found that absence of a tenured project leader can also create conditions for groupthink to prevail. Presence of an "experienced" project manager can reduce the likelihood of groupthink by taking steps like critically analysing ideas, promoting open communication, encouraging diverse perspectives, and raising team awareness of groupthink symptoms.
It was found that among people who have bicultural identity, those with highly integrated bicultural identity as opposed to less integrated were more prone to groupthink. In another 2022 study in Tanzania, Hofstede's cultural dimensions come into play. It was observed that in high power distance societies, individuals are hesitant to voice dissent, deferring to leaders' preferences in making decisions. Furthermore, as Tanzania is a collectivist society, community interests supersede those of individuals. The combination of high power distance and collectivism creates optimal conditions for groupthink to occur.

As observed by Aldag and Fuller (1993), the groupthink phenomenon seems to rest on a set of unstated and generally restrictive assumptions:
The purpose of group problem solving is mainly to improve decision quality
Group problem solving is considered a rational process.
Benefits of group problem solving:
variety of perspectives
more information about possible alternatives
better decision reliability
dampening of biases
social presence effects
Groupthink prevents these benefits due to structural faults and provocative situational context
Groupthink prevention methods will produce better decisions
An illusion of well-being is presumed to be inherently dysfunctional.
Group pressures towards consensus lead to concurrence-seeking tendencies.
It has been thought that groups with the strong ability to work together will be able to solve dilemmas in a quicker and more efficient fashion than an individual. Groups have a greater amount of resources which lead them to be able to store and retrieve information more readily and come up with more alternative solutions to a problem. There was a recognized downside to group problem solving in that it takes groups more time to come to a decision and requires that people make compromises with each other. However, it was not until the research of Janis appeared that anyone really considered that a highly cohesive group could impair the group's ability to generate quality decisions. Tight-knit groups may appear to make decisions better because they can come to a consensus quickly and at a low energy cost; however, over time this process of decision-making may decrease the members' ability to think critically. It is, therefore, considered by many to be important to combat the effects of groupthink.
According to Janis, decision-making groups are not necessarily destined to groupthink. He devised ways of preventing groupthink:
Leaders should assign each member the role of "critical evaluator". This allows each member to freely air objections and doubts.
Leaders should not express an opinion when assigning a task to a group.
Leaders should absent themselves from many of the group meetings to avoid excessively influencing the outcome.
The organization should set up several independent groups, working on the same problem.
All effective alternatives should be examined.
Each member should discuss the group's ideas with trusted people outside of the group.
The group should invite outside experts into meetings. Group members should be allowed to discuss with and question the outside experts.
At least one group member should be assigned the role of devil's advocate. This should be a different person for each meeting.
The devil's advocate in a group may provide questions and insight which contradict the majority group in order to avoid groupthink decisions. A study by Ryan Hartwig confirms that the devil's advocacy technique is very useful for group problem-solving. It allows for conflict to be used in a way that is most-effective for finding the best solution so that members will not have to go back and find a different solution if the first one fails. Hartwig also suggests that the devil's advocacy technique be incorporated with other group decision-making models such as the functional theory to find and evaluate alternative solutions. The main idea of the devil's advocacy technique is that somewhat structured conflict can be facilitated to not only reduce groupthink, but to also solve problems.
Diversity of all kinds is also instrumental in preventing groupthink. Individuals with varying backgrounds, thought, professional and life experiences etc. can offer unique perspectives and challenge assumptions.  In a 2004 study, a diverse team of problem-solver outperformed a team consisting of best problem solvers as they start to think alike.
Joris Graff offered a new debate format designed to prevent groupthink from occurring in a classroom setting specifically regarding debate lessons. He agreed that greater diversity in arguments both within a team and against an opposing side would prevent groupthink and suggested several ways to introduce that diversity into debates. Graff also suggested that the goal of debates should be on consensus or compromise over designating a winner. He argues that encouraging opposing teams to work together to come up with a viable solution prevents common arguments from becoming the only arguments used due to perceived success at getting a specific desired outcome.
Psychological safety, emphasized by Edmondson and Lei and Hirak et al., is crucial for effective group performance. It involves creating an environment that encourages learning and removes barriers perceived as threats by team members. Edmondson et al. demonstrated variations in psychological safety based on work type, hierarchy, and leadership effectiveness, highlighting its importance in employee development and fostering a culture of learning within organizations.
A similar situation to groupthink is the Abilene paradox, another phenomenon that is detrimental when working in groups. When organizations fall into an Abilene paradox, they take actions in contradiction to what their perceived goals may be and therefore defeat the very purposes they are trying to achieve. Failure to communicate desires or beliefs can cause an Abilene paradox.

The Watergate scandal is an example of this. Before the scandal had occurred, a meeting took place where they discussed the issue. One of Nixon's campaign aides was unsure if he should speak up and give his input. If he had voiced his disagreement with the group's decision, it is possible that the scandal could have been avoided.
After the Bay of Pigs invasion fiasco, President John F. Kennedy sought to avoid groupthink during the Cuban Missile Crisis using "vigilant appraisal". During meetings, he invited outside experts to share their viewpoints, and allowed group members to question them carefully. He also encouraged group members to discuss possible solutions with trusted members within their separate departments, and he even divided the group up into various sub-groups, to partially break the group cohesion. Kennedy was deliberately absent from the meetings, so as to avoid pressing his own opinion.
Cass Sunstein reports that introverts can sometimes be silent in meetings with extroverts; he recommends explicitly asking for each person's opinion, either during the meeting or afterwards in one-on-one sessions. Sunstein points to studies showing groups with a high level of internal socialization and happy talk are more prone to bad investment decisions due to groupthink, compared with groups of investors who are relative strangers and more willing to be argumentative. To avoid group polarization, where discussion with like-minded people drives an outcome further to an extreme than any of the individuals favored before the discussion, he recommends creating heterogeneous groups which contain people with different points of view. Sunstein also points out that people arguing a side they do not sincerely believe (in the role of devil's advocate) tend to be much less effective than a sincere argument. This can be accomplished by dissenting individuals, or a group like a Red Team that is expected to pursue an alternative strategy or goal "for real".

Testing groupthink in a laboratory is difficult because synthetic settings remove groups from real social situations, which ultimately changes the variables conducive or inhibitive to groupthink. Because of its subjective nature, researchers have struggled to measure groupthink as a complete phenomenon, instead frequently opting to measure its particular factors. These factors range from causal to effectual and focus on group and situational aspects.
Park (1990) found that "only 16 empirical studies have been published on groupthink", and concluded that they "resulted in only partial support of his [Janis's] hypotheses". Park concludes, "despite Janis' claim that group cohesiveness is the major necessary antecedent factor, no research has shown a significant main effect of cohesiveness on groupthink." Park also concludes that research does not support Janis' claim that cohesion and leadership style interact to produce groupthink symptoms. Park presents a summary of the results of the studies analyzed. According to Park, a study by Huseman and Drive (1979) indicates groupthink occurs in both small and large decision-making groups within businesses. This results partly from group isolation within the business. Manz and Sims (1982) conducted a study showing that autonomous work groups are susceptible to groupthink symptoms in the same manner as decisions making groups within businesses. Fodor and Smith (1982) produced a study revealing that group leaders with high power motivation create atmospheres more susceptible to groupthink. Leaders with high power motivation possess characteristics similar to leaders with a "closed" leadership style—an unwillingness to respect dissenting opinion. The same study indicates that level of group cohesiveness is insignificant in predicting groupthink occurrence. Park summarizes a study performed by Callaway, Marriott, and Esser (1985) in which groups with highly dominant members "made higher quality decisions, exhibited lowered state of anxiety, took more time to reach a decision, and made more statements of disagreement/agreement". Overall, groups with highly dominant members expressed characteristics inhibitory to groupthink. If highly dominant members are considered equivalent to leaders with high power motivation, the results of Callaway, Marriott, and Esser contradict the results of Fodor and Smith. A study by Leana (1985) indicates the interaction between level of group cohesion and leadership style is completely insignificant in predicting groupthink. This finding refutes Janis' claim that the factors of cohesion and leadership style interact to produce groupthink. Park summarizes a study by McCauley (1989) in which structural conditions of the group were found to predict groupthink while situational conditions did not. The structural conditions included group insulation, group homogeneity, and promotional leadership. The situational conditions included group cohesion. These findings refute Janis' claim about group cohesiveness predicting groupthink.
Overall, studies on groupthink have largely focused on the factors (antecedents) that predict groupthink. Groupthink occurrence is often measured by number of ideas/solutions generated within a group, but there is no uniform, concrete standard by which researchers can objectively conclude groupthink occurs. The studies of groupthink and groupthink antecedents reveal a mixed body of results. Some studies indicate group cohesion and leadership style to be powerfully predictive of groupthink, while other studies indicate the insignificance of these factors. Group homogeneity and group insulation are generally supported as factors predictive of groupthink.

Groupthink can have a strong hold on political decisions and military operations, which may result in enormous wastage of human and material resources. Highly qualified and experienced politicians and military commanders sometimes make very poor decisions when in a suboptimal group setting. Scholars such as Janis and Raven attribute political and military fiascoes, such as the Bay of Pigs Invasion, the Vietnam War, and the Watergate scandal, to the effect of groupthink. More recently, Dina Badie argued that groupthink was largely responsible for the shift in the U.S. administration's view on Saddam Hussein that eventually led to the 2003 invasion of Iraq by the United States. After the September 11 attacks, "stress, promotional leadership, and intergroup conflict" were all factors that gave rise to the occurrence of groupthink. Political case studies of groupthink serve to illustrate the impact that the occurrence of groupthink can have in today's political scene.

The United States Bay of Pigs Invasion of April 1961 was the primary case study that Janis used to formulate his theory of groupthink. The invasion plan was initiated by the Eisenhower administration, but when the Kennedy administration took over, it "uncritically accepted" the plan of the Central Intelligence Agency (CIA). When some people, such as Arthur M. Schlesinger Jr. and Senator J. William Fulbright, attempted to present their objections to the plan, the Kennedy team as a whole ignored these objections and kept believing in the morality of their plan. Eventually Schlesinger minimized his own doubts, performing self-censorship.  The Kennedy team stereotyped Fidel Castro and the Cubans by failing to question the CIA about its many false assumptions, including the ineffectiveness of Castro's air force, the weakness of Castro's army, and the inability of Castro to quell internal uprisings.
Janis argued the fiasco that ensued could have been prevented if the Kennedy administration had followed the methods to preventing groupthink adopted during the Cuban Missile Crisis, which took place just one year later in October 1962. In the latter crisis, essentially the same political leaders were involved in decision-making, but this time they learned from their previous mistake of seriously under-rating their opponents.

The attack on Pearl Harbor on December 7, 1941, is a prime example of groupthink. A number of factors such as shared illusions and rationalizations contributed to the lack of precaution taken by U.S. Navy officers based in Hawaii. The United States had intercepted Japanese messages and they discovered that Japan was arming itself for an offensive attack somewhere in the Pacific Ocean. Washington took action by warning officers stationed at Pearl Harbor, but their warning was not taken seriously. They assumed that the Empire of Japan was taking measures in the event that their embassies and consulates in enemy territories were usurped.
The U.S. Navy and Army in Pearl Harbor also shared rationalizations about why an attack was unlikely. Some of them included:
"The Japanese would never dare attempt a full-scale surprise assault against Hawaii because they would realize that it would precipitate an all-out war, which the United States would surely win."
"The Pacific Fleet concentrated at Pearl Harbor was a major deterrent against air or naval attack."
"Even if the Japanese were foolhardy enough to send their carriers to attack us [the United States], we could certainly detect and destroy them in plenty of time."
"No warships anchored in the shallow water of Pearl Harbor could ever be sunk by torpedo bombs launched from enemy aircraft."

On January 28, 1986, NASA launched the space shuttle Challenger. This was significant because a civilian, non-astronaut, high school teacher was to be the first American civilian in space. The space shuttle was perceived to be so safe as to make this possible. NASA's engineering and launch teams rely on teamwork. To launch the shuttle, individual team members must affirm each system is functioning nominally. Morton Thiokol engineers who designed and built the Challenger's rocket boosters ignored warnings that cooler temperature during the day of the launch could result in failure and death of the crew. The Space Shuttle Challenger Disaster grounded space shuttle flights for nearly three years. Ironic that this particular flight was to be a demonstration showing confidence in the safety of the space shuttle technology.
The Challenger case was subject to a more quantitatively oriented test of Janis's groupthink model performed by Esser and Lindoerfer, who found clear signs of positive antecedents to groupthink in the critical decisions concerning the launch of the shuttle. The day of the launch was rushed for publicity reasons. NASA wanted to captivate and hold the attention of America. Having civilian teacher Christa McAuliffe on board to broadcast a live lesson, and the possible mention by president Ronald Reagan in the State of the Union address, were opportunities NASA deemed critical to increasing interest in its potential civilian space flight program. The schedule NASA set out to meet was, however, self-imposed. It seemed incredible to many that an organization with a perceived history of successful management would have locked itself into a schedule it had no chance of meeting.

In the corporate world, ineffective and suboptimal group decision-making can negatively affect the health of a company and cause a considerable amount of monetary loss.

Aaron Hermann and Hussain Rammal illustrate the detrimental role of groupthink in the collapse of Swissair, a Swiss airline company that was thought to be so financially stable that it earned the title the "Flying Bank". The authors argue that, among other factors, Swissair carried two symptoms of groupthink: the belief that the group is invulnerable and the belief in the morality of the group. In addition, before the fiasco, the size of the company board was reduced, subsequently eliminating industrial expertise. This may have further increased the likelihood of groupthink. With the board members lacking expertise in the field and having somewhat similar background, norms, and values, the pressure to conform may have become more prominent. This phenomenon is called group homogeneity, which is an antecedent to groupthink. Together, these conditions may have contributed to the poor decision-making process that eventually led to Swissair's collapse.

Recent literature of groupthink attempts to study the application of this concept beyond the framework of business and politics. One particularly relevant and popular arena in which groupthink is rarely studied is sports. The lack of literature in this area prompted Charles Koerber and Christopher Neck to begin a case-study investigation that examined the effect of groupthink on the decision of the Major League Umpires Association (MLUA) to stage a mass resignation in 1999. The decision was a failed attempt to gain a stronger negotiating stance against Major League Baseball. Koerber and Neck suggest that three groupthink symptoms can be found in the decision-making process of the MLUA. First, the umpires overestimated the power that they had over the baseball league and the strength of their group's resolve. The union also exhibited some degree of closed-mindedness with the notion that MLB is the enemy. Lastly, there was the presence of self-censorship; some umpires who disagreed with the decision to resign failed to voice their dissent. These factors, along with other decision-making defects, led to a decision that was suboptimal and ineffective.

Researcher Robert Baron (2005) contends that the connection between certain antecedents which Janis believed necessary has not been demonstrated by the current collective body of research on groupthink. He believes that Janis' antecedents for groupthink are incorrect, and argues that not only are they "not necessary to provoke the symptoms of groupthink, but that they often will not even amplify such symptoms". As an alternative to Janis' model, Baron proposed a ubiquity model of groupthink. This model provides a revised set of antecedents for groupthink, including social identification, salient norms, and low self-efficacy.

Aldag and Fuller (1993) argue that the groupthink concept was based on a "small and relatively restricted sample" that became too broadly generalized. Furthermore, the concept is too rigidly staged and deterministic. Empirical support for it has also not been consistent. The authors compare groupthink model to findings presented by Maslow and Piaget; they argue that, in each case, the model incites great interest and further research that, subsequently, invalidate the original concept. Aldag and Fuller thus suggest a new model called the general group problem-solving (GGPS) model, which integrates new findings from groupthink literature and alters aspects of groupthink itself. The primary difference between the GGPS model and groupthink is that the former is more value neutral and more political.

Later scholars have re-assessed the merit of groupthink by reexamining case studies that Janis originally used to buttress his model. Roderick Kramer (1998) believed that, because scholars today have a more sophisticated set of ideas about the general decision-making process and because new and relevant information about the fiascos have surfaced over the years, a reexamination of the case studies is appropriate and necessary. He argues that new evidence does not support Janis' view that groupthink was largely responsible for President Kennedy's and President Johnson's decisions in the Bay of Pigs Invasion and U.S. escalated military involvement in the Vietnam War, respectively. Both presidents sought the advice of experts outside of their political groups more than Janis suggested. Kramer also argues that the presidents were the final decision-makers of the fiascos; while determining which course of action to take, they relied more heavily on their own construals of the situations than on any group-consenting decision presented to them. Kramer concludes that Janis' explanation of the two military issues is flawed and that groupthink has much less influence on group decision-making than is popularly believed.
Groupthink, while it is thought to be avoided, does have some positive effects. Choi and Kim found that group identity traits such as believing in the group's moral superiority, were linked to less concurrence seeking, better decision-making, better team activities, and better team performance. This study also showed that the relationship between groupthink and defective decision making was insignificant. These findings mean that in the right circumstances, groupthink does not always have negative outcomes. It also questions the original theory of groupthink.

Scholars are challenging the original view of groupthink proposed by Janis.
Whyte (1998) argues that a group's collective efficacy, i.e. confidence in its abilities, can lead to reduced vigilance and a higher risk tolerance, similar to how groupthink was described. McCauley (1998) proposes that the attractiveness of group members might be the most prominent factor in causing poor decisions. Turner and Pratkanis (1991) suggest that from social identity perspective, groupthink can be seen as a group's attempt to ward off potentially negative views of the group. Together, the contributions of these scholars have brought about new understandings of groupthink that help reformulate Janis' original model.

According to a theory many of the basic characteristics of groupthink – e.g., strong cohesion, indulgent atmosphere, and exclusive ethos – are the result of a special kind of mnemonic encoding (Tsoukalas, 2007). Members of tightly knit groups have a tendency to represent significant aspects of their community as episodic memories and this has a predictable influence on their group behavior and collective ideology, as opposed to what happens when they are encoded as semantic memories (which is common in formal and more loose group formations).

According to scientist Todd Rose, Collective Illusions and Groupthink are linked concepts that show how social dynamics affect behavior. Groupthink occurs when individuals who are right about what the group wants, conform to the group's consensus. Collective illusions are a specific form of Groupthink where individuals mistakenly assume the group's wants, leading everyone to behave in ways that don't reflect their true preferences. Both the concepts involve social influence and conformity.

In the 1979 religious satire Monty Python's Life of Brian, the concept of groupthink is satirized through the reactions of the crowds to Brian and his would-be followers: when he urges them that "You don't need to follow me. You don't need to follow anybody. You've got to think for yourselves. You're all individuals.", they respond in unison "Yes. We're all different." (with one voice saying "I'm not.") The film highlights how easily people can be swayed by charismatic figures, adopt a single, often illogical viewpoint, and blindly follow without individual thought.

A self-serving bias is any cognitive or perceptual process that is distorted by the need to maintain and enhance self-esteem, or the tendency to perceive oneself in an overly favorable manner. It is the belief that individuals tend to ascribe success to their own abilities and efforts, but ascribe failure to external factors. When individuals reject the validity of negative feedback, focus on their strengths and achievements but overlook their faults and failures, or take more credit for their group's work than they give to other members, they are protecting their self-esteem from threat and injury. These cognitive and perceptual tendencies perpetuate illusions and error, but they also serve the self's need for esteem.  For example, a student who attributes earning a good grade on an exam to their own intelligence and preparation but attributes earning a poor grade to the teacher's poor teaching ability or unfair test questions might be exhibiting a self-serving bias. Studies have shown that similar attributions are made in various situations, such as the workplace, interpersonal relationships, sports, and consumer decisions.
Both motivational processes (i.e. self-enhancement, self-presentation) and cognitive processes (i.e. locus of control, self-esteem) influence the self-serving bias. There are both cross-cultural (i.e. individualistic and collectivistic culture differences) and special clinical population (i.e. depression) considerations within the bias. Much of the research on the self-serving bias has used participant self-reports of attribution based on experimental manipulation of task outcomes or in naturalistic situations. Some more modern research, however, has shifted focus to physiological manipulations, such as emotional inducement and neural activation, in an attempt to better understand the biological mechanisms that contribute to the self-serving bias.

The theory of self-serving biases first came to attention in the late 1960s to early 1970s. As research on this topic grew, some people had concerns about it. In 1971, a fear emerged that the hypothesis would prove to be incorrect, much like the perceptual defense hypothesis by Dixon. However, the theory now holds strong. When this theory was still being developed it was during the research of attribution bias. Fritz Heider found that in ambiguous situations people made attributions based on their own needs, in order to maintain a higher self-esteem and viewpoint. This specific tendency became what we now know as the self-serving bias. Miller and Ross conducted a study in 1975 that was one of the earliest to assess not only self-serving bias but also the attributions for successes and failures within this theory. They argued that the self-serving bias people create is rational and not dependent on one's need for self-esteem. This means that if the outcome of an event is consistent with the person's expectation, then they will attribute dispositional (internal) factors. On the other hand, if the outcome of the event does not match the person's expectations, they will make situational attributions by blaming their surroundings instead of themselves.

Investigations of the self-serving bias in the laboratory differ depending on the experimental goals, but have basic fundamental aspects. Participants perform some task, often of intelligence, social sensitivity, teaching ability, or therapy skills. Participants may be asked to work alone, in pairs, or in groups. After task completion, participants are given randomized bogus feedback. Some studies employ emotion-induction mechanisms to investigate moderating effects on the self-serving bias. Finally, participants make attributions for the given outcomes. These attributions are assessed by the researchers to determine implications for the self-serving bias.

Some more modern testing employs neural imaging techniques to supplement the fundamental self-serving bias laboratory procedures. Neural correlates of the self-serving bias have been investigated by electroencephalography (EEG), as well as functional magnetic resonance imaging (fMRI). These procedures allow for insight into brain area activity during exhibition of a self-serving bias, as well as a mechanism to differentiate brain activity between healthy and clinical populations.

Retrospective performance outcomes can be used in investigation of the self-serving bias. An example of this is reported company performance followed up by self-report of outcome attributions. These self-report attributions can then be used to assess how successes and failures are viewed by company employees and executives. This method can be used for numerous outcome variables to determine the presence or absence of the self-serving bias.

Two types of motivation affect the self-serving bias: self-enhancement and self-presentation. Self-enhancement aims to uphold self-worth; attributing successes internally and failures externally helps people in their self-enhancement. Self-presentation refers to the drive to convey a desired image to others and make self-serving attributions to manage impressions. For example, they claim personal responsibility for successes but not failures in an attempt to influence how others perceive them. Motivation works in conjunction with cognitive factors to produce personally satisfying and self-preserving attributions for outcomes.

Locus of control is one of the main influences of attribution style. Individuals with an internal locus of control believe that they have personal control over situations and that their actions matter. Those with an external locus of control believe that outside forces, chance, and luck determine situations and that their actions cannot change anything. Individuals with an external locus of control are more likely to exhibit a self-serving bias following failure than those with an internal locus of control. The difference in attribution style between individuals with internal and external loci of control, however, is not as marked in successful outcomes, as individuals with both types attribution style have less need to defend their self-images in success. Airplane pilots with an internal locus of control were likely to exhibit a self-serving bias in regard to their skill and levels of safety.

Studies have shown a slight discrepancy in males' and females' use of the self-serving bias. In self-report surveys investigating partner interactions of romantic couples, men tended to attribute negative interactions to their partners more than women did. This is evidence that men may exhibit the self-serving bias more than women, although the study did not look at positive interaction attributions.

Older adults have been shown to make more internal causal attributions for negative outcomes. Differential attribution style at different ages indicates that the self-serving bias may be less likely in older adults. These older adults who attributed negative outcomes to more internal factors also rated themselves to be in poorer health, so negative emotional factors may confound the found age effects.

There is evidence of cross-cultural differences in the tendency to exhibit the self-serving bias, particularly when considering individualistic (Western) versus collectivistic (non-Western) societies. Family and group goals are important in collectivistic cultures. In contrast, the individual goals and identity focused on in individualistic societies increases the need for people within those cultures to guard and boost their personal self-esteem. While differences have been shown, conflicting literature has cited similarity in causal attributions across both individual and collective cultures, specifically between Belgium, West Germany, South Korea, and England. Naturalistic observation and information comparing United States and Japanese companies outcome attributions shows that the meaning and psychological function of internal versus external attributions are similar across cultures but that the difference is in the strategy of attribution. No consensus has been reached on cross-culture influences on the self-serving bias, though some systematic differences do seem to be present, especially between Western and non-Western cultures. For example, a study conducted by Kudo and Numuzaki showed that the participants in the success condition provided more internal attributions than the participants in the failure condition even though past research has constantly shown that Japanese people do not tend to show a self-serving bias. Another study conducted by Hugten and Witteloostuijn displayed the results that student participants between the ages of 13 and 15 who mainly process feedback in a non-native English tend to show more self-serving bias than those who process feedback in their native Dutch language.

Investigations of self-serving bias distinguish between the role of participants as the actor of a task or as the observer of someone else performing a task, relating closely to actor–observer asymmetry. Actors of a task exhibit the self-serving bias in their attributions to their own success or failure feedback, whereas observers do not make the same attributions about another person's task outcome. Observers tend to be more objective in their tendency to ascribe internal or external attributions to other people's outcomes. This may be due to the fact that the self-image of actors is challenged directly and therefore actors feel the need to protect their own self-image, but do not feel the same inclination to do so when the self-image of others is threatened.

Emotions can influence feelings of self-esteem, which in turn alters the need to protect one's self-identity. Individuals with higher self-esteem are thought to have more to protect in their self-image, and therefore exhibit the self-serving bias more often than those individuals with lower self-esteem. In a study, participants who were induced to feel the emotions of guilt or revulsion were less likely to make self-serving attributions for success and less likely to make self-protecting attributions for failure. Coleman concluded that the two emotions of guilt and revulsion lead to a drop in self-esteem, and thus a reduction in the use of the self-serving bias.

The relationship between individuals' awareness levels and perceived probability of improvement also influences the activation of the self-serving bias. Individuals with high self-awareness attribute failure internally when they perceive a high probability of improvement. However, they will engage in self-serving bias, attributing failure externally when they perceive a low probability of improvement. Individuals low in self-awareness will attribute failure externally regardless of their perceived probability of improvement.

Whether the self-serving bias is exhibited may depend on interpersonal closeness, relationships in a social context. When working in pairs to complete interdependent outcome tasks, relationally close pairs did not show a self-serving bias while relationally distant pairs did. A study on self-serving bias in relational context suggests this is due to the idea that close relationships place limits on an individual's self enhancement tendencies.  The individual becomes more modest, when in a close relationship, and is less likely to use that relationship for his or her own benefit. Understanding why partners refrain from the self-serving bias is still in question but can partially be explained by favorable impression those in close relationships have for one another. A similar result was shown when looking at pairs of friends and strangers. Pairs performed an interdependent outcomes creativity test and were then given a bogus pipeline for a success or failure outcome. Strangers exhibited the self-serving bias in responsibility attributions, but friends tended to make joint attributions for both success and failure. Researchers have taken this as evidence for "boundaries on self-enhancement". In another study conducted in 2016, the implicit and explicit evaluation of 108 partners and exes as parents who were either married, separated or divorced was researched to investigate if the self-serving bias influenced them. Using two Implicit Association tests, one measuring Self vs Partner and the other measuring Self vs Ex, results showed that most of the time, men and women consider their exes or partners as less suitable parents which demonstrates the self-serving bias as they "ascribe more easily successes to internal factors" and failures towards their partners. Also, another result demonstrated that "women revealed a higher self-serving bias than men on both implicit and explicit measures, but only toward exes and not toward current partners" because they consider their exes as an out-group and their partners as an in-group.

The self-serving bias can be found in several aspects of the workplace. Research shows that the self-serving bias is used to explain employment: being hired for a job is attributed to personal factors, whereas failure to obtain a job is attributed to external factors. Experimental investigation of the explanations for unemployment through asking participants to imagine particular job opportunities and likelihood of getting those jobs, however, did not show such a self-serving bias. Researchers claim that this may be due to the actor-observer role differences in the self-serving bias. Within the workplace, victims of serious occupational accidents tend to attribute their accidents to external factors, whereas their coworkers and management tend to attribute the accidents to the victims' own actions. Interpersonal dynamics of the self-serving bias in the previous section have implications for attributions for outcomes in the workplace. In an investigation of group dynamics, virtual group members had to complete a decision-making task via computer-mediated communication. Results showed that the self-serving bias was present in negative outcomes, and that greater interpersonal distance from group members increased blame for negative outcomes.

Studies revealed that narcissism was related to enhanced self-ratings of leadership, even when controlling for the Big Five traits. Another study showed that narcissism was related to enhanced leadership self-perceptions; indeed, whereas narcissism was significantly positively correlated with self-ratings of leadership, it was significantly negatively related to other ratings of leadership. This study also revealed that narcissism was related to more favorable self-ratings of workplace deviance and contextual performance compared to other (supervisor) ratings.  Because narcissism broadly reflects strong self-admiration and behavioral tendencies which may not be viewed positively by others it is possible that narcissism influences self- and other perceptions differently, and insight into this possibility may be important given that differences in perceptions are the foundation for certain types of performance management and development practices.

Studies in both lab and field settings have shown both teachers and students hold self-serving biases in regard to outcomes in the classroom. These attributions hold the potential for conflict between teacher and student since neither will take personal responsibility, as the student may blame the teacher while the teacher holds the student accountable. However, both teachers and students also reported being aware of the others' bias, which indicated there may be a feasible conflict resolution mechanism.

Computers have become an integral part of everyday life, and research has shown that individuals may subconsciously treat interactions with computers as they would treat a social situation. This finding combined with what is known about the self-serving bias in interpersonal relations indicates that consumers that use a computer to buy products will take personal credit for successful purchases but blame the computer for negative purchase experiences. It was also found, however, that consumers are more willing to attribute successful purchases to the computer and not ascribe blame to the computer for failed purchases if they have "intimate self-disclosure" with the computer, which Moon describes as revelation of personal information that makes the discloser feel vulnerable. Another reason is that people are so used to bad functionality, counterintuitive features, bugs, and sudden crashes of most contemporary software applications that they tend not to complain about computer problems. Instead, they believe it is their personal responsibility to predict possible issues and to find solutions to computer problems. This unique phenomenon has been recently observed in several human-computer interaction investigations.

Individuals have been shown to exhibit the self-serving bias in regard to sports outcomes. In one study, collegiate wrestlers at the Division I level made self-reported attributions of the outcomes in their preseason matches. Winners were more likely than losers to attribute the match outcome to internal causes. The researchers note that wrestling is a one-on-one sport and has clearly defined winners. Therefore, other sports of this nature may show similar results, but other team sports or sports with more ambiguous outcomes may not show the same pattern for the self-serving bias. In another study conducted in 1987, the research focused on comparing the self-serving attributions made by individuals who played single sports and those who played in teams. The study gathered 549 statements that were coded for attributional content from lone performers such as tennis and golf players and team performers such as baseball, football or basketball players. The results showed that "lone performers made more self-serving attributions than team performers" because their performance outcomes have a greater effect on their individual esteem unlike for group outcomes where it "must often be distributed among all participants." To expand upon self-serving attributions made by team sports, a study conducted in 1980 coded "newspaper accounts of baseball and football for attributional content." The coding of the newspaper accounts showed that there was a "tendency to make internal attributions for success and external attributions for failure" which supports the self-serving bias as about 75% of the attributions from winning teams were internal while about 55% of attributions from losing teams were internal.

Clinically depressed patients tend to show less of a self-serving bias than individuals in the general population. In a study exploring the effects of mood on the self-serving bias, the moods of participants were manipulated to be either positive or negative. Negative mood participants were less likely to attribute successful outcomes to the self than positive mood participants, attributing success to external factors. It has been suggested that the negative mood in depressed individuals as well as their self-focused attention explains why clinically depressed populations are less likely to exhibit the self-serving bias than normal populations.

Self-Serving bias is defined also by Kaplan et al. to individual's preferences, which effects to his/her beliefs in an optimistic way. Kaplan et al. also defines, that SSB should be termed as 'self-defeating' phenomenon as it appears for persons, who does not give up. Furthermore, Kaplan et al. state, that specific type of self-serving bias is wishful thinking. This is typically present, when an unpleasant surprise pops up in the negotiation, particularly when the opponent has made preparations carefully. Another example is well-known phenomenon from law-court and is commonly used also in law-based TV-series. We can say, that according to Kaplan et al.'s finding, that self-serving bias is playing a major role in negotiation context. There is an emotional effect to the negotiations and there seems to be a reflection between self-serving bias and emotions.

The self-serving bias has been investigated by the fMRI method in normal populations. Attributions using the bias show activation in the dorsal striatum, which plays a role in motivated behavior, as well as in the dorsal anterior cingulate. In clinically depressed patients, there appear to be weaker connections between the dorsomedial prefrontal cortex and limbic areas of the brain, so this connection may play a role in self-serving attributions.

In a study employing the EEG method of examining brain activation, participants were given bogus outcome feedback that indicated either success or failure and told to make attributions. Different from non-self-serving responses, self-serving responses did not show increased dorsomedial frontal cortex activity preceding attribution decisions. Such lack of brain activity implies that self-control, which is controlled by the dorsomedial frontal cortex, is not as prominent in self-serving attributions as non-self-serving ones.

The base rate fallacy, also called base rate neglect or base rate bias, is a type of fallacy in which people tend to ignore the base rate (e.g., general prevalence) in favor of the  information pertaining only to a specific case. Base rate neglect is a specific form of the more general extension neglect.
It is also called the prosecutor's fallacy or defense attorney's fallacy when applied to the results of statistical tests (such as DNA tests) in the context of law proceedings. These terms were introduced by William C. Thompson and Edward Schumann in 1987, although it has been argued that their definition of the prosecutor's fallacy extends to many additional invalid imputations of guilt or liability that are not analyzable as errors in base rates or Bayes's theorem.

An example of the base rate fallacy is the false positive paradox (also known as accuracy paradox). This paradox describes situations where there are more false positive test results than true positives (this means the classifier has a low precision). For example, if a facial recognition camera can identify wanted criminals 99% accurately, but analyzes 10,000 people a day, the high accuracy is outweighed by the number of tests; because of this, the program's list of criminals will likely have far more innocents (false positives) than criminals (true positives) because there are far more innocents than criminals overall. The probability of a positive test result is determined not only by the accuracy of the test but also by the characteristics of the sampled population. The fundamental issue is that the far higher prevalence of true negatives means that the pool of people testing positively will be dominated by false positives, given that even a small fraction of the much larger [negative] group will produce a larger number of indicated positives than the larger fraction of the much smaller [positive] group.
When the prevalence, the proportion of those who have a given condition, is lower than the test's false positive rate, even tests that have a very low risk of giving a false positive in an individual case will give more false than true positives overall.
It is especially counter-intuitive when interpreting a positive result in a test on a low-prevalence population after having dealt with positive results drawn from a high-prevalence population. If the false positive rate of the test is higher than the proportion of the new population with the condition, then a test administrator whose experience has been drawn from testing in a high-prevalence population may conclude from experience that a positive test result usually indicates a positive subject, when in fact a false positive is far more likely to have occurred.

Imagine running an infectious disease test on a population A of 1,000 persons, of which 40% are infected. The test has a false positive rate of 5% (0.05) and a false negative rate of zero. The expected outcome of the 1,000 tests on population A would be:
So, in population A, a person receiving a positive test could be over 93% confident (⁠400/30 + 400⁠) that it correctly indicates infection.

Now consider the same test applied to population B, of which only 2% are infected. The expected outcome of 1000 tests on population B would be:
In population B, only 20 of the 69 total people with a positive test result are actually infected. So, the probability of actually being infected after one is told that one is infected is only 29% (⁠20/20 + 49⁠) for a test that otherwise appears to be "95% accurate".
A tester with experience of group A might find it a paradox that in group B, a result that had usually correctly indicated infection is now usually a false positive. The confusion of the posterior probability of infection with the prior probability of receiving a false positive is a natural error after receiving a health-threatening test result.

Imagine that a group of police officers have breathalyzers displaying false drunkenness in 5% of the cases in which the driver is sober. However, the breathalyzers never fail to detect a truly drunk person. One in a thousand drivers is driving drunk. Suppose the police officers then stop a driver at random to administer a breathalyzer test. It indicates that the driver is drunk. No other information is known about them.
Many would estimate the probability that the driver is drunk as high as 95%, but the correct probability is about 2%.
An explanation for this is as follows: on average, for every 1,000 drivers tested,
1 driver is drunk, and it is 100% certain that for that driver there is a true positive test result, so there is 1 true positive test result
999 drivers are not drunk, and among those drivers there are 5% false positive test results, so there are 49.95 false positive test results
The validity of this result does, however, hinge on the validity of the initial assumption that the police officer stopped the driver truly at random, and not because of bad driving. If that or another non-arbitrary reason for stopping the driver was present, then the calculation also involves the probability of a drunk driver driving competently and a non-drunk driver driving (in-)competently.
More formally, the same probability of roughly 0.02 can be established using Bayes' theorem. The goal is to find the probability that the driver is drunk given that the breathalyzer indicated they are drunk, which can be represented as
where D means that the breathalyzer indicates that the driver is drunk. Using Bayes's theorem,
The following information is known in this scenario:
As can be seen from the formula, one needs p(D) for Bayes' theorem, which can be computed from the preceding values using the law of total probability:
which gives
Plugging these numbers into Bayes' theorem, one finds that
which is the precision of the test.

In a city of 1 million inhabitants, let there be 100 terrorists and 999,900 non-terrorists. To simplify the example, it is assumed that all people present in the city are inhabitants. Thus, the base rate probability of a randomly selected inhabitant of the city being a terrorist is 0.0001, and the base rate probability of that same inhabitant being a non-terrorist is 0.9999. In an attempt to catch the terrorists, the city installs an alarm system with a surveillance camera and automatic facial recognition software.
The software has two failure rates of 1%:
The false negative rate: If the camera scans a terrorist, a bell will ring 99% of the time, and it will fail to ring 1% of the time.
The false positive rate: If the camera scans a non-terrorist, a bell will not ring 99% of the time, but it will ring 1% of the time.
Suppose now that an inhabitant triggers the alarm. Someone making the base rate fallacy would infer that there is a 99% probability that the detected person is a terrorist. Although the inference seems to make sense, it is actually bad reasoning, and a calculation below will show that the probability of a terrorist is actually near 1%, not near 99%.
Imagine that the first city's entire population of one million people pass in front of the camera. About 99 of the 100 terrorists will trigger the alarm—and so will about 9,999 of the 999,900 non-terrorists. Therefore, about 10,098 people will trigger the alarm, among which about 99 will be terrorists. The probability that a person triggering the alarm actually is a terrorist is only about 99 in 10,098, which is less than 1% and very, very far below the initial guess of 99%.
The base rate fallacy is so misleading in this example because there are many more non-terrorists than terrorists, and the number of false positives (non-terrorists scanned as terrorists) is so much larger than the true positives (terrorists scanned as terrorists).
Multiple practitioners have argued that as the base rate of terrorism is extremely low, using data mining and predictive algorithms to identify terrorists cannot feasibly work due to the false positive paradox. Estimates of the number of false positives for each accurate result vary from over ten thousand to one billion; consequently, investigating each lead would be cost- and time-prohibitive. The level of accuracy required to make these models viable is likely unachievable. Foremost, the low base rate of terrorism also means there is a lack of data with which to make an accurate algorithm. Further, in the context of detecting terrorism false negatives are highly undesirable and thus must be minimised as much as possible; however, this requires increasing sensitivity at the cost of specificity, increasing false positives. It is also questionable whether the use of such models by law enforcement would meet the requisite burden of proof given that over 99% of results would be false positives.

A crime is committed. Forensic analysis determines that the perpetrator has a certain blood type shared by 10% of the population. A suspect is arrested, and found to have that same blood type.
A prosecutor might charge the suspect with the crime on that basis alone, and claim at trial that the probability that the defendant is guilty is 90%.
However, this conclusion is only close to correct if the defendant was selected as the main suspect based on robust evidence discovered prior to the blood test and unrelated to it. Otherwise, the reasoning presented is flawed, as it overlooks the high prior probability (that is, prior to the blood test) that he is a random innocent person. Assume, for instance, that 1000 people live in the town where the crime occurred. This means that 100 people live there who have the perpetrator's blood type, of whom only one is the true perpetrator; therefore, the true probability that the defendant is guilty – based only on the fact that his blood type matches that of the killer – is only 1%, far less than the 90% argued by the prosecutor.
The prosecutor's fallacy involves assuming that the prior probability of a random match is equal to the probability that the defendant is innocent. When using it, a prosecutor questioning an expert witness may ask: "The odds of finding this evidence on an innocent man are so small that the jury can safely disregard the possibility that this defendant is innocent, correct?" The claim assumes that the probability that evidence is found on an innocent man is the same as the probability that a man is innocent given that evidence was found on him, which is not true. Whilst the former is usually small (10% in the previous example) due to good forensic evidence procedures, the latter (99% in that example) does not directly relate to it and will often be much higher, since, in fact, it depends on the likely quite high prior odds of the defendant being a random innocent person.

O. J. Simpson was tried and acquitted in 1995 for the murders of his ex-wife Nicole Brown Simpson and her friend Ronald Goldman.
Crime scene blood matched Simpson's with characteristics shared by 1 in 400 people. However, the defense argued that the number of people from Los Angeles matching the sample could fill a football stadium and that the figure of 1 in 400 was useless. It would have been incorrect, and an example of prosecutor's fallacy, to rely solely on the "1 in 400" figure to deduce that a given person matching the sample would be likely to be the culprit.
In the same trial, the prosecution presented evidence that Simpson had been violent toward his wife. The defense argued that there was only one woman murdered for every 2500 women who were subjected to spousal abuse, and that any history of Simpson being violent toward his wife was irrelevant to the trial. However, the reasoning behind the defense's calculation was fallacious. According to author Gerd Gigerenzer, the correct probability requires additional context: Simpson's wife had not only been subjected to domestic violence, but rather subjected to domestic violence (by Simpson) and killed (by someone). Gigerenzer writes "the chances that a batterer actually murdered his partner, given that she has been killed, is about 8 in 9 or approximately 90%". While most cases of spousal abuse do not end in murder, most cases of murder where there is a history of spousal abuse were committed by the spouse.

Sally Clark, a British woman, was accused in 1998 of having killed her first child at 11 weeks of age and then her second child at 8 weeks of age. The prosecution had expert witness Sir Roy Meadow, a professor and consultant paediatrician, testify that the probability of two children in the same family dying from SIDS is about 1 in 73 million. That was much less frequent than the actual rate measured in historical data –  Meadow estimated it from single-SIDS death data, and the assumption that the probability of such deaths should be uncorrelated between infants.
Meadow acknowledged that 1-in-73 million is not an impossibility, but argued that such accidents would happen "once every hundred years" and that, in a country of 15 million 2-child families, it is vastly more likely that the double-deaths are due to Münchausen syndrome by proxy than to such a rare accident. However, there is good reason to suppose that the likelihood of a death from SIDS in a family is significantly greater if a previous child has already died in these circumstances, (a genetic predisposition to SIDS is likely to invalidate that assumed statistical independence) making some families more susceptible to SIDS and the error an outcome of the ecological fallacy. The likelihood of two SIDS deaths in the same family cannot be soundly estimated by squaring the likelihood of a single such death in all otherwise similar families.
The 1-in-73 million figure greatly underestimated the chance of two successive accidents, but even if that assessment were accurate, the court seems to have missed the fact that the 1-in-73 million number meant nothing on its own. As an a priori probability, it should have been weighed against the a priori probabilities of the alternatives. Given that two deaths had occurred, one of the following explanations must be true, and all of them are a priori extremely improbable:
Two successive deaths in the same family, both by SIDS
Double homicide (the prosecution's case)
Other possibilities (including one homicide and one case of SIDS)
It is unclear whether an estimate of the probability for the second possibility was ever proposed during the trial, or whether the comparison of the first two probabilities was understood to be the key estimate to make in the statistical analysis assessing the prosecution's case against the case for innocence.
Clark was convicted in 1999, resulting in a press release by the Royal Statistical Society which pointed out the mistakes.
In 2002, Ray Hill (a mathematics professor at Salford) attempted to accurately compare the chances of these two possible explanations; he concluded that successive accidents are between 4.5 and 9 times more likely than are successive murders, so that the a priori odds of Clark's guilt were between 4.5 to 1 and 9 to 1 against.
After the court found that the forensic pathologist who had examined both babies had withheld exculpatory evidence, a higher court later quashed Clark's conviction, on 29 January 2003.

In experiments, people have been found to prefer individuating information over general information when the former is available.
In some experiments, students were asked to estimate the grade point averages (GPAs) of hypothetical students. When given relevant statistics about GPA distribution, students tended to ignore them if given descriptive information about the particular student even if the new descriptive information was obviously of little or no relevance to school performance. This finding has been used to argue that interviews are an unnecessary part of the college admissions process because interviewers are unable to pick successful candidates better than basic statistics.
Psychologists Daniel Kahneman and Amos Tversky attempted to explain this finding in terms of a simple rule or "heuristic" called representativeness. They argued that many judgments relating to likelihood, or to cause and effect, are based on how representative one thing is of another, or of a category. Kahneman considers base rate neglect to be a specific form of extension neglect. Richard Nisbett has argued that some attributional biases like the fundamental attribution error are instances of the base rate fallacy: people do not use the "consensus information" (the "base rate") about how others behaved in similar situations and instead prefer simpler dispositional attributions.
There is considerable debate in psychology on the conditions under which people do or do not appreciate base rate information. Researchers in the heuristics-and-biases program have stressed empirical findings showing that people tend to ignore base rates and make inferences that violate certain norms of probabilistic reasoning, such as Bayes' theorem. The conclusion drawn from this line of research was that human probabilistic thinking is fundamentally flawed and error-prone. Other researchers have emphasized the link between cognitive processes and information formats, arguing that such conclusions are not generally warranted.
Consider again Example 2 from above. The required inference is to estimate the (posterior) probability that a (randomly picked) driver is drunk, given that the breathalyzer test is positive. Formally, this probability can be calculated using Bayes' theorem, as shown above. However, there are different ways of presenting the relevant information. Consider the following, formally equivalent variant of the problem:
 1 out of 1000 drivers are driving drunk. The breathalyzers never fail to detect a truly drunk person. For 50 out of the 999 drivers who are not drunk the breathalyzer falsely displays drunkenness. Suppose the policemen then stop a driver at random, and force them to take a breathalyzer test. It indicates that they are drunk. No other information is known about them. Estimate the probability the driver is really drunk.

In cognitive science and behavioral economics, loss aversion refers to a cognitive bias in which the same situation is perceived as worse if it is framed as a loss, rather than a gain. It should not be confused with risk aversion, which describes the rational behavior of valuing an uncertain outcome at less than its expected value.
When defined in terms of the pseudo-utility function as in cumulative prospect theory (CPT), the left-hand of the function increases much more steeply than gains, thus being more "painful" than the satisfaction from a comparable gain. Empirically, losses tend to be treated as if they were twice as large as an equivalent gain. Loss aversion was first proposed by Amos Tversky and Daniel Kahneman as an important component of prospect theory.

In 1979, Daniel Kahneman and his associate Amos Tversky originally coined the term "loss aversion" in their initial proposal of prospect theory as an alternative descriptive model of decision making under risk. "The response to losses is stronger than the response to corresponding gains" is Kahneman's definition of loss aversion.
After the first 1979 proposal in the prospect theory framework paper, Tversky and Kahneman used loss aversion for a paper in 1991 about a consumer choice theory that incorporates reference dependence, loss aversion, and diminishing sensitivity. Compared to the original paper above that discusses loss aversion in risky choices, Tversky and Kahneman (1991) discuss loss aversion in riskless choices, for instance, not wanting to trade or even sell something that is already in our possession. Here, "losses loom larger than gains" correspondingly reflects how outcomes below the reference level (e.g. what we do not own) loom larger than those above the reference level (e.g. what we own), showing people's tendency to value losses more than gains relative to a reference point. Additionally, the paper supported loss aversion with the endowment effect theory and status quo bias theory. Loss aversion was popular in explaining many phenomena in traditional choice theory. In 1980, loss aversion was used in Thaler (1980) regarding endowment effect. Loss aversion was also used to support the status quo bias in 1988, and the equity premium puzzle in 1995. In the 2000s, behavioural finance was an area with frequent application of this theory, including on asset prices and individual stock returns.

In marketing, the use of trial periods and rebates tries to take advantage of the buyer's tendency to value the good more after the buyer incorporates it in the status quo. In past behavioral economics studies, users participate up until the threat of loss equals any incurred gains. Methods established by Botond Kőszegi and Matthew Rabin in experimental economics illustrates the role of expectation, wherein an individual's belief about an outcome can create an instance of loss aversion, whether or not a tangible change of state has occurred. 
Whether a transaction is framed as a loss or as a gain is important to this calculation. The same change in price framed differently, for example as a $5 discount or as a $5 surcharge avoided, has a significant effect on consumer behavior. Although traditional economists consider this "endowment effect", and all other effects of loss aversion, to be completely irrational, it is important to the fields of marketing and behavioral finance. Users in behavioral and experimental economics studies decided to cease participation in iterative money-making games when the threat of loss was close to the expenditure of effort, even when the user stood to further their gains. Loss aversion coupled with myopia has been shown to explain macroeconomic phenomena, such as the equity premium puzzle. Loss aversion to kinship is an explanation for aversion to inheritance tax.

Loss aversion is part of prospect theory, a cornerstone in behavioral economics.  The theory explored numerous behavioral biases leading to sub-optimal decisions making. Kahneman and Tversky found that people are biased in their real estimation of probability of events happening. They tend to over-weight both low and high probabilities and under-weight medium probabilities.
One example is which option is more attractive between option A ($1,500 with a probability of 33%, $1,400 with a probability of 66%, and $0 with a probability of 1%) and option B (a guaranteed $920). Prospect theory and loss aversion suggests that most people would choose option B as they prefer the guaranteed $920 since there is a probability of winning $0, even though it is only 1%. This demonstrates that people think in terms of expected utility relative to a reference point (i.e. current wealth) as opposed to absolute payoffs. When choices are framed as risky (i.e. risk losing 1 out of 10 lives vs the opportunity to save 9 out of 10 lives), individuals tend to be loss-averse as they weigh losses more heavily than comparable gains.

Loss aversion was first proposed as an explanation for the endowment effect—the fact that people place a higher value on a good that they own than on an identical good that they do not own—by Kahneman, Knetsch, and Thaler (1990). Loss aversion and the endowment effect lead to a violation of the Coase theorem—that "the allocation of resources will be independent of the assignment of property rights when costless trades are possible".
In several studies, the authors demonstrated that the endowment effect could be explained by loss aversion but not five alternatives, namely transaction costs, misunderstandings, habitual bargaining behaviors, income effects, and trophy effects. In each experiment, half of the subjects were randomly assigned a good and asked for the minimum amount they would be willing to sell it for while the other half of the subjects were given nothing and asked for the maximum amount they would be willing to spend to buy the good. Since the value of the good is fixed and individual valuation of the good varies from this fixed value only due to sampling variation, the supply and demand curves should be perfect mirrors of each other and thus half the goods should be traded. The authors also ruled out the explanation that lack of experience with trading would lead to the endowment effect by conducting repeated markets.
The first two alternative explanation are that under-trading was due to transaction costs or misunderstanding—were tested by comparing goods markets to induced-value markets under the same rules. If it was possible to trade to the optimal level in induced value markets, under the same rules, there should be no difference in goods markets. The results showed drastic differences between induced-value markets and goods markets. The median prices of buyers and sellers in induced-value markets matched almost every time leading to near perfect market efficiency, but goods markets sellers had much higher selling prices than buyers' buying prices. This effect was consistent over trials, indicating that this was not due to inexperience with the procedure or the market. Since the transaction cost that could have been due to the procedure was equal in the induced-value and goods markets, transaction costs were eliminated as an explanation for the endowment effect.
The third alternative explanation was that people have habitual bargaining behaviors, such as overstating their minimum selling price or understating their maximum bargaining price, that may spill over from strategic interactions where these behaviors are useful to the laboratory setting where they are sub-optimal. An experiment was conducted to address this by having the clearing prices selected at random. Buyers who indicated a willingness-to-pay (WTP) higher than the randomly drawn price got the good, and vice versa for those who indicated a lower WTP. Likewise, sellers who indicated a lower willingness-to-accept than the randomly drawn price sold the good and vice versa. This incentive compatible value elicitation method did not eliminate the endowment effect but did rule out habitual bargaining behavior as an alternative explanation.
Income effects were ruled out by giving one third of the participants mugs, one third chocolates, and one third neither mug nor chocolate. They were then given the option of trading the mug for the chocolate or vice versa and those with neither were asked to merely choose between mug and chocolate. Thus, wealth effects were controlled for those groups who received mugs and chocolate. The results showed that 86% of those starting with mugs chose mugs, 10% of those starting with chocolates chose mugs, and 56% of those with nothing chose mugs. This ruled out income effects as an explanation for the endowment effect. Also, since all participants in the group had the same good, it could not be considered a "trophy", eliminating the final alternative explanation. Thus, the five alternative explanations were eliminated, the first two through induced-value market vs. consumption goods market, the third with incentive compatible value elicitation procedure, and the fourth and fifth through a choice between endowed or alternative good.

Multiple studies have questioned the existence of loss aversion. In several studies examining the effect of losses in decision-making,  no loss aversion was found under risk and uncertainty. There are several explanations for these findings: one is that loss aversion does not exist in small payoff magnitudes (called magnitude dependent loss aversion by Mukherjee et al.(2017); which seems to hold true for time as well. The other is that the generality of the loss aversion pattern is lower than previously thought. David Gal (2006) argued that many of the phenomena commonly attributed to loss aversion, including the status quo bias, the endowment effect, and the preference for safe over risky options, are more parsimoniously explained by psychological inertia than by a loss/gain asymmetry. Gal and Rucker (2018) made similar arguments. Mkrva, Johnson, Gächter, and Herrmann (2019) cast doubt on these critiques, replicating loss aversion in five unique samples while also showing how the magnitude of loss aversion varies in theoretically predictable ways. 
Loss aversion may be more salient when people compete. Gill and Prowse (2012) provide experimental evidence that people are loss averse around reference points given by their expectations in a competitive environment with real effort. Losses may also have an effect on attention but not on the weighting of outcomes; losses lead to more autonomic arousal than gains even in the absence of loss aversion. This latter effect is sometimes known as Loss Attention.

Loss attention refers to the tendency of individuals to allocate more attention to a task or situation when it involve losses than when it does not involve losses. What distinguishes loss attention from loss aversion is that it does not imply that losses are given more subjective weight (or utility) than gains. Moreover, under loss aversion losses have a biasing effect whereas under loss attention they can have a debiasing effect. Loss attention was proposed as a distinct regularity from loss aversion by Eldad Yechiam and Guy Hochman.
Specifically, the effect of losses is assumed to be on general attention rather than plain visual or auditory attention. The loss attention account assumes that losses in a given task mainly increase the general attentional resource pool available for that task. The increase in attention is assumed to have an inverse-U shape effect on performance (following the so called Yerkes-Dodson law). The inverse U-shaped effect implies that the effect of losses on performance is most apparent in settings where task attention is low to begin with, for example in a monotonous vigilance task or when a concurrent task is more appealing. Indeed, it was found that the positive effect of losses on performance in a given task was more pronounced in a task performed concurrently with another task which was primary in its importance.
Loss attention is consistent with several empirical findings in economics, finance, marketing, and decision making. Some of these effects have been previously attributed to loss aversion, but can be explained by a mere attention asymmetry between gains and losses. An example is the performance advantage attributed to golf rounds where a player is under par (or in a disadvantage) compared to other rounds where a player is at an advantage. Clearly, the difference could be attributed to increased attention in the former type of rounds. 2010s studies suggested that loss aversion mostly occur for very large losses, although the exact boundaries of the effect are unclear. On the other hand, loss attention was found even for small payoffs, such as $1. This suggests that loss attention may be more robust than loss aversion. Still, one might argue that loss aversion is more parsimonious than loss attention.

Increased expected value maximization with losses – It was found that individuals are more likely to select choice options with higher expected value (namely, mean outcome) in tasks where outcomes are framed as losses than when they are framed as gains. Yechiam and Hochman found that this effect occurred even when the alternative producing higher expected value was the one that included minor losses. Namely, a highly advantageous alternative producing minor losses was more attractive compared when it did not produce losses. Therefore, paradoxically, in their study minor losses led to more selection from the alternative generating them (refuting an explanation of this phenomenon based on loss aversion).
Loss arousal – Individuals were found to display greater Autonomic Nervous System activation following losses than following equivalent gains. For example, pupil diameter and heart rate were found to increase following both gains and losses, but the size of the increase was higher following losses. Importantly, this was found even for small losses and gains where individuals do not show loss aversion. Similarly, a positive effect of losses compared to equivalent gains was found on activation of midfrontal cortical networks 200 to 400 milliseconds after observing the outcome. This effect as well was found in the absence of loss aversion.
Increased hot stove effect for losses – The hot stove effect is the finding that individuals avoid a risky alternative when the available information is limited to the obtained payoffs. A relevant example (proposed by Mark Twain) is of a cat which jumped on a hot stove and will never do it again, even when the stove is cold and potentially contains food. In a finding that is consistent with the notion that losses increase attention, when a given option produces losses, this increases the hot stove effect.
Out of pocket phenomenon – In financial decision making, it has been shown that people are more motivated when their incentives are to avoid losing personal resources, as opposed to gaining equivalent resources. Traditionally, this strong behavioral tendency was explained by loss aversion; however, it could also be explained simply as increased attention.
Allure of minor disadvantages – In marketing studies, it has been demonstrated that products whose minor negative features are highlighted (in addition to positive features) are perceived as more attractive. Similarly, messages discussing both the advantages and disadvantages of a product were found to be more convincing than one-sided messages. Loss attention explains this as due to attentional competition between options, and increased attention following the highlighting of small negatives, which can increase the attractiveness of a product or a candidate either due to exposure or learning.

Two types of explanations have been proposed for loss aversion. First, loss aversion may arise because downside risks are more threatening to survival than upside opportunities. Humans are theorized to be hardwired for loss aversion due to asymmetric evolutionary pressure on losses and gains: "for an organism operating close to the edge of survival, the loss of a day's food could cause death, whereas the gain of an extra day's food would not cause an extra day of life (unless the food could be easily and effectively stored)". This explanation has been proposed by Kahneman himself: "Organisms that treat threats as more urgent than opportunities have a better chance to survive and reproduce." 
It has also been proposed that loss aversion may be a useful feature of cognition by keeping aspirations around the level of achievement within our reach. From this perspective, loss aversion prevents us from setting aspirations that are too high and unrealistic. If we set aspirations too high, loss aversion increases the subjective pain of failing to reach them. Loss aversion complements the existence of anticipatory utility, which encourages us not to set aspirations that are too low.

In 2005, experiments were conducted on the ability of capuchin monkeys to use money. After several months of training, the monkeys began showing behavior considered to reflect understanding of the concept of a medium of exchange. They exhibited the same propensity to avoid perceived losses demonstrated by human subjects and investors. Chen, Lakshminarayanan and Santos (2006) also conducted experiments on capuchin monkeys to determine whether behavioral biases extend across species. In one of their experiments, subjects were presented with two choices that both delivered an identical payoff of one apple piece in exchange of their coins. Experimenter 1 displayed one apple piece and gave that exact amount. Experimenter 2 displayed two apple pieces initially but always removed one piece before delivering the remaining apple piece to the subject. Therefore, identical payoffs are yielded regardless of which experimenter the subject traded with. It was found that subjects strongly preferred the experimenter who initially displayed only one apple piece, even though both experimenters yielded the same outcome of one apple piece. This study suggests that capuchins weighted losses more heavily than equivalent gains.

Expectation-based loss aversion is a phenomenon in behavioral economics. When the expectations of an individual fail to match reality, they lose an amount of utility from the lack of experiencing fulfillment of these expectations. Analytical framework by Botond Kőszegi and Matthew Rabin provides a methodology through which such behavior can be classified and even predicted.  An individual's most recent expectations influences loss aversion in outcomes outside the status quo; a shopper intending to buy a pair of shoes on sale experiences loss aversion when the pair they had intended to buy is no longer available.
Subsequent research performed by Johannes Abeler, Armin Falk, Lorenz Goette, and David Huffman in conjunction with the Institute of Labor Economics used the framework of Kőszegi and Rabin to prove that people experience expectation-based loss aversion at multiple thresholds. The study evinced that reference points of people causes a tendency to avoid expectations going unmet. Participants were asked to participate in an iterative money-making task given the possibilities that they would receive either an accumulated sum for each round of "work", or a predetermined amount of money. With a 50% chance of receiving the "fair" compensation, participants were more likely to quit the experiment as this amount approached the fixed payment. They chose to stop when the values were equal as no matter which random result they received, their expectations would be matched. Participants were reluctant to work for more than the fixed payment as there was an equal chance their expected compensation would not be met.

Loss aversion experimentation has most recently been applied within an educational setting in an effort to improve achievement within the U.S. In this latest experiment, Fryer et al. posits framing merit pay in terms of a loss in order to be most effective. This study was performed in the city of Chicago Heights within nine K-8 urban schools, which included 3,200 students. 150 out of 160 eligible teachers participated and were assigned to one of four treatment groups or a control group.  Teachers in the incentive groups received rewards based on their students' end of the year performance on the ThinkLink Predictive Assessment and K-2 students took the Iowa Test of Basic Skills (ITBS). The control group followed the traditional merit pay process of receiving "bonus pay" at the end of the year based on student performance on standardized exams. The experimental groups received a lump sum given at beginning of the year, that would have to be paid back. The bonus was equivalent to approximately 8% of the average teacher salary in Chicago Heights, approximately $8,000. According to the authors, 'this suggests that there may be significant potential for exploiting loss aversion in the pursuit of both optimal public policy and the pursuit of profits'. Thomas Amadio, superintendent of Chicago Heights Elementary School District 170, where the experiment was conducted, stated that "the study shows the value of merit pay as an encouragement for better teacher performance".

In earlier studies, both bidirectional mesolimbic responses of activation for gains and deactivation for losses (or vice versa) and gain or loss-specific responses have been seen. While reward anticipation is associated with ventral striatum activation, negative outcome anticipation engages the amygdala. Only some studies have shown involvement of amygdala during negative outcome anticipation but not others, which has led to some inconsistencies. It has later been proven that inconsistencies may only have been due to methodological issues including the utilisation of different tasks and stimuli, coupled with ranges of potential gains or losses sampled from either payoff matrices rather than parametric designs, and most of the data are reported in groups, therefore ignoring the variability amongst individuals. Rather than focusing on subjects in groups, later studies focus more on individual differences in the neural bases by jointly looking at behavioural analyses and neuroimaging  
Neuroimaging studies on loss aversion involves measuring brain activity with functional magnetic resonance imaging (fMRI) to investigate whether individual variability in loss aversion were reflected in differences in brain activity through bidirectional or gain or loss specific responses, as well as multivariate source-based morphometry (SBM) to investigate a structural network of loss aversion and univariate voxel-based morphometry (VBM) to identify specific functional regions within this network.
Brain activity in a right ventral striatum cluster increases particularly when anticipating gains. This involves the ventral caudate nucleus, pallidum, putamen, bilateral orbitofrontal cortex, superior frontal and middle gyri, posterior cingulate cortex, dorsal anterior cingulate cortex, and parts of the dorsomedial thalamus connecting to temporal and prefrontal cortex. There is a significant correlation between degree of loss aversion and strength of activity in both the frontomedial cortex and the ventral striatum. This is shown by the slope of brain activity deactivation for increasing losses being significantly greater than the slope of activation for increasing gains in the appetitive system involving the ventral striatum in the network of reward-based behavioural learning. On the other hand, when anticipating loss, the central and basal nuclei of amygdala, right posterior insula extending into the supramarginal gyrus mediate the output to other structures involved in the expression of fear and anxiety, such as the right parietal operculum and supramarginal gyrus. Consistent with gain anticipation, the slope of the activation for increasing losses was significantly greater than the slope of the deactivation for increasing gains.
Multiple neural mechanisms are recruited while making choices, showing functional and structural individual variability. Biased anticipation of negative outcomes leading to loss aversion involves specific somatosensory and limbic structures. fMRI test measuring neural responses in striatal, limbic and somatosensory brain regions help track individual differences in loss aversion. Its limbic component involved the amygdala (associated with negative emotion and plays a role in the expression of fear) and putamen in the right hemisphere. The somatosensory component included the middle cingulate cortex, as well as the posterior insula and rolandic operculum bilaterally. The latter cluster partially overlaps with the right hemispheric one displaying the loss-oriented bidirectional response previously described, but, unlike that region, it mostly involved the posterior insula bilaterally. All these structures play a critical role in detecting threats and prepare the organism for appropriate action, with the connections between amygdala nuclei and the striatum controlling the avoidance of aversive events. There are functional differences between the right and left amygdala. Overall, the role of amygdala in loss anticipation suggested that loss aversion may reflect a Pavlovian conditioned approach-avoidance response. Hence, there is a direct link between individual differences in the structural properties of this network and the actual consequences of its associated behavioral defense responses. The neural activity involved in the processing of aversive experience and stimuli is not just a result of a temporary fearful overreaction prompted by choice-related information, but rather a stable component of one's own preference function, reflecting a specific pattern of neural activity encoded in the functional and structural construction of a limbic-somatosensory neural system anticipating heightened aversive state of the brain. Even when no choice is required, individual differences in the intrinsic responsiveness of this interoceptive system reflect the impact of anticipated negative effects on evaluative processes, leading preference for avoiding losses rather than acquiring greater but riskier gains. 
Individual differences in loss aversion are related to variables such as age, gender, and genetic factors, all of which affect thalamic norepinephrine transmission, as well as neural structure and activities. Outcome anticipation and ensuing loss aversion involve multiple neural systems, showing functional and structural individual variability directly related to the actual outcomes of choices. In a study, adolescents and adults are found to be similarly loss-averse on behavioural level but they demonstrated different underlying neural responses to the process of rejecting gambles. Although adolescents rejected the same proportion of trials as adults, adolescents displayed greater caudate and frontal pole activation than adults to achieve this. These findings suggest a difference in neural development during the avoidance of risk. It is possible that adding affectively arousing factors (e.g. peer influences) may overwhelm the reward-sensitive regions of the adolescent decision making system leading to risk-seeking behaviour. On the other hand, although men and women did not differ on their behavioural task performance, men showed greater neural activation than women in various areas during the task. Loss of striatal dopamine neurons is associated with reduced risk-taking behaviour. Acute administration of D2 dopamine agonists may cause an increase in risky choices in humans. This suggests dopamine acting on stratum and possibly other mesolimbic structures can modulate loss aversion by reducing loss prediction signalling.

Ert, E.; Erev, I. (2008). "The rejection of attractive gambles, loss aversion, and the lemon avoidance heuristic". Journal of Economic Psychology. 29 (5): 715–723. doi:10.1016/j.joep.2007.06.003.
Erev, I.; Ert, E.; Yechiam, E. (2008). "Loss aversion, diminishing sensitivity, and the effect of experience on repeated decisions". Journal of Behavioral Decision Making. 21 (5): 575–597. doi:10.1002/bdm.602. S2CID 143592144.
Gal, D. (2006). "A psychological law of inertia and the illusion of loss aversion". Judgment and Decision Making. 1 (1): 23–32. doi:10.1017/S1930297500000322.
Harinck, F.; Van Dijk, E.; Van Beest, I.; Mersmann, P. (2007). "When gains loom larger than losses: Reversed loss aversion for small amounts of money". Psychological Science. 18 (12): 1099–1105. doi:10.1111/j.1467-9280.2007.02031.x. PMID 18031418. S2CID 26981722.
Hochman, G.; Yechiam, E. (2011). "Loss aversion in the eye and in the heart: The Autonomic Nervous System's responses to losses". Journal of Behavioral Decision Making. 24 (2): 140–156. doi:10.1002/bdm.692.
Kahneman, D.; Knetsch, J.; Thaler, R. (1990). "Experimental Test of the endowment effect and the Coase Theorem". Journal of Political Economy. 98 (6): 1325–1348. doi:10.1086/261737. JSTOR 2937761. S2CID 154889372.
Kahneman, D.; Tversky, A. (1979). "Prospect Theory: An Analysis of Decision under Risk". Econometrica. 47 (2): 263–291. CiteSeerX 10.1.1.407.1910. doi:10.2307/1914185. JSTOR 1914185.
Kermer, D. A.; Driver-Linn, E.; Wilson, T. D.; Gilbert, D. T. (2006). "Loss aversion is an affective forecasting error". Psychological Science. 17 (8): 649–653. CiteSeerX 10.1.1.551.456. doi:10.1111/j.1467-9280.2006.01760.x. PMID 16913944. S2CID 1331820.
McGraw, A. P.; Larsen, J. T.; Kahneman, D.; Schkade, D. (2010). "Comparing gains and losses". Psychological Science. 21 (10): 1438–1445. doi:10.1177/0956797610381504. PMID 20739673. S2CID 2290585.
Nicolau, J. L. (2012). "Battle Royal: Zero-price effect vs relative vs referent thinking". Marketing Letters. 23 (3): 661–669. doi:10.1007/s11002-012-9169-2. hdl:10919/120804. S2CID 143589145.
Silberberg, A.; et al. (2008). "On loss aversion in capuchin monkeys". Journal of the Experimental Analysis of Behavior. 89 (2): 145–155. doi:10.1901/jeab.2008.89-145. PMC 2251327. PMID 18422015.
Tversky, A.; Kahneman, D. (1991). "Loss Aversion in Riskless Choice: A Reference Dependent Model". Quarterly Journal of Economics. 106 (4): 1039–1061. CiteSeerX 10.1.1.703.2614. doi:10.2307/2937956. JSTOR 2937956.
Yechiam, E.; Hochman, G. (2013). "Losses as modulators of attention: Review and analysis of the unique effects of losses over gains". Psychological Bulletin. 139 (2): 497–518. doi:10.1037/a0029383. PMID 22823738. S2CID 10521233.
Yechiam, E.; Telpaz, A. (2013). "Losses Induce Consistency in Risk Taking Even Without Loss Aversion". Journal of Behavioral Decision Making. 26 (1): 31–40. doi:10.1002/bdm.758.
Canessa, Nicola; Crespi, Chiara; Baud-Bovy, Gabriel; Dodich, Alessandra; Falini, Andrea; Antonellis, Giulia; Cappa, Stefano F. (2017). "Neural markers of loss aversion in resting-state brain activity". NeuroImage. 146: 257–265. doi:10.1016/j.neuroimage.2016.11.050. PMID 27884798. S2CID 3396784.
Barkley-Levenson, Emily E.; Van Leijenhorst, Linda; Galván, Adriana (2013). "Behavioral and neural correlates of loss aversion and risk avoidance in adolescents and adults". Developmental Cognitive Neuroscience. 3: 72–83. doi:10.1016/j.dcn.2012.09.007. PMC 6987718. PMID 23245222.

The representativeness heuristic is used when making judgments about the probability of an event being representational in character and essence of a known prototypical event. It is one of a group of heuristics (simple rules governing judgment or decision-making) proposed by psychologists Amos Tversky and Daniel Kahneman in the early 1970s as "the degree to which [an event] (i) is similar in essential characteristics to its parent population, and (ii) reflects the salient features of the process by which it is generated".  
The representativeness heuristic works by comparing an event to a prototype or stereotype that we already have in mind. For example, if we see a person who is dressed in eccentric clothes and reading a poetry book, we might be more likely to think that they are a poet than an accountant. This is because the person's appearance and behavior are more representative of the stereotype of a poet than an accountant.
The representativeness heuristic can be a useful shortcut in some cases, but it can also lead to errors in judgment. For example, if we only see a small sample of people from a particular group, we might overestimate the degree to which they are representative of the entire group. 
Heuristics are described as "judgmental shortcuts that generally get us where we need to go – and quickly – but at the cost of occasionally sending us off course." Heuristics are useful because they use effort-reduction and simplification in decision-making.
When people rely on representativeness to make judgments, they are likely to judge wrongly because the fact that something is more representative does not actually make it more likely. The representativeness heuristic is simply described as assessing similarity of objects and organizing them based around the category prototype (e.g., like goes with like, and causes and effects should resemble each other).
This heuristic is used because it is an easy computation. The problem is that people overestimate its ability to accurately predict the likelihood of an event. Thus, it can result in neglect of relevant base rates and other cognitive biases.

The representativeness heuristic is more likely to be used when the judgement or decision to be made has certain factors.

When judging the representativeness of a new stimulus/event, people usually pay attention to the degree of similarity between the stimulus/event and a standard/process. It is also important that those features be salient. Nilsson, Juslin, and Olsson (2008) found this to be influenced by the exemplar account of memory (concrete examples of a category are stored in memory) so that new instances were classified as representative if highly similar to a category as well as if frequently encountered.
Several examples of similarity have been described in the representativeness heuristic literature. This research has focused on medical beliefs.  People often believe that medical symptoms should resemble their causes or treatments. For example, people have long believed that ulcers were caused by stress, due to the representativeness heuristic, when in fact bacteria cause ulcers. In a similar line of thinking, in some alternative medicine beliefs patients have been encouraged to eat organ meat that corresponds to their medical disorder. Use of the representativeness heuristic can be seen in even simpler beliefs, such as the belief that eating fatty foods makes one fat. Even physicians may be swayed by the representativeness heuristic when judging similarity, in diagnoses, for example. The researcher found that clinicians use the representativeness heuristic in making diagnoses by judging how similar patients are to the stereotypical or prototypical patient with that disorder.

Irregularity and local representativeness affect judgments of randomness. Things that do not appear to have any logical sequence are regarded as representative of randomness and thus more likely to occur. For example, THTHTH as a series of coin tosses would not be considered representative of randomly generated coin tosses as it is too well ordered.

In a study done in 1973, Kahneman and Tversky divided their participants into three groups:
"Base-rate group", who were given the instructions: "Consider all the first-year graduate students in the U.S. today. Please write down your best guesses about the percentage of students who are now enrolled in the following nine fields of specialization." The nine fields given were business administration, computer science, engineering, humanities and education, law, library science, medicine, physical and life sciences, and social science and social work.
"Similarity group", who were given a personality sketch. "Tom W. is of high intelligence, although lacking in true creativity. He has a need for order and clarity, and for neat and tidy systems in which every detail finds its appropriate place. His writing is rather dull and mechanical, occasionally enlivened by somewhat corny puns and by flashes of imagination of the sci-fi type. He has a strong drive for competence. He seems to feel little sympathy for other people and does not enjoy interacting with others. Self-centered, he nonetheless has a deep moral sense." The participants in this group were asked to rank the nine areas listed in part 1 in terms of how similar Tom W. is to the prototypical graduate student of each area.
"Prediction group", who were given the personality sketch described in 2, but were also given the information "The preceding personality sketch of Tom W. was written during Tom's senior year in high school by a psychologist, on the basis of projective tests. Tom W. is currently a graduate student. Please rank the following nine fields of graduate specialization in order of the likelihood that Tom W. is now a graduate student in each of these fields."
The judgments of likelihood were much closer for the judgments of similarity than for the estimated base rates. The findings supported the authors' predictions that people make predictions based on how representative something is (similar), rather than based on relative base rate information. For example, more than 95% of the participants said that Tom would be more likely to study computer science than education or humanities, when there were much higher base rate estimates for education and humanities than computer science.

In another study done by Tversky and Kahneman, subjects were given the following problem:
A cab was involved in a hit and run accident at night. Two cab companies, the Green and the Blue, operate in the city. 85% of the cabs in the city are Green and 15% are Blue.
A witness identified the cab as Blue. The court tested the reliability of the witness under the same circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colours 80% of the time and failed 20% of the time.
What is the probability that the cab involved in the accident was Blue rather than Green knowing that this witness identified it as Blue?
Most subjects gave probabilities over 50%, and some gave answers over 80%. The correct answer, found using Bayes' theorem, is lower than these estimates:
This results in a 41% probability (0.41 ≈ 0.12 ÷ 0.29) that the cab identified as blue was actually blue.
This result can be achieved by Bayes' theorem which states:
where:
P(x) - a probability of x, 
B - the cab was blue,
I - the cab is identified by the witness as blue,
Representativeness is cited in the similar effect of the gambler's fallacy, the regression fallacy and the conjunction fallacy.

The use of the representativeness heuristic will likely lead to violations of Bayes' Theorem:
However, judgments by representativeness only look at the resemblance between the hypothesis and the data, thus inverse probabilities are equated:
As can be seen, the base rate P(H) is ignored in this equation, leading to the base rate fallacy. A base rate is a phenomenon's basic rate of incidence. The base rate fallacy describes how people do not take the base rate of an event into account when solving probability problems. This was explicitly tested by Dawes, Mirels, Gold and Donahue (1993) who had people judge both the base rate of people who had a particular personality trait and the probability that a person who had a given personality trait had another one. For example, participants were asked how many people out of 100 answered true to the question "I am a conscientious person" and also, given that a person answered true to this question, how many would answer true to a different personality question. They found that participants equated inverse probabilities (e.g., 
) even when it was obvious that they were not the same (the two questions were answered immediately after each other). 
A medical example is described by Axelsson. Say a doctor performs a test that is 99% accurate, and the patient tests positive for the disease. However, the incidence of the disease is 1/10,000. The patient's actual risk of having the disease is 1%, because the population of healthy people is so much larger than the disease. This statistic often surprises people, due to the base rate fallacy, as many people do not take the basic incidence into account when judging probability. Research by Maya Bar-Hillel (1980) suggests that perceived relevancy of information is vital to base-rate neglect: base rates are only included in judgments if they seem equally relevant to the other information.
Some research has explored base rate neglect in children, as there was a lack of understanding about how these judgment heuristics develop. The authors of one such study wanted to understand the development of the heuristic, if it differs between social judgments and other judgments, and whether children use base rates when they are not using the representativeness heuristic. The authors found that the use of the representativeness heuristic as a strategy begins early on and is consistent. The authors also found that children use idiosyncratic strategies to make social judgments initially, and use base rates more as they get older, but the use of the representativeness heuristic in the social arena also increase as they get older. The authors found that, among the children surveyed, base rates were more readily used in judgments about objects than in social judgments. After that research was conducted, Davidson (1995) was interested in exploring how the representativeness heuristic and conjunction fallacy in children related to children's stereotyping. Consistent with previous research, children based their responses to problems off of base rates when the problems contained nonstereotypic information or when the children were older. There was also evidence that children commit the conjunction fallacy. Finally, as students get older, they used the representativeness heuristic on stereotyped problems, and so made judgments consistent with stereotypes. There is evidence that even children use the representativeness heuristic, commit the conjunction fallacy, and disregard base rates. 
Research suggests that use or neglect of base rates can be influenced by how the problem is presented, which reminds us that the representativeness heuristic is not a "general, all purpose heuristic", but may have many contributing factors. Base rates may be neglected more often when the information presented is not causal. Base rates are used less if there is relevant individuating information. Groups have been found to neglect base rate more than individuals do. Use of base rates differs based on context. Research on use of base rates has been inconsistent, with some authors suggesting a new model is necessary.

A group of undergraduates were provided with a description of Linda, modelled to be representative of an active feminist. Then participants were then asked to evaluate the probability of her being a feminist, the probability of her being a bank teller, or the probability of being both a bank teller and feminist. Probability theory dictates that the probability of being both a bank teller and feminist (the conjunction of two sets) must be less than or equal to the probability of being either a feminist or a bank teller. A conjunction cannot be more probable than one of its constituents. However, participants judged the conjunction (bank teller and feminist) as being more probable than being a bank teller alone. Some research suggests that the conjunction error may partially be due to subtle linguistic factors, such as inexplicit wording or semantic interpretation of "probability". The authors argue that both logic and language use may relate to the error, and it should be more fully investigated.

From probability theory the disjunction of two events is at least as likely as either of the events individually. For example, the probability of being either a physics or biology major is at least as likely as being a physics major, if not more likely. However, when a personality description (data) seems to be very representative of a physics major (e.g., having a pocket protector) over a biology major, people judge that it is more likely for this person to be a physics major than a natural sciences major (which is a superset of physics). 
Evidence that the representativeness heuristic may cause the disjunction fallacy comes from Bar-Hillel and Neter (1993). They found that people judge a person who is highly representative of being a statistics major (e.g., highly intelligent, does math competitions) as being more likely to be a statistics major than a social sciences major (superset of statistics), but they do not think that he is more likely to be a Hebrew language major than a humanities major (superset of Hebrew language). Thus, only when the person seems highly representative of a category is that category judged as more probable than its superordinate category. These incorrect appraisals remained even in the face of losing real money in bets on probabilities.

Representativeness heuristic is also employed when subjects estimate the probability of a specific parameter of a sample. If the parameter highly represents the population, the parameter is often given a high probability. This estimation process usually ignores the impact of the sample size. 
A concept proposed by Tversky and Kahneman provides an example of this bias in a problem about two hospitals of differing size.
Approximately 45 babies are born in the large hospital while 15 babies are born in the small hospital. Half (50%) of all babies born in general are boys. However, the percentage changes from 1 day to another. For a 1-year period, each hospital recorded the days on which >60% of the babies born were boys. The question posed is: Which hospital do you think recorded more such days?
The larger hospital (21)
The smaller hospital (21)
About the same (that is, within 5% of each other) (53)
The values shown in parentheses are the number of students choosing each answer.
The results show that more than half the respondents selected the wrong answer (third option). This is due to the respondents ignoring the effect of sample size. The respondents selected the third option most likely because the same statistic represents both the large and small hospitals. According to statistical theory, a small sample size allows the statistical parameter to deviate considerably compared to a large sample. Therefore, the large hospital would have a higher probability to stay close to the nominal value of 50%.

The bandwagon effect is a psychological phenomenon where people  adopt certain behaviors, styles, or attitudes simply because others are doing so. More specifically, it is a cognitive bias by which public opinion or behaviours can alter due to particular actions and beliefs rallying amongst the public. It is a psychological phenomenon whereby the rate of uptake of beliefs, ideas, fads and trends increases with respect to the proportion of others who have already done so. As more people come to believe in something, others also "hop on the bandwagon", regardless of the underlying evidence.
Following others' actions or beliefs can occur because of conformism or deriving information from others. Much of the influence of the bandwagon effect comes from the desire to 'fit in' with peers; making similar selections as other people is seen as a way to gain access to a particular social group. An example of this is fashion trends wherein the increasing popularity of a certain garment or style encourages more acceptance. When individuals make rational choices based on the information they receive from others, economists have proposed that information cascades can quickly form in which people ignore their personal information signals and follow the behaviour of others. Cascades explain why behaviour is fragile as people understand that their behaviour is based on a very limited amount of information. As a result, fads form easily but are also easily dislodged. The phenomenon is observed in various fields, such as economics, political science, medicine, and psychology. In social psychology, people's tendency to align their beliefs and behaviors with a group is known as 'herd mentality' or 'groupthink'. The reverse bandwagon effect (also known as the snob effect in certain contexts) is a cognitive bias that causes people to avoid doing something, because they believe that other people are doing it.

The phenomenon where ideas become adopted as a result of their popularity has been apparent for some time. However, the metaphorical use of the term bandwagon in reference to this phenomenon began in 1848. A literal "bandwagon" is a wagon that carries a musical ensemble, or band, during a parade, circus, or other entertainment event.
The phrase "jump on the bandwagon" first appeared in American politics in 1848 during the presidential campaign of Zachary Taylor. Dan Rice, a famous and popular circus clown of the time, invited Taylor to join his circus bandwagon. As Taylor gained more recognition and his campaign became more successful, people began saying that Taylor's political opponents ought to "jump on the bandwagon" themselves if they wanted to be associated with such success.
Later, during the time of William Jennings Bryan's 1900 presidential campaign, bandwagons had become standard in campaigns, and the phrase "jump on the bandwagon" was used as a derogatory term, implying that people were associating themselves with success without considering that with which they associated themselves.
Despite its emergence in the late 19th century, it was only rather recently that the theoretical background of bandwagon effects has been understood. One of the best-known experiments on the topic is the 1950s' Asch conformity experiment, which illustrates the individual variation in the bandwagon effect. Academic study of the bandwagon effect especially gained interest in the 1980s, as scholars studied the effect of public opinion polls on voter opinions.

Individuals are highly influenced by the pressure and norms exerted by groups. As an idea or belief increases in popularity, people are more likely to adopt it; when seemingly everyone is doing something, there is an incredible pressure to conform. Individuals' impressions of public opinion or preference can originate from several sources.
Some individual reasons behind the bandwagon effect include:
Efficiency — Bandwagoning serves as a mental shortcut, or heuristic, allowing for decisions to be made quickly. It takes time for an individual to evaluate a behaviour or thought and decide upon it.
Normative social influence (belonging) — People have the tendency to conform with others out of a desire to fit in with the crowd and gain approval from others. As conformity ensures some level of social inclusion and acceptance, many people go along with the behaviours and/or ideas of their group in order to avoid being the odd one out. The 'spiral of silence' exemplifies this factor.
Informational social influence — People tend to conform with others out of a desire to be right, under the assumption that others may know something or may understand the situation better. In other words, people will support popular beliefs because they are seen as correct by the larger social group (the 'majority'). Moreover, when it seems as though the majority is doing a certain thing, not doing that thing becomes increasingly difficult. When individuals make rational choices based on the information they receive from others, economists have proposed that information cascades can quickly form in which people decide to ignore their personal information signals and follow the behaviour of others.
Fear of missing out — People who are anxious about 'missing out' on things that others are doing may be susceptible to the bandwagon effect.
Being on the 'winning side' — The desire to support a "winner" (or avoid supporting a "loser") can be what makes some susceptible to the bandwagon effect, such as in the case of voting for a candidate because they're in the lead.
Another cause can come from distorted perceptions of mass opinion, known as 'false consensus' or 'pluralistic ignorance'. In politics, bandwagon effects can also come as result of indirect processes that are mediated by political actors. Perceptions of popular support may affect the choice of activists about which parties or candidates to support by donations or voluntary work in campaigns.

The bandwagon effect works through a self-reinforcing mechanism, and can spread quickly and on a large-scale through a positive feedback loop, whereby the more who are affected by it, the more likely other people are to be affected by it too.
A new concept that is originally promoted by only a single advocate or a minimal group of advocates can quickly grow and become widely popular, even when sufficient supporting evidence is lacking. What happens is that a new concept gains a small following, which grows until it reaches a critical mass, until for example it begins being covered by mainstream media, at which point a large-scale bandwagon effect begins, which causes more people to support this concept, in increasingly large numbers. This can be seen as a result of the availability cascade, a self-reinforcing process through which a certain belief gains increasing prominence in public discourse.

The bandwagon effect can take place in voting: it occurs on an individual scale where a voters opinion on vote preference can be altered due to the rising popularity of a candidate or a policy position. The aim for the change in preference is for the voter to end up picking the "winner's side" in the end. Voters are more so persuaded to do so in elections that are non-private or when the vote is highly publicised.
The bandwagon effect has been applied to situations involving majority opinion, such as political outcomes, where people alter their opinions to the majority view. Such a shift in opinion can occur because individuals draw inferences from the decisions of others, as in an informational cascade.
Perceptions of popular support may affect the choice of activists about which parties or candidates to support by donations or voluntary work in campaigns. They may strategically funnel these resources to contenders perceived as well supported and thus electorally viable, thereby enabling them to run more powerful, and thus more influential campaigns.

American economist Gary Becker has argued that the bandwagon effect is powerful enough to flip the demand curve to be upward sloping. A typical demand curve is downward sloping—as prices rise, demand falls. However, according to Becker, an upward sloping would imply that even as prices rise, the demand rises.

The bandwagon effect comes about in two ways in financial markets.
First, through price bubbles: these bubbles often happen in financial markets in which the price for a particularly popular security keeps on rising. This occurs when many investors line up to buy a security bidding up the price, which in return attracts more investors. The price can rise beyond a certain point, causing the security to be highly overvalued.
Second is liquidity holes: when unexpected news or events occur, market participants will typically stop trading activity until the situation becomes clear. This reduces the number of buyers and sellers in the market, causing liquidity to decrease significantly. The lack of liquidity leaves price discovery distorted and causes massive shifts in asset prices, which can lead to increased panic, which further increases uncertainty, and the cycle continues.

In microeconomics, bandwagon effects may play out in interactions of demand and preference. The bandwagon effect arises when people's preference for a commodity increases as the number of people buying it increases. Consumers may choose their product based on others' preferences believing that it is the superior product. This selection choice can be a result of directly observing the purchase choice of others or by observing the scarcity of a product compared to its competition as a result of the choice previous consumers have made. This scenario can also be seen in restaurants where the number of customers in a restaurant can persuade potential diners to eat there based on the perception that the food must be better than the competition due to its popularity. This interaction potentially disturbs the normal results of the theory of supply and demand, which assumes that consumers make buying decisions exclusively based on price and their own personal preference.

Decisions made by medical professionals can also be influenced by the bandwagon effect. Particularly, the widespread use and support of now-disproven medical procedures throughout history can be attributed to their popularity at the time. Layton F. Rikkers (2002), professor emeritus of surgery at the University of Wisconsin–Madison, calls these prevailing practices medical bandwagons, which he defines as "the overwhelming acceptance of unproved but popular [medical] ideas."
Medical bandwagons have led to inappropriate therapies for numerous patients, and have impeded the development of more appropriate treatment.
One paper from 1979 on the topic of bandwagons of medicine describes how a new medical concept or treatment can gain momentum and become mainstream, as a result of a large-scale bandwagon effect:
The news media finds out about a new treatment and publicizes it, often by publishing pieces.
Various organizations, such as government agencies, research foundations, and private companies also promote the new treatment, typically because they have some vested interest in seeing it succeed.
The public picks up on the now-publicized treatment, and pressures medical practitioners to adopt it, especially when that treatment is perceived as being novel.
Doctors often want to accept the new treatment, because it offers a compelling solution to a difficult issue.
Since doctors have to consume large amounts of medical information in order to stay aware of the latest trends in their field, it is sometimes difficult for them to read new material in a sufficiently critical manner.

One who supports a particular sports team, despite having shown no interest in that team until it started gaining success, can be considered a "bandwagon fan".

As an increasing number of people begin to use a specific social networking site or application, people are more likely to begin using those sites or applications. The bandwagon effect also affects random people that which posts are viewed and shared.
This research used bandwagon effects to examine the comparative impact of two separate bandwagon heuristic indicators (quantitative vs. qualitative) on changes in news readers' attitudes in an online comments section. Furthermore, Study 1 demonstrated that qualitative signals had a higher influence on news readers' judgments than quantitative clues. Additionally, Study 2 confirmed the results of Study 1 and showed that people's attitudes are influenced by apparent public opinion, offering concrete proof of the influence that digital bandwagons.

The bandwagon effect can also affect the way the masses dress and can be responsible for clothing trends. People tend to want to dress in a manner that suits the current trend and will be influenced by those who they see often – normally celebrities. Such publicised figures will normally act as the catalyst for the style of the current period. Once a small group of consumers attempt to emulate a particular celebrity's dress choice more people tend to copy the style due to the pressure or want to fit in and be liked by their peers.

Goidel, Robert K.; Shields, Todd G. (1994). "The Vanishing Marginals, the Bandwagon, and the Mass Media". The Journal of Politics. 56 (3): 802–810. doi:10.2307/2132194. JSTOR 2132194. S2CID 153664631.
McAllister, Ian; Studlar, Donley T. (1991). "Bandwagon, Underdog, or Projection? Opinion Polls and Electoral Choice in Britain, 1979-1987". The Journal of Politics. 53 (3): 720–740. doi:10.2307/2131577. JSTOR 2131577. S2CID 154257577.
Mehrabian, Albert (1998). "Effects of Poll Reports on Voter Preferences". Journal of Applied Social Psychology. 28 (23): 2119–2130. doi:10.1111/j.1559-1816.1998.tb01363.x.
Morwitz, Vicki G.; Pluzinski, Carol (1996). "Do Polls Reflect Opinions or Do Opinions Reflect Polls?". Journal of Consumer Research. 23 (1): 53–65. doi:10.1086/209466. JSTOR 2489665.

The overconfidence effect is a cognitive bias in which a person's subjective confidence in their judgments is reliably greater than the objective accuracy of those judgments, especially when confidence is relatively high. 
Overconfidence is one example of a miscalibration of subjective probabilities. Throughout the research literature, overconfidence has been defined in three distinct ways: (1) overestimation of one's actual performance; (2) overplacement of one's performance relative to others; and (3) overprecision in expressing unwarranted certainty in the accuracy of one's beliefs.
The most common way in which overconfidence has been studied is by asking people how confident they are of specific beliefs they hold or answers they provide.  The data show that confidence systematically exceeds accuracy, implying people are more sure that they are correct than they deserve to be. If human confidence had perfect calibration, judgments with 100% confidence would be correct 100% of the time, 90% confidence correct 90% of the time, and so on for the other levels of confidence. By contrast, the key finding is that confidence exceeds accuracy so long as the subject is answering hard questions about an unfamiliar topic. For example, in a spelling task, subjects were correct about 80% of the time, whereas they claimed to be 100% certain. Put another way, the error rate was 20% when subjects expected it to be 0%.  In a series where subjects made true-or-false responses to general knowledge statements, they were overconfident at all levels. When they were 100% certain of their answer to a question, they were wrong 20% of the time.

One manifestation of the overconfidence effect is the tendency to overestimate one's standing on a dimension of judgment or performance. This subsection of overconfidence focuses on the certainty one feels in their own ability, performance, level of control, or chance of success. This phenomenon is most likely to occur on hard tasks, hard items, when failure is likely or when the individual making the estimate is not especially skilled. Overestimation has been seen to occur across domains other than those pertaining to one's own performance. This includes the illusion of control, planning fallacy.

Illusion of control describes the tendency for people to behave as if they might have some control when in fact they have none. However, evidence does not support the notion that people systematically overestimate how much control they have; when they have a great deal of control, people tend to underestimate how much control they have.

The planning fallacy describes the tendency for people to overestimate their rate of work or to underestimate how long it will take them to get things done. It is strongest for long and complicated tasks, and disappears or reverses for simple tasks that are quick to complete.

Wishful-thinking effects, in which people overestimate the likelihood of an event because of its desirability, are relatively rare.
This may be in part because people engage in more defensive pessimism in advance of important outcomes, in an attempt to reduce the disappointment that follows overly optimistic predictions.

Overprecision is the excessive confidence that one knows the truth. For reviews, see Harvey or Hoffrage.
Much of the evidence for overprecision comes from studies in which participants are asked about their confidence that individual items are correct. This paradigm, while useful, cannot distinguish overestimation from overprecision; they are one and the same in these item-confidence judgments. After making a series of item-confidence judgments, if people try to estimate the number of items they got right, they do not tend to systematically overestimate their scores. The average of their item-confidence judgments exceeds the count of items they claim to have gotten right.
One possible explanation for this is that item-confidence judgments were inflated by overprecision, and that their judgments do not demonstrate systematic overestimation.

The strongest evidence of overprecision comes from studies in which participants are asked to indicate how precise their knowledge is by specifying a 90% confidence interval around estimates of specific quantities. If people were perfectly calibrated, their 90% confidence intervals would include the correct answer 90% of the time.
In fact, hit rates are often as low as 50%, suggesting people have drawn their confidence intervals too narrowly, implying that they think their knowledge is more accurate than it actually is.

Overplacement is the most prominent manifestation of the overconfidence effect which is a belief that erroneously rates someone as better than others. This subsection of overconfidence occurs when people believe themselves to be better than others, or "better-than-average". It is the act of placing yourself or rating yourself above others (superior to others). Overplacement more often occurs on simple tasks, ones we believe are easy to accomplish successfully.

Perhaps the most celebrated better-than-average finding is Svenson's finding that 93% of American drivers rate themselves as better than the median.
The frequency with which school systems claim their students outperform national averages has been dubbed the "Lake Wobegon" effect, after Garrison Keillor's apocryphal town in which "all the children are above average."  Overplacement has likewise been documented in a wide variety of other circumstances.
Kruger, however, showed that this effect is limited to "easy" tasks in which success is common or in which people feel competent. For difficult tasks, the effect reverses itself and people believe they are worse than others.

Some researchers have claimed that people think good things are more likely to happen to them than to others, whereas bad events were less likely to happen to them than to others.
But others have pointed out that prior work tended to examine good outcomes that happened to be common (such as owning one's own home) and bad outcomes that happened to be rare (such as being struck by lightning).
Event frequency accounts for a proportion of prior findings of comparative optimism.  People think common events (such as living past 70) are more likely to happen to them than to others, and rare events (such as living past 100) are less likely to happen to them than to others.

Taylor and Brown have argued that people cling to overly positive beliefs about themselves, illusions of control, and beliefs in false superiority, because it helps them cope and thrive.
Although there is some evidence that optimistic beliefs are correlated with better life outcomes, most of the research documenting such links is vulnerable to the alternative explanation that their forecasts are accurate.

People tend to overestimate what they personally know, unconsciously assuming they know facts they would actually need to access by asking someone else or consulting a written work. Asking people to explain how something works (like a bicycle, helicopter, or international policy) exposes knowledge gaps and reduces the overestimation of knowledge on that topic.

Social psychologist Scott Plous wrote, "No problem in judgment and decision making is more prevalent and more potentially catastrophic than overconfidence." It has been blamed for lawsuits, strikes, wars, poor corporate acquisitions, and stock market bubbles and crashes.
Strikes, lawsuits, and wars could arise from overplacement. If plaintiffs and defendants were prone to believe that they were more deserving, fair, and righteous than their legal opponents, that could help account for the persistence of inefficient enduring legal disputes.
If corporations and unions were prone to believe that they were stronger and more justified than the other side, that could contribute to their willingness to endure labor strikes.
If nations were prone to believe that their militaries were stronger than were those of other nations, that could explain their willingness to go to war.
Overprecision could have important implications for investing behavior and stock market trading.  Because Bayesians cannot agree to disagree, classical finance theory has trouble explaining why, if stock market traders are fully rational Bayesians, there is so much trading in the stock market.  Overprecision might be one answer. If market actors are too sure their estimates of an asset's value is correct, they will be too willing to trade with others who have different information than they do.
Oskamp tested groups of clinical psychologists and psychology students on a multiple-choice task in which they drew conclusions from a case study. Along with their answers, subjects gave a confidence rating in the form of a percentage likelihood of being correct. This allowed confidence to be compared against accuracy. As the subjects were given more information about the case study, their confidence increased from 33% to 53%. However their accuracy did not significantly improve, staying under 30%. Hence this experiment demonstrated overconfidence which increased as the subjects had more information to base their judgment on.
Even if there is no general tendency toward overconfidence, social dynamics and adverse selection could conceivably promote it.  For instance, those most likely to have the courage to start a new business are those who most overplace their abilities relative to those of other potential entrants.  And if voters find confident leaders more credible, then contenders for leadership learn that they should express more confidence than their opponents in order to win election. However, Overconfidence can be liability or asset during the political election. Candidates tend to lose advantage when verbally expressed overconfidence does not meet current performance, and tend to gain advantage express overconfidence non-verbally.
Overconfidence can be beneficial to individual self-esteem as well as giving an individual the will to succeed in their desired goal. Just believing in oneself may give one the will to take one's endeavours further than those who do not.

Kahneman and Klein further document how most experts can be beaten by simple heuristics developed by intelligent lay people.  Genuine expert intuition is acquired by learning from frequent, rapid, high-quality feedback about the quality of previous judgments. Few professionals have that.  Those who master a body of knowledge without learning from such expertise are called "respect experts" by Kahneman, Sibony, and Sunstein.  With some data, ordinary least squares (OLS) models often outperform simple heuristics.  With lots of data, artificial intelligence (AI) routinely outperforms OLS.

Very high levels of core self-evaluations, a stable personality trait composed of locus of control, neuroticism, self-efficacy, and self-esteem, may lead to the overconfidence effect. People who have high core self-evaluations will think positively of themselves and be confident in their own abilities, although extremely high levels of core self-evaluations may cause an individual to be more confident than is warranted.

The following is an incomplete list of events related or triggered by bias/overconfidence and a failing (safety) culture:
Chernobyl disaster
Sinking of the Titanic
Space Shuttle Challenger disaster
Space Shuttle Columbia disaster
Deepwater Horizon oil spill
Titan submersible implosion
Chicago flood
Hyatt Regency walkway collapse
Aberfan disaster

A status quo bias or default bias is a cognitive bias which results from a preference for the maintenance of one's existing state of affairs. The current baseline (or status quo) is taken as a reference point, and any change from that baseline is perceived as a loss or gain. Corresponding to different alternatives, this current baseline or default option is perceived and evaluated by individuals as a positive.
Status quo bias should be distinguished from a rational preference for the status quo, as for when the current state of affairs is more beneficial than the available alternatives, or when imperfect information is a significant problem. A large body of evidence, however, shows that status quo bias frequently affects human decision-making. Status quo bias should also be distinguished from psychological inertia, which refers to a lack of intervention in the current course of affairs.
The bias intersects with other non-rational cognitive processes such as loss aversion, in which losses comparative to gains are weighed to a greater extent. Further non-rational cognitive processes include existence bias, endowment effect, longevity, mere exposure, and regret avoidance.  Experimental evidence for the detection of status quo bias is seen through the use of the reversal test. A vast amount of experimental and field examples exist. Behaviour in regard to economics, retirement plans, health, and ethical choices show evidence of the status quo bias.

Status quo experiments have been conducted over many fields with Kahneman, Thaler, and Knetsch (1991) creating experiments on the endowment effect, loss aversion and status quo bias. 
Questionnaire: Samuelson and Zeckhauser (1988) demonstrated status quo bias using a questionnaire in which subjects faced a series of decision problems, which were alternately framed to be with and without a pre-existing status quo position. Subjects tended to remain with the status quo when such a position was offered to them. Results of the experiment further show that status quo bias advantage relatively increases with the number of alternatives given within the choice set. Furthermore, a weaker bias resulted from when the individual exhibited a strong discernible preference for a chosen alternative.
Hypothetical choice tasks:  Samuelson and Zeckhauser (1988) gave subjects a hypothetical choice task in the following "neutral" version, in which no status quo was defined: "You are a serious reader of the financial pages but until recently you have had few funds to invest. That is when you inherited a large sum of money from your great-uncle. You are considering different portfolios. Your choices are to invest in: a moderate-risk company, a high-risk company, treasury bills, municipal bonds." Other subjects were presented with the same problem but with one of the options designated as the status quo. In this case, the opening passage continued: "A significant portion of this portfolio is invested in a moderate risk company ... (The tax and broker commission consequences of any changes are insignificant.)" The result was that an alternative became much more popular when it was designated as the status quo.
Electric power consumers: California electric power consumers were asked about their preferences regarding trade-offs between service reliability and rates. The respondents fell into two groups, one with much more reliable service than the other. Each group was asked to state a preference among six combinations of reliability and rates, with one of the combinations designated as the status quo. A strong bias to the status quo was observed. Of those in the high-reliability group, 60.2 percent chose the status quo, whereas a mere 5.7 percent chose the low-reliability option that the other group had been experiencing, despite its lower rates. Similarly, of those in the low reliability group, 58.3 chose their low-reliability status quo, and only 5.8 chose the high-reliability option.
Automotive insurance consumers: The US states of New Jersey and Pennsylvania inadvertently ran a real-life experiment providing evidence of status quo bias in the early 1990s. As part of tort law reform programs, citizens were offered two options for their automotive insurance: an expensive option giving them full right to sue and a less expensive option with restricted rights to sue. In New Jersey the cheaper insurance was the default and in Pennsylvania the expensive insurance was the default. Johnson, Hershey, Meszaros and Kunreuther (1993) conducted a questionnaire to test whether consumers will stay with the default option for car insurance. They found that only 20% of New Jersey drivers changed from the default option and got the more expensive option. Also, only 25% of Pennsylvanian drivers changed from the default option and got the cheaper insurance. Therefore, framing and status quo bias can have significant financial consequences. 
General practitioners: Boonen, Donkers and Schut created two discrete choice experiments for Dutch residents to conclude a consumer's preference for general practitioners and whether they would leave their current practitioner. The Dutch health care system was chosen as general practitioners play the role of a gatekeeper. The experiment was conducted to investigate the effect of status quo bias on a consumer's decision to leave their current practitioner, with knowledge of other practitioners and their current relationship with their practitioner determining the role status quo bias plays.  
Through the questionnaire it was shown that respondents were aware of the lack of added benefit aligned with their current general practitioner and were aware of the quality differences between potential practitioners. 35% of respondents were willing to a pay a copayment to stay with their current general practitioner, while only 30% were willing to switch to another practitioner in exchange for a financial gain. These consumers were willing to pay a considerable amount to continue going to their current practitioner up to €17.32.  For general practitioners the value assigned by the consumer to staying with their current one exceeded the total value assigned to all other attributes tested such as discounts or a certificate of quality.
Within the discrete choice experiment the respondents were offered a choice between their current practitioner and a hypothetical provider with identical attributes. The respondents were 40% more likely to choose their current practitioner than if both options were hypothetical providers, which would result in the probability being 50% percent for both. It was found that status quo bias had a massive impact on which general practitioner the respondents would choose. Despite consumers being offered positive financial incentives, qualitative incentives or the addition of negative financial incentives respondents were still extremely hesitant to switch from their current practitioner. The impact of status quo bias was determined as making attempts to channel consumers away from the general practitioner they are currently seeing a daunting task.

Status quo bias has been attributed to a combination of loss aversion and the endowment effect, two ideas relevant to prospect theory. An individual weighs the potential losses of switching from the status quo more heavily than the potential gains; this is due to the prospect theory value function being steeper in the loss domain.  As a result, the individual will prefer not to switch at all. In other words, we tend to oppose change unless the benefits outweigh the risks. However, the status quo bias is maintained even in the absence of gain/loss framing: for example, when subjects were asked to choose the colour of their new car, they tended towards one colour arbitrarily framed as the status quo. Loss aversion, therefore, cannot wholly explain the status quo bias, with other potential causes including regret avoidance, transaction costs and psychological commitment.

A status quo bias can also be a rational route if there are cognitive or informational limitations.
Informational limitations
Cognitive limitations
Cognitive limitations of status quo bias involve the cognitive cost of choice, in which decisions are more susceptible to postponement as increased alternatives are added to the choice set. Moreover, mental effort needed to maintain status quo alternatives would often be lesser and easier, resulting in a superior choice's benefit being outweighed by decision-making cognitive costs. Consequently, maintenance of current or previous state of affairs would be regarded as the easier alternative.

The irrational maintenance of the status quo bias links and confounds many cognitive biases.
Existence bias
An assumption of longevity and goodness are part of the status quo bias.  People treat existence as a prima facie case for goodness, aesthetic and longevity increases this preference.
The status quo bias affects people's preferences; people report preferences for what they are likely rather than unlikely to receive. People simply assume, with little reason or deliberation, the goodness of existing states.
Longevity is a corollary of the existence bias: if existence is good, longer existence should be better. This thinking resembles quasi-evolutionary notions of "survival of the fittest", and also the augmentation principle in attribution theory.
Psychological inertia is another reason used to explain a bias towards the status quo. Another explanation is fear of regret in making a wrong decision, i.e.  If we choose a partner, when we think there could be someone better out there.

Mere exposure is an explanation for the status quo bias.  Existing states are encountered more frequently than non-existent states and because of this they will be perceived as more true and evaluated more preferably. One way to increase liking for something is repeated exposure over time.
Loss aversion
Loss aversion also leads to greater regret for action than for inaction; more regret is experienced when a decision changes the status quo than when it maintains it. Together these forces provide an advantage for the status quo; people are motivated to do nothing or to maintain current or previous decisions. Change is avoided, and decision makers stick with what has been done in the past.
Changes from the status quo will typically involve both gains and losses, with the change having good overall consequences if the gains outweigh these losses. A tendency to overemphasize the avoidance of losses will thus favour retaining the status quo, resulting in a status quo bias. Even though choosing the status quo may entail forfeiting certain positive consequences, when these are represented as forfeited "gains" they are psychologically given less weight than the "losses" that would be incurred if the status quo were changed.
The loss aversion explanation for the status quo bias has been challenged by David Gal and Derek Rucker who argue that evidence for loss aversion (i.e., a tendency to avoid losses more than to pursue gains) is confounded with a tendency towards inertia (a tendency to avoid intervention more than to intervene in the course of affairs). Inertia, in this sense, is related to omission bias, except it need not be a bias but might be perfectly rational behavior stemming from transaction costs or lack of incentive to intervene due to fuzzy preferences.
Omission bias
Omission bias may account for some of the findings previously ascribed to status quo bias. Omission bias is diagnosed when a decision maker prefers a harmful outcome that results from an omission to a less harmful outcome that results from an action.
Overall implications of a study conducted by Ilana Ritov and Jonathan Baron, regarding status quo and omission biases, reveal that omission bias may further be diagnosed when the decision maker is unwilling to take preference from any of the available options given to them, thus enabling reduction of the number of decisions where utility comparison and weight is unavoidable.

The reversal test: when a proposal to change a certain parameter is thought to have bad overall consequences, consider a change to the same parameter in the opposite direction. If this is also thought to have bad overall consequences, then the onus is on those who reach these conclusions to explain why our position cannot be improved through changes to this parameter. If they are unable to do so, then we have reason to suspect that they suffer from status quo bias. The rationale of the reversal test is: if a continuous parameter admits of a wide range of possible values, only a tiny subset of which can be local optima, then it is prima facie implausible that the actual value of that parameter should just happen to be at one of these rare local optima.

A study found that erroneous status quo rejections have a greater neural impact than erroneous status quo acceptances.  This asymmetry in the genesis of regret might drive the status quo bias on subsequent decisions.
A study was done using a visual detection task in which subjects tended to favour the default when making difficult, but not easy, decisions. This bias was suboptimal in that more errors were made when the default was accepted. A selective increase in sub-thalamic nucleus (STN) activity was found when the status quo was rejected in the face of heightened decision difficulty. Analysis of effective connectivity showed that inferior frontal cortex, a region more active for difficult decisions, exerted an enhanced modulatory influence on the STN during switches away from the status quo.
Research by University College London scientists that examines the neural pathways involved in 'status quo bias' in the human brain and found that the more difficult the decision we face, the more likely we are not to act.
The study, published in Proceedings of the National Academy of Sciences (PNAS), looked at the decision-making of participants taking part in a tennis 'line judgement' game while their brains were scanned using functional MRI (fMRI).
The 16 study participants were asked to look at a cross between two tramlines on a screen while holding down a 'default' key. They then saw a ball land in the court and had to make a decision as to whether it was in or out. On each trial, the computer signalled which was the current default option – 'in' or 'out'. The participants continued to hold down the key to accept the default and had to release it and change to another key to reject the default. The results showed a consistent bias towards the default, which led to errors. As the task became more difficult, the bias became even more pronounced. The fMRI scans showed that a region of the brain known as the sub-thalamic nucleus (STN) was more active in the cases when the default was rejected. Also, greater flow of information was seen from a separate region sensitive to difficulty (the prefrontal cortex) to the STN. This indicates that the STN plays a key role in overcoming status quo bias when the decision is difficult.

Against this background, two behavioral economists devised an opt-out plan to help employees of a particular company build their retirement savings. In an opt-out plan, the employees are automatically enrolled unless they explicitly ask to be excluded. They found evidence for status quo bias and other associated effects. The impact of defaults on decision making due to status quo bias is not purely due to subconscious bias, as it has been found that even when disclosing the intent of the default to consumers, the effect of the default is not reduced.
An experiment conducted by Sen Geng, regarding status quo bias and decision time allocation, reveal that individuals allocate more attention to default options in comparison to alternatives. This is due to individuals who are mainly risk-averse who seek to attain greater expected utility and decreased subjective uncertainty in making their decision. Furthermore, by optimally allocating more time and asymmetric attention to default options or positions, the individual's estimate of the default's value is consequently more precise than estimates of alternatives. This behaviour thus reflects the individual's asymmetric choice error, and is therefore an indication of status quo bias.

Status-quo educational bias can be both a barrier to political progress and a threat to the state's legitimacy and argue that the values of stability, compliance, and patriotism underpin important reasons for status quo bias that appeal not to the substantive merits of existing institutions but merely to the fact that those institutions are the status quo.

The status quo bias is seen in important real life decisions; it has been found to be prominent in data on selections of health care plans and retirement programs.

There is a belief that preference for the status quo represents a core component of conservative ideology in societies where government power is limited and laws restricting actions of individuals exist. Conversely, in liberal societies, movements to impose restrictions on individuals or governments are met with widespread opposition by those that favor the status quo. Regardless of the type of society, the bias tends to hinder progressive movements in the absence of a reaction or backlash against the powers that be.

Status quo bias may be responsible for much of the opposition to human enhancement in general and to genetic cognitive enhancement in particular. Some ethicists argue, however, that status quo bias may not be irrational in such cases. The rationality of status quo bias is also an important question in the ethics of disability.

Education can (sometimes unintentionally) encourage children's belief in the substantive merits of a particular existing law or political institution, where the effect does not derive from an improvement in their ability or critical thinking about that law or institution. However, this biasing effect is not automatically illegitimate or counterproductive: a balance between social inculcation and openness needs to be maintained.
Given that educational curriculums are developed by Governments and delivered by individuals with their own political thoughts and feelings, the content delivered may be inadvertently affected by bias. When Governments implement certain policies, they become the status quo and are then presented as such to children in the education system. Whether through intentional or unintentional means, when learning about a topic, educators may favour the status quo. They may simply not know the full extent of the arguments against the status quo or may not be able to present an unbiased account of each side because of their personal biases.

An experiment to determine if a status-quo bias, toward current medication even when better alternatives are offered—, exists in a stated-choice study among asthma patients who take prescription combination maintenance medications.
The results of this study indicate that the status quo bias may exist in stated-choice studies, especially with medications that patients must take daily such as asthma maintenance medications. Stated-choice practitioners should include a current medication in choice surveys to control for this bias.

A study in 1986 examined the effect of status quo bias on those planning their retirement savings when given the yearly choice between two investment funds. Participants were able to choose how to proportionally split their retirement savings between the two funds at the beginning of each year. After each year, they were able to amend their chose split without switching costs as their preferences changed. Even though the two funds had vastly different returns in both absolute and relative terms, the majority of participants never switched the preferences across the trial period. Status quo bias was also more evident in older participants as they preferred to stay with their original investment, rather than switching as new information came to light.

Korobkin's has studied a link between negotiation and status quo bias in 1998. In this studies shows that in negotiating contracts favor inaction that exist in situations in which a legal standard and defaults from contracts will administer absent action. This involves a biased opinion opposed to alternative solutions. Heifetz's and Segev's study in 2004 found support for existence of a toughness bias. It is like so-called endowment effect which affects seller's behavior.

Status quo bias provides a maintenance role in the theory-practice gap in price management, and is revealed in Dominic Bergers' research regarding status quo bias and its individual differences from a price management perspective. He identified status quo bias as a possible influencer of 22 rationality deficits identified and explained by Rullkötter (2009), and is further attributed to deficits within Simon and Fassnacht's (2016) price management process phases. Status quo bias remained as an underlying possible cause of 16 of the 22 rationality deficits. Examples of these can be seen within the analysis phase and implementation phase of price management processes.
Bergers reveal that status quo bias within the former price management process phase potentially led to complete reliance on external information sources that existed traditionally. This bias, through a price management perspective, can be demonstrated when monitoring competitor's pricing. In the latter phase, status quo bias potentially led to the final price being determined by decentralised staff, which is potentially perpetuated by existing system profitability within price management practices.

An empirical study conducted by Alexandre Kempf and Stefan Ruenzi examined the presence of status quo bias within the U.S. equity mutual fund market, and the extent in which this depends on the number of alternatives given. Using real data obtained from the U.S. mutual fund market, this study reveals status quo bias influences fund investors, in which a stronger correlation for positive dependence of status quo bias was found when the number of alternatives was larger, and therefore confirms Samuelson and Zeckhauser (1988) experimental results.

Status quo bias has a significant impact on economics research and policy creation. Anchoring and adjustment theory in economics is where people's decision-making and outcome are affected by their initial reference point. The reference point for a consumer is usually the status quo. Status quo bias results in the default option to be better understood by consumers compared to alternatives options. This results in the status quo option providing less uncertainty and higher expected utility for risk-averse decision makers. Status quo bias is compounded by loss aversion theory where consumers see disadvantages as larger than advantages when making decision away from the reference point. Economics can also describe the effect of loss aversion graphically with a consumer's utility function for losses having a negative and 2 times steeper curve than the utility function for gains. Therefore, they perceive the negative effect of a loss as more significant and will stay with status quo.  Consumers choosing the status quo goes against rational consumer choice theory as they are not maximising their utility. Rational consumer choice theory underpins many economic decisions by defining a set of rules for consumer behaviour. Therefore, status quo bias has substantial implications in economic theory.

The halo effect (sometimes called the halo error)—a term coined by Edward Thorndike—is the tendency for positive impressions of a person, company, country, brand, or product in one area to positively influence one's opinion or feelings of a person, company, country, brand, or product in another area.  It is "the name given to the phenomenon whereby evaluators tend to be influenced by their previous judgments of performance or personality;" in other words, a cognitive bias that can prevent people from forming an image based on the sum of all objective circumstances at hand.  
A simplified example of the halo effect could be when people, after noticing that an individual in a photograph is attractive, well groomed, and properly attired, then assumes—using a mental heuristic  based on the rules of their own social concept—that the person in the photograph is a good person. This constant error in judgment is reflective of the evaluators' preferences, prejudices, ideology, aspirations, and social perception.

The term halo effect is used in psychology to describe a perception distortion that affects the way people interpret the information about others with whom they have formed a positive gestalt. For example, they find out that someone with whom they have formed a positive gestalt has cheated on his taxes; but because of the positive gestalt, they may dismiss the significance of this behavior or even think the person simply made a mistake. The halo effect refers to the tendency to evaluate an individual positively on many traits because of a shared belief.
It is a type of immediate judgment discrepancy, or cognitive bias, in which a person making an initial assessment of another person, place, or thing will assume ambiguous information based on concrete information.  The halo effect is an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information.
The halo effect can also be explained as the behavior (usually unconscious) of using evaluations based on unrelated criteria to make judgments about something or someone. The halo effect is sometimes used to refer specifically to when this behavior has a positive correlation, such as viewing someone who is attractive as likely to be successful and popular. When this judgment has a negative connotation, however, such as when someone unattractive is more readily blamed for a crime than someone attractive, it is sometimes referred to as the horn effect.

The term halo effect is used in marketing to explain consumer bias toward certain products because of favorable experience with other products made by the same company. It is used in the part of brand marketing called "line extensions." One common halo effect is when the perceived positive features of a particular item extend to a broader brand. A notable example is the manner in which the popularity of Apple's iPod generated enthusiasm for the corporation's other products. Advertising often makes use of television shows, movies and those who star in them, to promote products via the halo effect.
In the automotive industry, exotic, limited-production luxury models or low-volume sports cars made by a manufacturer's racing, motorsports, or in-house modification teams, are sometimes referred to as "halo cars" for the effect they are intended to produce on selling other vehicles within the make. To contrast this with the automotive terminology "flagship model," see Flagship car.
In the wine industry, certain wine features create a halo effect that can influence the customer's opinion of a given wine. The inclusion of the category "organic" on the label of a wine can increase the consumer's positive valuation of the wine. Organic wines are conceived of as being healthy, having a better taste, scent, and color, and resulting in a higher degree of overall satisfaction. Another example of the halo effect in the wine industry is the association of traditional corks with wine quality: corked bottles are systematically rated as of higher quality than bottles that use screw caps and plastic caps since the latter are viewed as signifiers of low-quality wines.
Advertising in one channel has been shown to have a halo effect on advertising in another channel.
A halo effect with regard to health, dubbed a "health halo," is used in food marketing to increase sales of a product; it can result in increased consumption of the product in the halo, which may be unhealthy.
The term "halo effect" has also been applied to human rights organizations that have used their status to move away from their stated goals. Political scientist Gerald Steinberg has claimed that non-governmental organizations (NGOs) take advantage of the halo effect and are "given the status of impartial moral watchdogs" by governments and the news media.
The Ronald McDonald House, a widely known NGO, openly celebrates the positive outcomes it receives from the halo effect. The web page for the Ronald McDonald House in Durham, North Carolina, states that 95% of survey participants were aware of Ronald McDonald House Charities. This awareness is attributed to the halo effect, as employees, customers, and stakeholders are more likely to be involved in a charity that they recognize and trust, with a name and logo that are familiar.
A brand's halo effect can protect its reputation in the event of a crisis. An event that is detrimental to a brand that is viewed favorably would not be as threatening or damaging to a brand that consumers view unfavorably.

Non-psychology/business use of the term "halo effect" describes the monetary value of the spillover effect when an organization's marketing budget is subsequently reduced. This was first demonstrated to students via the 1966 version of a textbook and a software package named "The Marketing Game."
The halo effect can also be used in the case of institutions, as one's favorable perceptions regarding an aspect of an organization could determine a positive view of its entire operations. For example, if a hospital is known for its excellent open heart and cardiac program, then the community would expect it to excel in other areas as well. This can also be demonstrated in the positive perceptions of financial institutions that gained favorable coverage in the media due to meteoric growth but eventually failed afterward.
The term "halo effect" is also used in metal detecting  to denote the enhanced ability of a metal item or coin to be detectable when it has been left undisturbed for some period of time in wet soil. The object can leach some metallic properties into the soil, making it more detectable. The area surrounding the object is called its "halo."

The halo effect was originally identified in 1907 by the American psychologist Frederick L. Wells (1884–1964). However, it was only officially recognized in 1920 with empirical evidence provided by the psychologist Edward Thorndike (1874–1949). Edward Thorndike was the first to say the halo effect is a specific cognitive bias in which one aspect of the person, brand, product, or institution affects one's thoughts or judgment of the entity's other aspects or dimensions. Thorndike, an early behaviorist, was an important contributor to the study of the psychology of learning. He gave the phenomenon its name in his 1920 article "A Constant Error in Psychological Ratings." In "Constant Error," Thorndike set out to replicate the study in hopes of pinning down the bias that he thought was present in these ratings. Subsequent researchers have studied it in relation to attractiveness and its bearing on the judicial and educational systems. Thorndike originally coined the term referring only to people; however, its use has been greatly expanded, especially in the area of brand marketing.

In Thorndike's words, "Ratings were apparently affected by a marked tendency to think of the person in general as rather good or rather inferior and to color the judgments of the qualities by this general feeling." In "A Constant Error in Psychological Ratings," Thorndike asked two commanding officers to evaluate their soldiers in terms of physical qualities (neatness, voice, physique, bearing, and energy), intellect, leadership skills, and personal qualities (including dependability, loyalty, responsibility, selflessness, and cooperation).  In Thorndike's study, attractiveness plays an important role in how people tend to consider a person, such as whether a person is friendly or not based on their physical appearance. His goal was to see how the ratings of one characteristic affected other characteristics.
Thorndike's study showed how there was too great a correlation in the commanding officers' responses. In his review, he stated, "The correlations are too high and too even. For example, for the three raters next studied[,] the average correlation for physique with intelligence is .31; for physique with leadership, .39; and for physique with character, .28."  The ratings of one of the special qualities of an officer often started a trend in the rating results. The halo effect is not an indication of the existence of a correlation, but instead indicates that the correlation is too high. Thorndike used the halo effect to describe both a positive and negative halo.
In 2023, a large study of 2748 participants found that the same individuals received significantly higher ratings of intelligence, trustworthiness, sociability and happiness after having applied a beauty filter. It found a correlation of .30 for intelligence, .20 for trustworthiness, .39 for sociability and .39 for happiness. However, the study also found that beautified men received significantly higher scores to their perceived intelligence compared to women.

Cognitive bias is a pattern in perception, interpretation, or judgment that consistently leads to an individual misunderstanding something about themselves or their social environment, leading to poor decision-making or irrational behavior. The halo effect is classified as a cognitive bias because the halo effect is a perception error that distorts the way a person sees someone, and cognitive bias is a perception error that distorts the way that people see themselves.
The term "halo" is used in analogy with the religious concept: a glowing circle crowning the heads of saints in countless medieval and Renaissance paintings, bathing the saint's face in heavenly light. The observer may be subject to overestimating the worth of the observed by the presence of a quality that adds light on the whole, like a halo. In other words, observers tend to bend their judgement according to one patent characteristic of the person (the "halo") or a few of his traits, generalizing toward a judgement of that person's character (e.g., in the literal hagiologic case, "entirely good and worthy").
The effect works in both positive and negative directions (and is hence sometimes called the horns and halo effect). If the observer likes one aspect of something, they will have a positive predisposition toward everything about it. If the observer dislikes one aspect of something, they will have a negative predisposition toward everything about it.

A person's attractiveness has also been found to produce a halo effect. Attractiveness contributes to the halo effect because it can be influenced by several specific traits. These perceptions of attractiveness may affect judgments tied to personality traits. Physical attributes contribute to perceptions of attractiveness (e.g., physique, hair, eye color). For example, someone who is perceived as attractive, due in part to physical traits, may be more likely to be perceived as kind or intelligent. The role of attractiveness in producing the halo effect has been illustrated through a number of studies. Recent research, for example, has revealed that attractiveness may affect perceptions tied to life success and personality. In this study, attractiveness was correlated with weight, indicating that attractiveness itself may be influenced by various specific traits. Trustworthiness and friendliness were included in the personality variables. People perceived as being more attractive were more likely to be perceived as trustworthy and friendly.  What this suggests is that perceptions of attractiveness may influence a variety of other traits, which supports the concept of the halo effect.

People's first impressions of others influence their later decision to either approach or avoid those individuals. When people first encounter someone, the information present about that individual is limited; therefore, people will use the information available to assume other characteristics about that person; for instance, observable behaviors such as eye contact, leaning forward, smiling and positive hand gestures (ex. steepling hands) are linked to positive emotions, while avoiding eye contact, leaning back, avoiding touch, and defensive hand gestures (ex. hands in pockets) or no gestures at all are linked to feelings of detachment. Besides that, another popular example used when referring to the halo effect is the phenomenon called the attractiveness stereotype  or when encountering individuals who are similar to others in some aspects, like personality or life history like the school they attended. People tend to assume that physically attractive individuals are more likely to be more healthy, successful, courteous, containing higher moral standards, and greater social competence than other people; on the other hand, the attractiveness stereotype can also carry a negative connotation as some people may think of attractive people as less honest and more conceited than others.

Officeholders who create what The New York Times called "a living legacy" benefit from a halo effect when their overall accomplishments are subsequently evaluated.

Study results showing the influence of the halo effect in the judicial context exist:
Efran (1974) found subjects were more lenient when sentencing attractive individuals than unattractive ones, even though exactly the same crime was committed. The researchers attributed the result to a societal perception that people with a high level of attractiveness are seen as more likely to have successful futures due to corresponding socially desirable traits.
Monahan (1941) studied social workers who were accustomed to interacting with a diverse range of people and found that the majority experienced difficulty when asked to consider that a beautiful person was guilty of a crime.
A study presented two hypothetical crimes: a burglary and a swindle. The burglary involved a woman illegally obtaining a key and stealing $2,200 (equivalent to $13,000 today); the swindle involved a woman manipulating a man to invest $2,200 in a nonexistent corporation.  The results showed that when the offense was not related to attractiveness (as in the burglary) the unattractive defendant was punished more severely than the attractive one. However, when the offense was related to attractiveness (the swindle), the attractive defendant was punished more severely than the unattractive one.  The study imputes that the usual leniency given to the attractive woman (as a result of the halo effect) was negated or reversed when the nature of the crime involved her looks.

Rating error effect, mistakes made by raters when they use a rating scale, reflect the task competence of the rater, as well as the rater's sex, social position, race, religion, and age. Researchers showed that halo effect is one component of this error. Fisicaro and Lance introduced three explanatory models. The first model named the general impression model states that global evaluation affects the rating of individual characteristics. The salient dimension model states that how people perceive an individual characteristic affects their evaluation of other characteristics. The inadequate discrimination model refers to the rater's failure to identify different behaviors of the person being evaluated.

The reverse halo effect occurs when positive evaluations of an individual cause negative consequences. Rater errors pose special problems for the issues of reliability and validity. Furthermore, ratings that differ in time may accurately reflect a change in behavior even though this difference would demonstrate an artificial lack of reliability. A follow-up study with both men and women participants supported this, as well as showing that attractive women were expected to be conceited and have a higher socioeconomic status. Eagly et al. (1991) also commented on this phenomenon, showing that more attractive individuals of both sexes were expected to be higher in vanity and possibly egotistic. Applied instances of the reverse halo effect include negative evaluations of criminals who use their attractiveness to their advantage and rating a philosophical essay lower when written by a young female than an old male.

A negative form of the halo effect, called the horn effect, the devil effect, or the reverse halo effect, allows a disliked trait or aspect of a person or product to negatively influence overall perception. Psychologists call it a "bias blind spot:" "Individuals believe (that negative) traits are inter-connected." due to a negative first impression.  The Guardian wrote of the devil effect in relation to Hugo Chavez: "Some leaders can become so demonized that it's impossible to assess their achievements and failures in a balanced way." For those seen in a negative light, anything they do that is negative is exemplified, while the positive things they do are not seen, or are doubted.

Abikoff et al. (1993) found the halo effect is also present in the classroom. In this study, both regular and special education elementary school teachers watched videotapes of what they believed to be children in regular 4th-grade classrooms. In reality, the children were actors, depicting behaviors present in attention deficit hyperactivity disorder (ADHD), oppositional defiant disorder (ODD), or standard behavior. The teachers were asked to rate the frequency of hyperactive behaviors observed in the children. Teachers rated hyperactive behaviors accurately for children with ADHD; however, the ratings of hyperactivity were much higher for the children with ODD-like behaviors, showing a horn effect for children who appeared to have ODD.

"In the classroom, teachers are subject to the halo effect rating error when evaluating their students. For example, a teacher who sees a well-behaved student might tend to assume this student is also bright, diligent, and engaged before that teacher has objectively evaluated the student's capacity in these areas. When these types of halo effects occur, they can affect students' approval ratings in certain areas of functioning and can even affect students' grades."
"In the work setting, the halo effect is most likely to show up in a supervisor's appraisal of a subordinate's job performance. In fact, the halo effect is probably the most common bias in performance appraisal. Think about what happens when a supervisor evaluates the performance of a subordinate. The supervisor may give prominence to a single characteristic of the employee, such as enthusiasm, and allow the entire evaluation to be colored by how he or she judges the employee on that one characteristic. Even though the employee may lack the requisite knowledge or ability to perform the job successfully, if the employee's work shows enthusiasm, the supervisor may very well give him or her a higher performance rating than is justified by knowledge or ability."

A study by Forgas (2011) states that one's mood can affect the degree of the halo effect's influence. When someone is in a favorable mood, the halo effect is more likely to be influential—this was demonstrated by study participants choosing between pictures of an elderly man with a beard and a young woman, and deciding which subject possessed more philosophical attributes. Additionally, when asked to list the happy, neutral, or negative times in their life, the halo effect was more evident in the perceptions of the participants who chose to write about happy prior experiences. Forgas's study suggests that when one is gauging the extent of the halo effect in a situation, one must consider the emotional state of the person making the judgment.
A 2013 report on "the link between disease and leader preferences" claimed that "congressional districts with a higher incidence of disease" were more likely to show a halo effect "on electoral outcomes."

The planning fallacy is a phenomenon in which predictions about how much time will be needed to complete a future task display an optimism bias and underestimate the time needed. This phenomenon sometimes occurs regardless of the individual's knowledge that past tasks of a similar nature have taken longer to complete than generally planned. The bias affects predictions only about one's own tasks. On the other hand, when outside observers predict task completion times, they tend to exhibit a pessimistic bias, overestimating the time needed. The planning fallacy involves estimates of task completion times more optimistic than those encountered in similar projects in the past.
The planning fallacy was first proposed by Daniel Kahneman and Amos Tversky in 1979. In 2003, Lovallo and Kahneman proposed an expanded definition as the tendency to underestimate the time, costs, and risks of future actions and at the same time overestimate the benefits of the same actions. According to this definition, the planning fallacy results in not only time overruns, but also cost overruns and benefit shortfalls.

In a 1994 study, 37 psychology students were asked to estimate how long it would take to finish their senior theses. The average estimate was 33.9 days. They also estimated how long it would take "if everything went as well as it possibly could" (averaging 27.4 days) and "if everything went as poorly as it possibly could" (averaging 48.6 days). The average actual completion time was 55.5 days, with about 30% of the students completing their thesis in the amount of time they predicted.
Another study asked students to estimate when they would complete their personal academic projects. Specifically, the researchers asked for estimated times by which the students thought it was 50%, 75%, and 99% probable their personal projects would be done.
13% of subjects finished their project by the time they had assigned a 50% probability level;
19% finished by the time assigned a 75% probability level;
45% finished by the time of their 99% probability level.
A survey of Canadian tax payers, published in 1997, found that they mailed in their tax forms about a week later than they predicted. They had no misconceptions about their past record of getting forms mailed in, but expected that they would get it done more quickly next time. This illustrates a defining feature of the planning fallacy: that people recognize that their past predictions have been over-optimistic, while insisting that their current predictions are realistic.

Carter and colleagues conducted three studies in 2005 that demonstrate empirical support that the planning fallacy also affects predictions concerning group tasks. This research emphasizes the importance of how temporal frames and thoughts of successful completion contribute to the planning fallacy.

Kahneman and Tversky originally explained the fallacy by envisaging that planners focus on the most optimistic scenario for the task, rather than using their full experience of how much time similar tasks require.
Roger Buehler and colleagues account for the fallacy by examining wishful thinking; in other words, people think tasks will be finished quickly and easily because that is what they want to be the case.
In a different paper, Buehler and colleagues suggest an explanation in terms of the self-serving bias in how people interpret their past performance. By taking credit for tasks that went well but blaming delays on outside influences, people can discount past evidence of how long a task should take. One experiment found that when people made their predictions anonymously, they do not show the optimistic bias. This suggests that the people make optimistic estimates so as to create a favorable impression with others, which is similar to the concepts outlined in impression management theory.
Another explanation proposed by Roy and colleagues is that people do not correctly recall the amount of time that similar tasks in the past had taken to complete; instead people systematically underestimate the duration of those past events. Thus, a prediction about future event duration is biased because memory of past event duration is also biased. Roy and colleagues note that this memory bias does not rule out other mechanisms of the planning fallacy.
Sanna and colleagues examined temporal framing and thinking about success as a contributor to the planning fallacy. They found that when people were induced to think about a deadline as distant (i.e., much time remaining) vs. rapidly approaching (i.e., little time remaining), they made more optimistic predictions and had more thoughts of success. In their final study, they found that the ease of generating thoughts also caused more optimistic predictions.
One explanation, focalism, proposes that people fall victim to the planning fallacy because they only focus on the future task and do not consider similar tasks of the past that took longer to complete than expected.
As described by Fred Brooks in The Mythical Man-Month, adding new personnel to an already-late project incurs a variety of risks and overhead costs that tend to make it even later; this is known as Brooks's law.
The "authorization imperative" offers another possible explanation: much of project planning takes place in a context which requires financial approval to proceed with the project, and the planner often has a stake in getting the project approved. This dynamic may lead to a tendency on the part of the planner to deliberately underestimate the project effort required. It is easier to get forgiveness (for overruns) than permission (to commence the project if a realistic effort estimate were provided). Such deliberate underestimation has been named by Jones and Euske "strategic misrepresentation".
Apart from psychological explanations, the phenomenon of the planning fallacy has also been explained by Taleb as resulting from natural asymmetry and from scaling issues.  The asymmetry results from random events giving negative results of delay or cost, not evenly balanced between positive and negative results.  The scaling difficulties relate to the observation that consequences of disruptions are not linear, that as size of effort increases the error increases much more as a natural effect of inefficiencies of larger efforts' ability to react, particularly efforts that are not divisible in increments.  Additionally this is contrasted with earlier efforts being more commonly on-time (e.g. the Empire State Building, The Crystal Palace, the Golden Gate Bridge) to conclude it indicates inherent flaws in more modern planning systems and modern efforts having hidden fragility. (For example, that modern efforts – being computerized and less localized invisibly – have less insight and control, and more dependencies on transportation.)
Bent Flyvbjerg and Dan Gardner write that planning on government-funded projects is often rushed so that construction can begin as soon as possible to avoid later administrations undoing or cancelling the project. They say a longer planning period tends to result in faster and cheaper construction.

The segmentation effect is defined as the time allocated for a task being significantly smaller than the sum of the time allocated to individual smaller sub-tasks of that task. In a study performed by Forsyth in 2008, this effect was tested to determine if it could be used to reduce the planning fallacy. In three experiments, the segmentation effect was shown to be influential. However, the segmentation effect demands a great deal of cognitive resources and is not very feasible to use in everyday situations.

Implementation intentions are concrete plans that accurately show how, when, and where one will act. It has been shown through various experiments that implementation intentions help people become more aware of the overall task and see all possible outcomes. Initially, this actually causes predictions to become even more optimistic. However, it is believed that forming implementation intentions "explicitly recruits willpower" by having the person commit themselves to the completion of the task. Those that had formed implementation intentions during the experiments began work on the task sooner, experienced fewer interruptions, and later predictions had reduced optimistic bias than those who had not. It was also found that the reduction in optimistic bias was mediated by the reduction in interruptions.

Reference class forecasting predicts the outcome of a planned action based on actual outcomes in a reference class of similar actions to that being forecast.

The Sydney Opera House was expected to be completed in 1963. A scaled-down version opened in 1973, a decade later. The original cost was estimated at $7 million, but its delayed completion led to a cost of $102 million.
The Eurofighter Typhoon defense project took six years longer than expected, with an overrun cost of €8 billion.
The Big Dig which undergrounded the Boston Central Artery was completed seven years later than planned,
for $8.08 billion on a budget of $2.8 billion (in 1988 dollars). 
The Denver International Airport opened sixteen months later than scheduled, with a total cost of $4.8 billion, over $2 billion more than expected.
The Berlin Brandenburg Airport is another case. After 15 years of planning, construction began in 2006, with the opening planned for October 2011. There were numerous delays. It was finally opened on October 31, 2020. The original budget was €2.83 billion; current projections are close to €10.0 billion.
Olkiluoto Nuclear Power Plant Unit 3 faced severe delay and a cost overrun. The construction started in 2005 and was expected to be completed by 2009, but completed only in 2023. Initially, the estimated cost of the project was around 3 billion euros, but the cost has escalated to approximately 10 billion euros.
California High-Speed Rail is still under construction, with tens of billions of dollars in overruns expected, and connections to major cities postponed until after completion of the rural segment.
The James Webb Space Telescope went over budget by approximately 9 billion dollars, and was sent into orbit 14 years later than its originally planned launch date.

An availability cascade is a self-reinforcing cycle that explains the development of certain kinds of collective beliefs. A novel idea or insight, usually one that seems to explain a complex process in a simple or straightforward manner, gains rapid currency in the popular discourse by its very simplicity and by its apparent insightfulness. Its rising popularity triggers a chain reaction within the social network: individuals adopt the new insight because other people within the network have adopted it, and on its face it seems plausible. The reason for this increased use and popularity of the new idea involves both the availability of the previously obscure term or idea, and the need of individuals using the term or idea to appear to be current with the stated beliefs and ideas of others, regardless of whether they in fact fully believe in the idea that they are expressing. Their need for social acceptance, and the apparent sophistication of the new insight, overwhelm their critical thinking.
The idea of the availability cascade was first developed by Timur Kuran and Cass Sunstein as a variation of information cascades mediated by the availability heuristic, with the addition of reputational cascades. The availability cascade concept has been highly influential in finance theory and regulatory research, particular with respect to assessing and regulating risk.

Availability cascades occur in a society via public discourse (e.g. the public sphere and the news media) or over social networks—sets of linked actors in one or more of several roles. These actors process incoming information to form their private beliefs according to various rules, both rational and semi-rational. The semi-rational rules include the heuristics, in particular the availability heuristic. The actors then behave and express their public beliefs according to self-interest, which might cause their publicly expressed beliefs to deviate from their privately held beliefs. 
Kuran and Sunstein emphasize the role of availability entrepreneurs, agents willing to invest resources into promoting a belief in order to derive some personal benefit. Other availability entrepreneurs with opposing interests may wage availability counter-campaigns. Other key roles include journalists and politicians, both of which are subject to economic and reputational pressures, the former in competition in the media, the latter for political status. As resources (e.g. attention and money) are limited, beliefs compete with one another in the "availability market". A given incident and subsequent availability campaign may succeed in raising the availability of one issue at the expense of other issues.

Dual process theory posits that human reasoning is divided into two systems, often called System 1 and System 2. System 1 is automatic and unconscious; other terms used for it include the implicit system, the experiential system, the associative system, and the heuristic system. System 2 is evolutionarily recent and specific to humans, performing the more slow and sequential thinking. It is also known as the explicit system, the rule-based system, the rational system, or the analytic system. In The Happiness Hypothesis, Jonathan Haidt refers to System 1 and System 2 as the elephant and the rider: while human beings incorporate reason into their beliefs, whether via direct use of facts and logic or their application as a test to hypotheses formed by other means, it is the elephant that is really in charge.

Heuristics are simple, efficient rules which people often use to form judgments and make decisions. They are mental shortcuts that replace a complex problem with a simpler one. These rules work well under most circumstances, but they can lead to systematic deviations from logic, probability or rational choice theory. The resulting errors are called "cognitive biases" and many different types have been documented. These have been shown to affect people's choices in situations like valuing a house or deciding the outcome of a legal case. Heuristics usually govern automatic, intuitive judgments but can also be used as deliberate mental strategies when working from limited information. While seemingly irrational, the cognitive biases may be interpreted as the result of bounded rationality, with human beings making decisions while economizing time and effort.
Kuran and Sunstein describe the availability heuristic as more fundamental than the other heuristics: besides being important in its own right, it enables and amplifies the others, including framing, representativeness, anchoring, and reference points.

Even educated human beings are notoriously poor at thinking statistically. The availability heuristic, first identified by Daniel Kahneman and Amos Tversky, is a mental shortcut that occurs when people judge the probability of events by how easy it is to think of examples. The availability heuristic operates on the notion that, "if you can think of it, it must be important." Availability can be influenced by the emotional power of examples and by their perceived frequency; while personal, first-hand incidents are more available than those that happened to others, availability can be skewed by the media. In his book Thinking, Fast and Slow, Kahneman cites the examples of celebrity divorces and airplane crashes; both are more often reported by the media, and thus tend to be exaggerated in perceived frequency.

An important class of judgments is those concerning risk: the expectation of harm to result from a given threat, a function of the threat's likelihood and impact. Changes in perceived risk result in risk compensation—correspondingly more or less mitigation, including precautionary measures and support for regulation. Kuran and Sunstein offer three examples of availability cascades—Love Canal, the Alar scare, and TWA Flight 800—in which a spreading public panic led to growing calls for increasingly expensive government action to deal with risks that turned out later to be grossly exaggerated. Others have used the term "culture of fear" to refer to the habitual achieving of goals via such fear appeals, notably in the case of the threat of terrorism.

In the early years of the HIV/AIDS epidemic, many believed that the disease received less attention than warranted, in part due to the stigma attached to its sufferers. Since that time advocates— availability entrepreneurs that include LGBT activists and conservative Surgeon General of the United States C. Everett Koop—have succeeded in raising awareness to achieve significant funding. Similarly, awareness and funding for breast cancer and prostate cancer are high, thanks in part to the availability of these diseases. Other prevalent diseases competing for funding but lacking the availability of HIV/AIDS or cancer include lupus, sickle-cell anemia, and tuberculosis.

The MMR vaccine controversy was an example of an unwarranted health scare. It was triggered by the publication in 1998 of a paper in the medical journal The Lancet which presented apparent evidence that autism spectrum disorders could be caused by the MMR vaccine, an immunization against measles, mumps and rubella.
In 2004, investigations by Sunday Times journalist Brian Deer revealed that the lead author of the article, Andrew Wakefield, had multiple undeclared conflicts of interest, had manipulated evidence, and had broken other ethical codes. The Lancet paper was partially retracted in 2004 and fully retracted in 2010, and Wakefield was found guilty of professional misconduct. The scientific consensus is that no evidence links the vaccine to the development of autism, and that the vaccine's benefits greatly outweigh its risks.
The claims in Wakefield's 1998 The Lancet article were widely reported; vaccination rates in the UK and Ireland dropped sharply, which was followed by significantly increased incidence of measles and mumps, resulting in deaths and severe and permanent injuries. Reaction to vaccine controversies has contributed to a significant increase in preventable diseases including measles and pertussis (whooping cough), which in 2011 experienced its worst outbreak in 70 years as a result of reduced vaccination rates. 
Concerns about immunization safety often follow a pattern: some investigators suggest that a medical condition is an adverse effect of vaccination; a premature announcement is made of the alleged adverse effect; the initial study is not reproduced by other groups; and finally, it takes several years to regain public confidence in the vaccine.

Extreme weather events provide opportunities to raise the availability of global warming. In the United States, the mass media devoted little coverage to global warming until the drought of 1988, and the testimony of James E. Hansen to the United States Senate, which explicitly attributed "the abnormally hot weather plaguing our nation" to global warming. The global warming controversy has attracted availability entrepreneurs on both sides, e.g. the book Merchants of Doubt claiming that scientific consensus had long ago been reached, and climatologist Patrick Michaels providing the denialist viewpoint.

The media inclination to sensationalism results in a tendency to devote disproportionate coverage to sympathetic victims (e.g. missing white woman syndrome), terrifying assailants (e.g. Media coverage of the Virginia Tech massacre), and incidents with multiple victims. Although half the victims of gun violence in the United States are black, generally young urban black males, media coverage and public awareness spike after suburban school shootings, as do calls for stricter gun control laws.

International adoption scandals receive disproportionate attention in the countries of adoptees' origins. As the incidents involve abuse of children, they easily spark media attention, and availability entrepreneurs (e.g. populist politicians) fan the flames of xenophobia, without making statistical comparisons of adoptee abuse in the source and target nations, or of the likelihood of abuse vs. other risks.

Poisoned candy myths are urban legends that malevolent individuals could hide poison or drugs, or sharp objects such as razor blades, needles, or broken glass in candy and distribute the candy in order to harm random children, especially during Halloween trick-or-treating. Several events fostered the candy tampering myth. The first took place in 1964, when an annoyed Long Island, New York housewife started giving out packages of inedible objects to children who she believed were too old to be trick-or-treating. The packages contained items such as steel wool, dog biscuits, and ant buttons (which were clearly labeled with the word "poison"). Although nobody was injured, she was prosecuted and pleaded guilty to endangering children. The same year saw reports of lye-filled bubble gum being handed out in Detroit and rat poison being given in Philadelphia.
The second milestone in the spread of the candy-tampering myths was an article published in The New York Times in 1970. It claimed that "Those Halloween goodies that children collect this weekend on their rounds of ‘trick or treating’ may bring them more horror than happiness", and provided specific examples of potential tampering.
In 2008, candy was found with metal shavings and metal blades embedded in it. The candy was Pokémon Valentine's Day lollipops purchased from a Dollar General store in Polk County, Florida. The candy was determined to have been manufactured in China and not tampered with within the United States. The lollipops were pulled from the shelves after a mother reported a blade in her child's lollipop and after several more lollipops with metal shavings in them were confiscated from a local elementary school. Also in 2008, some cold medicine was discovered in cases of Smarties that were handed out to children in Ontario.
Over the years, various experts have tried to debunk the various candy tampering stories. Among this group is Joel Best, a University of Delaware sociologist who specializes in investigating candy tampering legends. In his studies, and the book Threatened Children: Rhetoric and Concern about Child-Victims,  he researched newspapers from 1958 on in search of candy tampering. Of these stories, fewer than 90 instances might have qualified as actual candy tampering. Best has found five child deaths that were initially thought by local authorities to be caused by homicidal strangers, but none of those were sustained by investigation.
Despite the falsity of these claims, the news media promoted the story continuously throughout the 1980s, with local news stations featuring frequent coverage. During this time, cases of poisoning were repeatedly reported based on unsubstantiated claims or before a full investigation could be completed and often never followed up on. This one-sided coverage contributed to the overall panic and caused rival media outlets to issue reports of candy tampering as well. By 1985, the media had driven the hysteria about candy poisonings to such a point that an ABC News/The Washington Post poll that found 60% of parents feared that their children would be injured or killed because of Halloween candy sabotage.

The phenomenon of media feeding frenzies is driven by a combination of the psychology described by the availability cascade model and the financial imperatives of media organizations to retain their funding.

There are two schools of thought on how to cope with risks raised by availability cascades: technocratic and democratic. 
The technocratic approach, championed by Kuran and Sunstein, emphasizes assessing, prioritizing, and mitigating risks according to objective risk measures (e.g. expected costs, expected disability-adjusted life years (DALY)). The technocratic approach considers availability cascades to be phenomena of mass irrationality that can distort or hijack public policy, misallocating resources or imposing regulatory burdens whose costs exceed the expected costs of the risks they mitigate.
The democratic approach, championed by Paul Slovic, respects risk preferences as revealed by the availability market. For example, though lightning strikes kill far more people each year than shark attacks, if people genuinely consider death by shark worse than death by lightning, a disproportionate share of resources should be devoted to averting shark attacks.

Kuran and Sunstein recommend that availability cascades be recognized, and institutional safeguards be implemented in all branches of government. They recommend expanded product defamation laws, analogous to personal libel laws, to discourage availability entrepreneurs from knowingly spreading false and damaging reports about a product. They recommend that the legislative branch create a Risk Regulation Committee to assess risks in a broader context and perform cost-benefit analyses of risks and regulations, avoiding hasty responses pandering to public opinion. They recommend that the executive branch use peer review to open agency proposals to scrutiny by informed outsiders. They also recommend the creation of a Risk Information Center with a Risk Information Web Site to provide the public with objective risk measures. In the United States, the Centers for Disease Control and Prevention and the Federal Bureau of Investigation maintain web sites that provide objective statistics on the causes of death and violent crime.

Actor–observer asymmetry (also actor–observer bias or actor–observer difference) is a bias one exhibits when forming attributions about the behavior of others or themselves. When explaining their own behavior, people are more likely to attribute their actions to the particular situation rather than their personality, also known as a situational attribution. However, when an observer is explaining the behavior of another person, they are more likely to attribute this behavior to the actors' personality rather than situational factors, also known as dispositional attribution. For example, a politician explaining why they voted against war may say it is because war is not needed, a situational factor. On the other hand, a person judging why the politician voted in this way may say it is because the politician is too liberal, a personality trait.
Sometimes, the actor–observer asymmetry is defined as the fundamental attribution error, defined as when people tend to explain behavior using internal, personal characteristics rather than the external factors or situational influences. However, Malle (2006) highlights that these two phenomena should be distinguished because the fundamental attribution error refers to inferring stable internal traits from behaviour, whereas actor-observer asymmetry specifically refers to explanations of behaviour.
Actor-observer asymmetry is often explained using perspectives and salience. When forming attributions, perspective highlights the situation, and what is occurring around the perceiver is most salient. As a result, perceivers may be more likely to make attributions based on these salient situational factors. However, when judging someone else, their behaviour is more salient than the situation. This may explain the greater chance of making dispositional attributions. Furthermore, when making judgements on one's own behaviour, much more information regarding the self is available, including knowledge of past behaviour. On the other hand, when judging others' behaviour, much less information is available. This lack of quality information likely also contributes to differences in attributions made. 
The specific hypothesis of an actor–observer asymmetry in attribution was originally proposed by Edward Jones and Richard Nisbett, who stated that "actors tend to attribute the causes of their behavior to stimuli inherent in the situation, while observers tend to attribute behavior to stable dispositions of the actor". Supported by initial evidence, the hypothesis was long held as firmly established. However, a meta-analysis of all the published tests of the hypothesis between 1971 and 2004 found that there was no actor–observer asymmetry of the sort that had been previously proposed. The author of the study interpreted this result not so much as proof that actors and observers explained behavior exactly the same way but as evidence that the original hypothesis was fundamentally flawed in the way it framed people's explanations of behavior as either stable dispositional attributions or situational attributions.
Considerations of actor–observer differences can be found in other disciplines as well, such as philosophy (e.g. privileged access, incorrigibility), management studies, artificial intelligence, semiotics, anthropology, and political science.

The background of the actor-observer asymmetry was established in the 1960s, with social psychology's increasing interest in the cognitive mechanisms by which people make sense of their own and other people's behavior. This interest was instigated by Fritz Heider's book, The Psychology of Interpersonal Relations, and the research in its wake has become known as "attribution research" or "attribution theory."
The specific hypothesis of an "actor–observer asymmetry" was first proposed by social psychologists Jones and Nisbett in 1971. Jones and Nisbett hypothesized that these two roles (actors and observers) produce asymmetric explanations.Their research findings were that "there is pervasive tendency for actors to attribute their actions to situational requirements, whereas observers tend to attribute the same actions to stable personal dispositions". By this theory, a student who studies hard for an exam is likely to explain her own (the actor's) intensive studying by referring to the upcoming difficult exam (a situational factor), whereas other people (the observers) are likely to explain her studying by referring to her dispositions, such as being hardworking or ambitious.

Soon after the publication of the actor–observer hypothesis, numerous research studies tested its validity, most notably the first such test in 1973 by Nisbett et al. The authors found initial evidence for the hypothesis, and so did Storms,who also examined one possible explanation of the hypothesis: actors explain their behaviors because they attend to the situation (not to their own behaviors) whereas observers attend to the actor's behavior (not to the situation). Based largely on this initial supporting evidence, the confidence in the hypothesis became uniformly high.
In the Nisbett et al. (1973) study, actor-observer asymmetry was tested by having participants select between two traits (such as energetic and relaxed), choosing which trait best matched the personality of the target, or if the trait that best matched them depended on the situation. They had participants chose between traits many times to see if participants mainly chose a specific trait or said it depended on the situation. Participants repeated the task saying what trait best matched for different people: their best friend, father, a famous news anchor, and themselves. The results showed that participants more frequently stated that the trait depended on the situation for themselves whereas for others' they often chose one trait that best described them. This provided evidence for actor-observer asymmetry because participants viewed other's personality traits as stable whereas their own as dependent on the situation.
Functional neuroimaging studies have also demonstrated differential activation of brain regions when making self-focused vs. other-focused judgments. The medial prefrontal cortex (mPFC), left temporoparietal junction (TPJ), and posterior cingulate were involved in both self-related and other-related judgments. However, self-related judgments more often activated the ventral mPFC (vmPFC), left ventrolateral PFC, and left insula. In contrast, other-related judgments more frequently activated the dorsal mPFC (dmPFC), bilateral TPJ, and cuneus. These findings provide neurological depth to support the actor-observer asymmetry, with fundamentally different cognitive and neural processes involved in different types of attribution. Such work highlights how differential attribution is not only a cognitive bias but a biological one, too.

Over 100 studies have been published since 1971 in which the hypothesis was put to further tests (often in the context of testing another hypothesis about causal attributions). Bertram Malle examined this entire literature in a meta-analysis, finding that, across 170 individual tests, the asymmetry practically did not exist. Under circumscribed conditions (i.e. if the actor was portrayed as highly idiosyncratic, or in negative events), it could sometimes be found, but under other conditions, the opposite was found. The conclusion was that the widely held assumption of an actor–observer asymmetry was false.
External influences on the actor-observer effect are largely underemphasized. Research has shown that the actor-observer asymmetry is more likely to occur when an outcome is uncertain, partially controllable, and important. These findings suggest that the actor-observer asymmetry is not a fixed bias but rather a context-dependent phenomenon, challenging the idea that it universally governs attribution patterns.

A significant body of literature exists to support the idea that there are cross-cultural differences in the attribution process. When considering the fundamental attribution error, it has been extended to be known as the "ultimate attribution error" instead, for initially in its discovery it was assumed to be a universal, or a fundamental, phenomenon. It has since been demonstrated that Western cultures are more susceptible to making the fundamental attribution error in comparison to Eastern cultures. Today, the ultimate attribution error is understood to occur when members of an in-group attribute negative behaviours of an out-group to their disposition, while attributing positive behaviours to situational factors. The opposite is true for when members of their own in-group engage in positive or negative behaviours.

Findings on the actor-observer asymmetry extend beyond social perception, influencing how individuals internalize judgments from others. Others’ attributions can influence one’s self-view. When someone is frequently exposed to a critical observer who attributes mistakes to personal flaws or enduring character traits, they may begin to adopt this perspective, interpreting their own actions through the same lens. Internalizing such criticism can lead to a belief that their abilities are severely lacking and that their character is fundamentally flawed. Conversely, if a person regularly hears a supportive observer acknowledge their competence while recognizing that certain tasks are inherently challenging, they are more likely to develop a balanced approach to interpreting both their successes and setbacks.

In attribution, the actor–observer asymmetry is often confused with the concept of self-serving bias — the claim that people attribute positive outcomes of their behaviour to dispositional factors, while attributing negative outcomes to situational factors. The difference between the two hypotheses is that the actor–observer asymmetry requires a specific comparison between actor explanations and observer explanations, while the self-serving bias refers only to actor explanations. Furthermore, actor-observer asymmetry is expected to hold for all events and behaviors (whether they are positive or negative), while the self-serving bias is often formulated as a complete reversal in actors' and observers' explanation tendencies as a function of positive or negative events. For example, the self-serving bias holds that for positive events, actors will select explanations that refer to their own dispositions, (e.g., "I am smart"); however, for negative events, actors will select explanations that refer to the situation, (e.g., "the test was hard").

The correspondence bias is similar to actor-observer asymmetry in that both involve systematic differences in how people attribute behavior. However, the correspondence bias specifically focuses on disposition-congruent judgements of others based on their behaviours, even if these behaviours originate due to the situation. It states that observers believe they know an individual’s underlying disposition solely based on their actions. Actor-observer asymmetry, on the other hand, more closely aligns with a self-serving bias.
Much like actor-observer asymmetry, correspondence bias is supported by several cognitive and environmental factors that contribute to its prevalence. There are four key mechanisms that each produce different forms of this bias: lack of awareness, unrealistic expectations, inflated categorizations, and incomplete corrections.
A lack of awareness is constituted by ignorance of the situation in forming attributions. This often occurs due to naive realism, the tendency to believe that one’s own perception of reality is objective and unbiased. Such beliefs may cause observers to think the actor shares their interpretation of the situation, preventing further deliberation to consider situational factors.
Unrealistic expectations include underestimating the power of the situation in various contexts. Interestingly, situational influence can also be overestimated, although this condition does not result in the correspondence bias.
Inflated categorization refers to how ambiguous behaviors can be “inflated” due to the situation in which they occurred. If a perciever uses contextual cues to infer the nature, strength, etc. of behaviours, any disparities between their inference and reality will become especially striking. These disparities then prompt strong dispositional attributions.
Incomplete corrections are the inability to properly correct for one’s immediate assessment of the situation. Observers may judge based on disposition at first, then consider situational factors, but the adjustment between these cognitive mechanisms is not always optimal. Thus, disposition is still largely overrepresented in cognitive appraisals.
Despite the specific mechanism that produces correspondence bias, all forms highlight the pervasive tendency to overattribute behavior to dispositional factors while neglecting situational influences. This process gives rise to various positive and negative implications. For example, while correspondence bias gives observers control of their social world by predicting others, it can also lead to false interpretations of the situation. In some cases, the benefits outweigh the costs by saving the observer valuable time and cognitive effort, but nevertheless, this heuristic must be used with caution.
Additionally, the correspondence bias has a forward thinking component. Observers tend to attribute the actions of others to their future behavior. When someone witnesses another person's actions, they are likely to attribute those same actions to that person's future behavior, which is why first impressions are so important. Once an action is observed, it can be difficult for the observer to imagine the actor behaving differently. On the other hand, actors may find it difficult to attribute a single action to their own overall behavior. They view themselves as more responsive and in control of situational matters. While actors can attribute their past actions, observers can only attribute the one action they have witnessed to the actor, leading them to attribute dispositional rather than situational factors to the actor's behavior.

The concept of trait ascription bias provides an alternative explanation for actor-observer asymmetry. Trait ascription bias refers to the tendency to perceive one's own personality, beliefs, and behaviors as dynamic and adaptable while viewing others as more fixed and predictable. This leads people to oversimplify and categorize others based on their actions, attributing behavior to inherent traits, whereas they see their own actions as influenced by context and circumstance.

The clustering illusion is the tendency to erroneously consider the inevitable "streaks" or "clusters" arising in small samples from random distributions to be non-random. The illusion is caused by a human tendency to underpredict the amount of variability likely to appear in a small sample of random or pseudorandom data.
Thomas Gilovich, an early author on the subject, argued that the effect occurs for different types of random dispersions. Some might perceive patterns in stock market price fluctuations over time, or clusters in two-dimensional data such as the locations of impact of World War II V-1 flying bombs on maps of London. Although Londoners developed specific theories about the pattern of impacts within London, a statistical analysis by R. D. Clarke originally published in 1946 showed that the impacts of V-2 rockets on London were a close fit to a random distribution.

Using this cognitive bias in causal reasoning may result in the Texas sharpshooter fallacy, in which differences in data are ignored and similarities are overemphasized. More general forms of erroneous pattern recognition are pareidolia and apophenia. Related biases are the illusion of control which the clustering illusion could contribute to, and insensitivity to sample size in which people don't expect greater variation in smaller samples. A different cognitive bias involving misunderstanding of chance streams is the gambler's fallacy.

Daniel Kahneman and Amos Tversky explained this kind of misprediction as being caused by the representativeness heuristic (which itself they also first proposed).

In psychology and behavioral economics, the endowment effect, also known as divestiture aversion, is the finding that people are more likely to retain an object they own than acquire that same object when they do not own it. The endowment theory can be defined as "an application of prospect theory positing that loss aversion associated with ownership explains observed exchange asymmetries."
This is typically illustrated in two ways. In a valuation paradigm, people's maximum willingness to pay (WTP) to acquire an object is typically lower than the least amount they are willing to accept (WTA) to give up that same object when they own it—even when there is no cause for attachment, or even if the item was only obtained minutes ago. In an exchange paradigm, people given a good are reluctant to trade it for another good of similar value. For example, participants first given a pen of equal expected value to that of a coffee mug were generally unwilling to trade, whilst participants first given the coffee mug were also unwilling to trade it for the pen.
A more controversial third paradigm used to elicit the endowment effect is the mere ownership paradigm, primarily used in experiments in psychology, marketing, and organizational behavior. In this paradigm, people who are randomly assigned to receive a good ("owners") evaluate it more positively than people who are not randomly assigned to receive the good ("controls"). The distinction between this paradigm and the first two is that it is not incentive-compatible. In other words, participants are not explicitly incentivized to reveal the extent to which they truly like or value the good.
The endowment effect can be equated to the behavioural model willingness to accept or pay (WTAP), a formula sometimes used to find out how much a consumer or person is willing to put up with or lose for different outcomes. However, this model has come under recent criticism as potentially inaccurate.

Other examples of the endowment effect include work by Ziv Carmon and Dan Ariely, who found that participants' hypothetical selling price (willingness to accept or WTA) for NCAA final four tournament tickets were 14 times higher than their hypothetical buying price (willingness to pay or WTP). Also, work by Hossain and List (Working Paper) discussed in the Economist in 2010, showed that workers worked harder to maintain ownership of a provisionally awarded bonus than they did for a bonus framed as a potential yet-to-be-awarded gain. In addition to these examples, the endowment effect has been observed using different goods in a wide range of different populations, including children, great apes, and new world monkeys.

The endowment effect has been observed from ancient times:
For most things are differently valued by those who have them and by those who wish to get them: what belongs to us, and what we give away, always seems very precious to us.
Psychologists first noted the difference between consumers' WTP and WTA as early as the 1960s. The term endowment effect however was first explicitly coined in 1980 by the economist Richard Thaler in reference to the under-weighting of opportunity costs as well as the inertia introduced into a consumer's choice processes when goods included in their endowment become more highly valued than goods that are not.
At the time Thaler's conceptualisation of the endowment effect was in direct contrast to that of accepted economic theory, which assumed humans were completely rational when making decisions. Through his contrasting viewpoint, Thaler was able to offer a clearer understanding of how humans make economic decisions. In the years that followed, extensive investigations into the endowment effect have been conducted producing a wealth of interesting empirical and theoretical findings.

The leading explanation for the aforementioned WTP-WTA gap is that of loss aversion. It was first linked by Kahneman and his colleagues that selling an endowment means the loss of the object, and as humans are aligned to be more loss-averse, less utility is obtained from acquirement of the same endowment. They go on to suggest that the endowment effect, when considered as a facet of loss-aversion, would thus violate the Coase theorem, and was described as inconsistent with standard economic theory which asserts that a person's willingness to pay (WTP) for a good should be equal to their willingness to accept (WTA) compensation to be deprived of the good, a hypothesis which underlies consumer theory and indifference curves. Another aspect of loss aversion exhibited within the endowment effect is that opportunity costs are often undervalued. The overcharging of the selling item stems from the fixation of losing the item rather than the unattained gain if the sale falls through.
The correlation between the two theories is so high that the endowment effect is often seen as the presentation of loss aversion in a riskless setting. However, these claims have been disputed and other researchers claim that psychological inertia, differences in reference prices relied on by buyers and sellers, and ownership (attribution of the item to self) and not loss aversion are the key to this phenomenon.

David Gal proposed a psychological inertia account of the endowment effect. In this account, sellers require a higher price to part with an object than buyers are willing to pay because neither has a well-defined, precise valuation for the object and therefore there is a range of prices over which neither buyers nor sellers have much incentive to trade. For example, in the case of Kahneman et al.'s (1990) classic mug experiments (where sellers demanded about $7 to part with their mug whereas buyers were only willing to pay, on average, about $3 to acquire a mug) there was likely a range of prices for the mug ($4 to $6) that left the buyers and sellers without much incentive to either acquire or part with it. Buyers and sellers therefore maintained the status quo out of inertia. Conversely, a high price ($7 or more) yielded a meaningful incentive for an owner to part with the mug; likewise, a relatively low price ($3 or less) yielded a meaningful incentive for a buyer to acquire the mug.

According to reference-dependent theories, consumers first evaluate the potential change in question as either being a gain or a loss. In line with prospect theory (Tversky and Kahneman, 1979), changes that are framed as losses are weighed more heavily than are the changes framed as gains. Thus an individual owning "A" amount of a good, asked how much he/she would be willing to pay to acquire "B", would be willing to pay a value (B-A) that is lower than the value that he/she would be willing to accept to sell (C-A) units; the value function for perceived gains is not as steep as the value function for perceived losses.
Figure 1 presents this explanation in graphical form. An individual at point A, asked how much he/she would be willing to accept (WTA) as compensation to sell X units and move to point C, would demand greater compensation for that loss than he/she would be willing to pay for an equivalent gain of X units to move him/her to point B. Thus the difference between (B-A) and (C-A) would account for the endowment effect. In other words, he/she expects more money while selling; but wants to pay less while buying the same amount of goods.
Figure 1: Prospect Theory and the Endowment Effect

Hanemann (1991), develops a neoclassical explanation for the endowment effect, accounting for the effect without invoking prospect theory.
Figure 2 presents this explanation in graphical form. In the figure, two indifference curves for a particular good X and wealth are given. Consider an individual who is given goods X such that they move from point A (where they have X0 of good X) to point B (where they have the same wealth and X1 of good X). Their WTP represented by the vertical distance from B to C, because (after giving up that amount of wealth) the individual is indifferent about being at A or C. Now consider an individual who gives up goods such that they move from B to A. Their WTA represented by the (larger) vertical distance from A to D because (after receiving that much wealth) they are indifferent about either being at point B or D. Shogren et al. (1994) has reported findings that lend support to Hanemann's hypothesis. However, Kahneman, Knetsch, and Thaler (1991) find that the endowment effect continues even when wealth effects are fully controlled for.
Figure 2: Hanemann's Endowment Effect Explanation
When goods are indivisible, a coalitional game can be set up so that a utility function can be defined on all subsets of the goods.
Hu (2020) shows the endowment effect when the utility function is superadditive, i.e., the value of the whole is greater than the sum of its parts. Hu (2020) also introduces a few unbiased solutions which mitigate endowment bias.
Experiments in cognitive psychology have demonstrated that the endowment effect can be brought about by asymmetries in cognitive processing in judging owned and not-owned goods. A 2007 fMRI study by Knutson et al. demonstrated that the insula, an area of the brain associated with loss aversion, is stimulated when people ponder relinquishing goods they already possess. This is consistent with the hypothesis that the endowment effect entails not only ownership bias but also emotional attachment and loss anticipation neural processes. Further, the effect is also conditioned by cultural environment and personality difference in risk sensitivity.

Sellers may dictate a price based on the desires of multiple potential buyers, whereas buyers may consider their own taste. This can lead to differences between buying and selling prices because the market price is typically higher than one's idiosyncratic price estimate. According to this account, the endowment effect can be viewed as under-pricing for buyers compared to the market price; or over-pricing for sellers compared to their individual taste. Two recent lines of study support this argument. Weaver and Frederick (2012)  presented their participants with retail prices of products, and then asked them to specify either their buying or selling price for these products. The results revealed that sellers' valuations were closer to the known retail prices than those of buyers. A second line of studies is a meta-analysis of buying and selling of lotteries. A review of over 30 empirical studies showed that selling prices were closer to the lottery's expected value, which is the normative price of the lottery: hence the endowment effect was consistent with buyers' tendency to under-price lotteries as compared to the normative price. One possible reason for this tendency of buyers to indicate lower prices is their risk aversion. By contrast, sellers may assume that the market is heterogeneous enough to include buyers with potential risk neutrality and therefore adjust their price closer to a risk neutral expected value.

Some economists have questioned the effect's existence. Hanemann (1991) noted that economic theory only suggests that WTP and WTA should be equal for goods which are close substitutes, so observed differences in these measures for goods such as environmental resources and personal health can be explained without reference to an endowment effect. Shogren, et al. (1994) noted that the experimental technique used by Kahneman, Knetsch and Thaler (1990) to demonstrate the endowment effect created a situation of artificial scarcity. They performed a more robust experiment with the same goods used by Kahneman, Knetsch and Thaler (chocolate bars and mugs) and found little evidence of the endowment effect in substitutable goods, acknowledging the endowment effect as valid for goods without substitutes—non-renewable Earth resources being an example of these. Others have argued that the use of hypothetical questions and experiments involving small amounts of money tells us little about actual behavior (e.g. Hoffman and Spitzer, 1993, p. 69, n. 23) with some research supporting these points (e.g., Kahneman, Knetsch and Thaler, 1990, Harless, 1989) and others not (e.g. Knez, Smith and Williams, 1985). More recently, Plott and Zeiler have challenged the endowment effect theory by arguing that observed disparities between WTA and WTP measures are not reflective of human preferences, but rather such disparities stem from faulty experimental designs.

Implications regarding the endowment effect are present at both the individual and corporate level. Its presence can cause market inefficiencies and value irregularities between buyers and sellers with similar consequences at smaller or upscaled transactions.

Herbert Hovenkamp (1991) has argued that the presence of an endowment effect has significant implications for law and economics, particularly in regard to welfare economics. He argues that the presence of an endowment effect indicates that a person has no indifference curve (see however Hanemann, 1991) rendering the neoclassical tools of welfare analysis useless, concluding that courts should instead use WTA as a measure of value. Fischel (1995) however, raises the counterpoint that using WTA as a measure of value would deter the development of a nation's infrastructure and economic growth. The endowment effect changes the shape of the indifference curves substantially Similarly, another study that is focused on the Strategic Reallocations for Endowment analyses how it is the case that economics's agents welfare could potentially increase if they change their endowment holding.
Further to this, the endowment effect has been linked to both economic and psychological impacts of various scale. For example, often individuals refuse the sale of their house or upscale their expected value simply due to their emotional attachment and effort poured into it. This means they might either stick with a property which causes greater inconvenience to alternatives or have an increased level of difficulties associated with its sale. Either of these scenarios both negatively impact the relevant economy and the individual's mental welfare. Alternatively, if a buyer is subject to purchasing the item at the WTA level when it is set above market price, they are subject to overspending which positively impacts the economy whilst potentially reducing individual welfare yet again.

In recent years the endowment effect has largely been leveraged within e-commerce. Businesses have expanded more rapidly than previous years through its effective integration into marketing products and services. Here consumers are often given a sense of ownership over what the business possesses thereby unlocking the cognitive bias.

By offering free trials to select services, business not only expand the number of users reached, but during this trial period they also give consumers a sense of ownership. Consumer's psychological perception thus makes them more reluctant to part with the service when the trial ends, thereby increasing the quantity of subscribers.

This marketing strategy makes consumers more likely to purchase the product due to the perception of it being more endowing. However, once purchased, customers are less inclined to return it even if a level of dissatisfaction was experienced.

Various businesses offer a sense of ownership through showing customers what their product might look like in a relatable environment. Fashion and furniture businesses largely rely on haptic imagery to sell their products. While they do not necessarily offer customers to use their products they create an image of what could be, by either offering online viewing adjustments or appealing to ones sense of imagination. This feeling of ownership makes it harder for consumers to let go of the image and thus the product.

Belief perseverance (also known as conceptual conservatism) is maintenance of a belief despite new information that firmly contradicts it.
Since rationality involves conceptual flexibility, belief perseverance is consistent with the view that human beings act at times in an irrational manner. Philosopher F.C.S. Schiller holds that belief perseverance "deserves to rank among the fundamental 'laws' of nature".
If beliefs are strengthened after others attempt to present evidence debunking them, this is known as a backfire effect. There are psychological mechanisms by which backfire effects could potentially occur, but the evidence on this topic is mixed, and backfire effects are very rare in practice.
A 2020 review of the scientific literature on backfire effects found that there have been widespread failures to replicate their existence, even under conditions that theoretically would be favorable to observing them. Due to the lack of reproducibility, as of 2020 most researchers believe that backfire effects either are unlikely to occur on the broader population level, or only occur in very specific circumstances, or do not exist.
For most people, corrections and fact-checking are very unlikely to have a negative effect, and there is no specific group of people in which backfire effects have been consistently observed.

According to Lee Ross and Craig A. Anderson, "beliefs are remarkably resilient in the face of empirical challenges that seem logically devastating".
The first study of belief perseverance was carried out by Festinger, Riecken, and Schachter. These psychiatrists spent time with members of a doomsday cult who believed the world would end on December 21, 1954. Despite the failure of the forecast, most believers continued to adhere to their faith. In When Prophecy Fails: A Social and Psychological Study of a Modern Group That Predicted the Destruction of the World (1956) and A Theory of Cognitive Dissonance (1957), Festinger proposed that human beings strive for internal psychological consistency to function mentally in the real world. A person who experiences internal inconsistency tends to become psychologically uncomfortable and is motivated to reduce the cognitive dissonance. They tend to make changes to justify the stressful behavior, either by adding new parts to the cognition causing the psychological dissonance (rationalization) or by avoiding circumstances and contradictory information likely to increase the magnitude of the cognitive dissonance (confirmation bias).
When asked to reappraise probability estimates in light of new information, subjects displayed a marked tendency to give insufficient weight to the new evidence. They refused to acknowledge the inaccurate prediction as a reflection of the overall validity of their faith. In some cases, subjects reported having a stronger faith in their religion than before.
Lee Ross and Craig A. Anderson led some subjects to the false belief that there existed a positive correlation between a firefighter's stated preference for taking risks and their occupational performance. Other subjects were told that the correlation was negative. The participants were then thoroughly debriefed and informed that there was no link between risk taking and performance. These authors found that post-debriefing interviews pointed to significant levels of belief perseverance.
In another study, subjects spent about four hours following instructions of a hands-on instructional manual.  At a certain point, the manual introduced a formula which led them to believe that spheres were 50 percent larger than they are. Subjects were then given an actual sphere and asked to determine its volume; first by using the formula, and then by filling the sphere with water, transferring the water to a box, and directly measuring the volume of the water in the box. In the last experiment in this series, all 19 subjects held a Ph.D. degree in a natural science, were employed as researchers or professors at two major universities, and carried out the comparison between the two volume measurements a second time with a larger sphere. All but one of these scientists clung to the spurious formula despite their empirical observations.

If beliefs are strengthened after others attempt to present evidence debunking them, this is known as a backfire effect (compare boomerang effect). For example, this would apply if providing information on the safety of vaccinations resulted in increased vaccination hesitancy. Types of backfire effects include: Familiarity Backfire Effect (from making myths more familiar), Overkill Backfire Effect (from providing too many arguments), and Worldview Backfire Effect (from providing evidence that threatens someone's worldview). There are a number of techniques to debunk misinformation, such as emphasizing the core facts and not the myth, or providing explicit warnings that the upcoming information is false, and providing alternative explanations to fill the gaps left by debunking the misinformation. However, more recent studies provided evidence that the backfire effects are not as likely as once thought.
There are psychological mechanisms by which backfire effects could potentially occur, but the evidence on this topic is mixed, and backfire effects are very rare in practice. A 2020 review of the scientific literature on backfire effects found that there have been widespread failures to replicate their existence, even under conditions that would be theoretically favorable to observing them. Due to the lack of reproducibility, as of 2020 most researchers believe that backfire effects are either unlikely to occur on the broader population level, or they only occur in very specific circumstances, or they do not exist. Brendan Nyhan, one of the researchers who initially proposed the occurrence of backfire effects, wrote in 2021 that the persistence of misinformation is most likely due to other factors.
For most people, corrections and fact-checking are very unlikely to have a negative impact, and there is no specific group of people in which backfire effects have been consistently observed. Presenting people with factual corrections has been demonstrated to have a positive effect in many circumstances. For example, this has been studied in the case of informing believers in 9/11 conspiracy theories about statements by actual experts and witnesses. One possibility is that criticism is most likely to backfire if it challenges someone's worldview or identity. This suggests that an effective approach may be to provide criticism while avoiding such challenges.
In many cases, when backfire effects have been discussed by the media or by bloggers, they have been over-generalized from studies on specific subgroups to incorrectly conclude that backfire effects apply to the entire population and to all attempts at correction.

Physicist Max Planck wrote that "the new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it". For example, the heliocentric theory of the great Greek astronomer, Aristarchus of Samos, had to be rediscovered about 1,800 years later, and even then undergo a major struggle before astronomers took its veracity for granted.
Belief persistence is frequently accompanied by intrapersonal cognitive processes. "When the decisive facts did at length obtrude themselves upon my notice," wrote the chemist Joseph Priestley, "it was very slowly, and with great hesitation, that I yielded to the evidence of my senses."

Students often "cling to ideas that form part of their world view even when confronted by information that does not coincide with this view." For example, students may spend months studying the Solar System and do well on related tests, but still believe that the Moon's perceived phases are produced by Earth's shadow. What they learned was not able to intrude on the beliefs they held prior to that knowledge.

The causes of belief perseverance remain unclear. Experiments in the 2010s suggest that neurochemical processes in the brain underlie the strong attentional bias of reward learning. Similar processes could underlie belief perseverance.
Peter Marris suggests that the process of abandoning a conviction is similar to the working out of grief. "The impulse to defend the predictability of life is a fundamental and universal principle of human psychology." Human beings possess "a deep-rooted and insistent need for continuity".
Philosopher of science Thomas Kuhn points to the resemblance between conceptual change and Gestalt perceptual shifts (e.g., the difficulty encountered in seeing the hag as a young lady). Hence, the difficulty of switching from one conviction to another could be traced to the difficulty of rearranging one's perceptual or cognitive field.

Optimism bias is the tendency of an individual to overestimate the likelihood of positive events and underestimate that of negative events. 
A cognitive bias, the optimistic bias is common across cultures, genders, ethnicities, nationalities, and age groups. It has implications to individual and group decision making, public health, policy, economics, and law.
The extent of optimism bias depends on a person's overall mood, their desired end state, the information they have about themselves and others, and their cognitive mechanisms. Generally, the optimism bias is stronger for underestimating negative events than overestimating positive events. 
It is also known as unrealistic optimism, comparative optimism, and optimist's delusion.

Optimism bias is typically measured through two determinants of risk: absolute risk, where individuals are asked to estimate their likelihood of experiencing a negative event, and comparative risk, where individuals are asked to estimate the likelihood of experiencing a negative event compared to others of the same age and sex. Problems can occur when trying to measure absolute risk, because it is difficult to determine the actual risk statistic for a person and compare it against their personal estimate. 
Direct comparisons ask whether an individual's own risk of experiencing an event is less than, greater than, or equal to someone else's risk. Indirect comparisons ask individuals to provide separate estimates of their own risk and that of others in experiencing the same event.
After obtaining scores, researchers use the information to determine if there is a difference in the average risk estimate of the individual compared to the average risk estimate of their peers. Generally, in negative events, the mean risk of an individual appears lower than the risk estimate of others. This is then used to demonstrate the bias' effect. 
Optimistic bias can only be defined and measured at a group level, because at an individual level the positive assessment could be true. Likewise, difficulties can arise in measurement procedures, as it is difficult to determine when someone is being optimistic, realistic, or pessimistic. Research suggests that the bias comes from an overestimate of group risks rather than underestimating one's own risk.

The valence effect refers to the influence of positive or negative framing on unrealistic optimism. Since 2003, Ron S. Gold and colleagues have studied this phenomenon by presenting the same event in different ways. For example, some participants receive information about conditions that promote a health-related event, such as developing heart disease, and are asked to rate the likelihood of experiencing it. Others receive matched information about conditions that prevent the same event and are asked to rate the likelihood of avoiding it. Their findings consistently show that unrealistic optimism is greater when events are framed with negative valence than with positive valence.

Functional neuroimaging suggests a key role for the rostral Anterior Cingulate Cortex (ACC) in modulating both emotional processing and autobiographical retrieval. It is part of brain network showing extensive correlation between rostral ACC and amygdala during imagining of future positive events and restricted correlation during imagining of future negative events. Based on these data, it is suggested that the rostral ACC has a crucial part to play in creating positive images of the future and ultimately, in ensuring and maintaining the optimism bias.
It has also been reported in other animals, such as rats and birds.

Factors leading to the optimistic bias can be categorized into four different groups: desired end states of comparative judgment; cognitive mechanisms; information about the self versus a target; and underlying affect.

Many explanations for the optimistic bias come from the goals that people want and outcomes they wish to see.

Self-enhancement theories suggest that optimistic predictions are satisfying, that it feels good to think that positive events will happen, and that people can control their anxiety and other negative emotions if they believe they are better off than others. As such, the theory predicts that people tend to focus on finding information that supports what they want to see happen. This also suggests that people might lower their risks compared to others to make themselves look better than average, a form of self-presentation.

Self-presentation theories suggest that people unconsciously attempt to establish and maintain a desired personal image in social situations, and that the optimistic bias is a representative of self-presentational processes. Studies also suggest that individuals who present themselves in a pessimistic and more negative light are generally less accepted by the rest of society.

People tend to be more optimistically biased when they believe they have more control over events than others. Studies have suggested that the greater perceived control someone has, the greater their optimistic bias. Stemming from this, control is a stronger factor when it comes to personal risk assessments, but not when assessing others.
The relationship between the optimistic bias and perceived control is moderated by a number of factors, such as nationality, age, occupation, research methodology, and situation. An opposing factor to perceived control is prior experience. Prior experience is typically associated with less optimistic bias; some studies suggest this is from a decrease in the perception of personal control, or by making it easier for individuals to imagine themselves at risk.
In public health and safety contexts, people are more likely to think that they will not be harmed in a car accident if they are driving the vehicle. Someone believes that they have a lot of control over becoming infected with HIV, they are more likely to view their risk of contracting the disease to be low.

The optimistic bias is possibly also influenced by cognitive mechanisms that guide judgments and decision-making processes.

Likelihood estimates linked to optimistic bias often depend on how closely an event matches a person’s mental model of that event. Some researchers suggest the representativeness heuristic underlies this bias; people tend to rely on stereotypes in decision making, relying upon specific examples that relate directly to what they are asked rather than an average example.

One of the difficulties of the optimistic bias is that people know more about themselves than they do about others. While individuals know how to think about themselves as a single person, they still think of others as a generalized group, which leads to biased estimates and inabilities to sufficiently understand their target or comparison group. Likewise, when making judgments and comparisons about their risk compared to others, people generally ignore the average person, but primarily focus on their own feelings and experiences.

Perceived risk differences increase with the perceived interpersonal distance between self and target. Closer targets produce more similar risk estimates, while distant ones amplify differences. Studies show that in-group comparisons yield greater perceived similarity than out-group comparisons. People within an in-group not only tend to more readily work with each other initially but also display closer perceived risk differences than when comparing themselves to out-groups. Optimistic bias is also stronger when the target is vague or unfamiliar, and weaker when it is a known person such as a friend or family member, though this may also reflect the greater information available about close others.

Individuals know a lot more about themselves than they do about others. Because information about others is less available, information about the self versus others leads people to make specific conclusions about their own risk, but results in them having a harder time making conclusions about the risks of others. This leads to differences in judgments and conclusions about self-risks compared to the risks of others, leading to larger gaps in the optimistic bias.

Person-positivity bias is the tendency to evaluate an object more favorably the more the object resembles an individual human being. Generally, the more a comparison target resembles a specific person, the more familiar it will be. Groups of people are considered to be more abstract concepts, which leads to less favorable judgments. With regards to the optimistic bias, when people compare themselves to an average person, whether someone of the same sex or age, the target continues to be viewed as less human and less personified, which will result in less favorable comparisons between the self and others.

Egocentric thinking refers to how individuals know more of their own personal information and risk that they can use to form judgments and make decisions. One difficulty, though, is that people have a large amount of knowledge about themselves, but no knowledge about others. Therefore, when making decisions, people have to use other information available to them, such as population data, in order to learn more about their comparison group. This can relate to an optimism bias because while people are using the available information they have about themselves, they have more difficulty understanding correct information about others. 
It is also possible that someone can escape egocentric thinking. In one study, researchers had one group of participants list all factors that influenced their chances of experiencing a variety of events, and then a second group read the list. Those who read the list showed less optimistic bias in their own reports. It's possible that greater knowledge about others and their perceptions of their chances of risk bring the comparison group closer to the participant.

It is also possible that individuals underestimate the amount of control the average person has. This is explained in two different ways: First, that people underestimate the control that others have in their lives; second, that people completely overlook that others have control over their own outcomes.

The last factor of optimistic bias is that of underlying affect and affect experience. Research has found that people show less optimistic bias when experiencing a negative mood, and more optimistic bias when in a positive mood. Sad moods reflect greater memories of negative events, which lead to more negative judgments, while positive moods promote happy memories and more positive feelings. This suggests that overall negative moods, including depression, result in increased personal risk estimates but less optimistic bias overall. Anxiety also leads to less optimistic bias, continuing to suggest that overall positive experiences and positive attitudes lead to more optimistic bias in events.

In public health contexts, optimistic bias can discourage individuals from adopting preventive measures.
People who underestimate their comparative risk of heart disease tend to know less about the condition and, even after reading informative material, remain less concerned about their risk. Risk perception plays a significant role in behaviors such as exercise, diet, and sunscreen use. Prevention efforts often focus on adolescents, a group with a high incidence of risky health-related behaviors including smoking, drug use, and unsafe sex. Although adolescents are generally aware of these risks, awareness alone rarely alters habits. Moreover, adolescents with a strong positive optimistic bias toward risky behaviors tend to show an increase in such bias with age.
Methodological issues complicate the study prevention in public health. Cross-sectional studies frequently use unconditional risk questions, which ask about the likelihood of an event without clarifying outcomes or distinguishing between events that have and have not occurred. Such designs limit interpretability. Similarly, vaccine studies often compare perceptions of vaccinated and unvaccinated individuals without first assessing each participant’s underlying perception of risk.

Autistic people are less susceptible to this kind of bias.
Pessimism bias is an effect in which people exaggerate the likelihood that negative things will happen to them. People with depression are particularly likely to exhibit pessimism bias. Surveys of smokers have found that their ratings of their risk of heart disease showed a small but significant pessimism bias; the literature as a whole is inconclusive.

Optimism bias influences decisions and forecasts in policy, planning, and management. The planning fallacy, that is, that costs and completion times of planned decisions tend to be underestimated and the benefits overestimated, was first proposed by Daniel Kahneman and Amos Tversky. There is a growing body of evidence proving that optimism bias represents one of the biggest single causes of risk for megaproject overspend.
Valence effects also have notable real-world implications. In finance, they can lead investors to overestimate a company’s future earnings, potentially inflating stock prices. In organizational contexts, they may foster overly ambitious schedules, contributing to the planning fallacy and increasing the risk of poor decisions or project abandonment.

Studies have shown that it is very difficult to eliminate the optimistic bias. While some scholars argue that mitigating the bias could promote health-protective behaviors, experimental evidence suggests that attempts to reduce it often intensify the effect. In one study testing four interventions of reducing optimism bias, all four measures increased rather than decreased bias. Other research has attempted to reduce optimistic bias by decreasing perceived distance between self and comparison targets, but with limited overall success.
Although eliminating optimistic bias entirely appears challenging, certain conditions can attenuate it. Reducing social distance—such as comparing oneself to close friends rather than strangers—has been shown to narrow or even eliminate differences in perceived likelihood of events. Direct experience with an event also reduces optimistic bias.

Rosenhan, David; Messick, Samuel (1966). "Affect and expectation" (PDF). Journal of Personality and Social Psychology. 3 (1): 38–44. doi:10.1037/h0022633. PMID 5902075. Archived from the original (PDF) on 2016-05-24.
Taylor, Nigel (2000). Making Actuaries Less Human. Staple Inn Actuarial Society, 15. For picking a card see section 6.2 on page 15.

The illusion of control is the tendency for people to overestimate their ability to control events. It was named by U.S. psychologist Ellen Langer and is thought to influence gambling behavior and belief in the paranormal. Along with illusory superiority and optimism bias, the illusion of control is one of the positive illusions.

The illusion of control is the tendency for people to overestimate their ability to control events, for example, when someone feels a sense of control over outcomes that they demonstrably do not influence. 
The illusion might arise because a person lacks direct introspective insight into whether they are in control of events. This has been called the introspection illusion. Instead, they may judge their degree of control by a process which is often unreliable. As a result, they see themselves as responsible for events to which there is little or no causal link. For example, in one study, college students were in a virtual reality setting to treat a fear of heights using an elevator. Those who were told that they had control, yet had none, felt as though they had as much control as those who actually did have control over the elevator. Those who were led to believe they did not have control said they felt as though they had little control.

Psychological theorists have consistently emphasized the importance of perceptions of control over life events. One of the earliest instances was when Alfred Adler argued that people strive for proficiency in their lives. Heider later proposed that humans have a strong motive to control their environment and Wyatt Mann hypothesized a basic competence motive that people satisfy by exerting control. Wiener, an attribution theorist, modified his original theory of achievement motivation to include a controllability dimension. Kelley then argued that people's failure to detect noncontingencies may result in their attributing uncontrollable outcomes to personal causes. Nearer to the present, Taylor and Brown argued that positive illusions, including the illusion of control, foster mental health.
The effect was named by U.S. psychologist Ellen Langer and has been replicated in many different contexts.

The illusion is more common in familiar situations, and in situations where the person knows the desired outcome. Feedback that emphasizes success rather than failure can increase the effect, while feedback that emphasizes failure can decrease or reverse the effect. The illusion is weaker for depressed individuals and is stronger when individuals have an emotional need to control the outcome. The illusion is strengthened by stressful and competitive situations, including financial trading. Although people are likely to overestimate their control when the situations are heavily chance-determined, they also tend to underestimate their control when they actually have it, which runs contrary to some theories of the illusion and its adaptiveness. People also showed a higher illusion of control when they were allowed to become familiar with a task through practice trials, make their choice before the event happens like with throwing dice, and when they can make their choice rather than have it made for them with the same odds.  People are more likely to show control when they have more answers right at the beginning than at the end, even when the people had the same number of correct answers.
Being in a position of power enhances the illusion of control, which may lead to overreach in risk taking.

At times, people attempt to gain control by transferring responsibility to more capable or "luckier" others to act for them.  By forfeiting direct control, it is perceived to be a valid way of maximizing outcomes. This illusion of control by proxy is a significant theoretical extension of the traditional illusion of control model. People will of course give up control if another person is thought to have more knowledge or skill in areas such as medicine where actual skill and knowledge are involved. In cases like these it is entirely rational to give up responsibility to people such as doctors. However, when it comes to events of pure chance, allowing another to make decisions (or gamble) on one's behalf, because they are seen as luckier is not rational and would go against people's well-documented desire for control in uncontrollable situations.  However, it does seem plausible since people generally believe that they can possess luck and employ it to advantage in games of chance, and it is not a far leap that others may also be seen as lucky and able to control uncontrollable events. In a study conducted in Singapore, the perception of control, luck, and skill when gambling led to an increase in gambling behavior.
In one instance, a lottery pool at a company decides who picks the numbers and buys the tickets based on the wins and losses of each member. The member with the best record becomes the representative until they accumulate a certain number of losses and then a new representative is picked based on wins and losses. Even though no member is truly better than the other and it is all by chance, they still would rather have someone with seemingly more luck to have control over them.
In another real-world example, in the 2002 Olympics men's and women's hockey finals, Team Canada beat Team USA. Prior to the match, a Canadian coin was secretly placed under the ice before the game, an action which the players and officials believed would bring them luck. The members of Team Canada were the only people who knew the coin had been placed there. The coin was later put in the Hockey Hall of Fame where there was an opening so people could touch it. People believed they could transfer luck from the coin to themselves by touching it, and thereby change their own luck.

The illusion of control is demonstrated by three converging lines of evidence: 1) laboratory experiments, 2) observed behavior in familiar games of chance such as lotteries, and 3) self-reports of real-world behavior.

One kind of laboratory demonstration involves two lights marked "Score" and "No Score". Subjects have to try to control which one lights up. In one version of this experiment, subjects could press either of two buttons. Another version had one button, which subjects decided on each trial to press or not. Subjects had a variable degree of control over the lights, or none at all, depending on how the buttons were connected. The experimenters made clear that there might be no relation between the subjects' actions and the lights. Subjects estimated how much control they had over the lights. These estimates bore no relation to how much control they actually had, but was related to how often the "Score" light lit up. Even when their choices made no difference at all, subjects confidently reported exerting some control over the lights.

Ellen Langer's research demonstrated that people were more likely to behave as if they could exercise control in a chance situation where "skill cues" were present. By skill cues, Langer meant properties of the situation more normally associated with the exercise of skill, in particular the exercise of choice, competition, familiarity with the stimulus and involvement in decisions. One simple form of this effect is found in casinos: when rolling dice in a craps game people tend to throw harder when they need high numbers and softer for low numbers.
In another experiment, subjects had to predict the outcome of thirty coin tosses. The feedback was rigged so that each subject was right exactly half the time, but the groups differed in where their "hits" occurred. Some were told that their early guesses were accurate. Others were told that their successes were distributed evenly through the thirty trials. Afterwards, they were surveyed about their performance. Subjects with early "hits" overestimated their total successes and had higher expectations of how they would perform on future guessing games. This result resembles the irrational primacy effect in which people give greater weight to information that occurs earlier in a series. Forty percent of the subjects believed their performance on this chance task would improve with practice, and twenty-five percent said that distraction would impair their performance.
Another of Langer's experiments replicated by other researchers involves a lottery. Subjects are either given tickets at random or allowed to choose their own. They can then trade their tickets for others with a higher chance of paying out. Subjects who had chosen their own ticket were more reluctant to part with it. Tickets bearing familiar symbols were less likely to be exchanged than others with unfamiliar symbols. Although these lotteries were random, subjects behaved as though their choice of ticket affected the outcome.  Participants who chose their own numbers were less likely to trade their ticket even for one in a game with better odds.

Yet another way to investigate perceptions of control is to ask people about hypothetical situations, for example their likelihood of being involved in a motor vehicle accident. On average, drivers regard accidents as much less likely in "high-control" situations, such as when they are driving, than in "low-control" situations, such as when they are in the passenger seat. They also rate a high-control accident, such as driving into the car in front, as much less likely than a low-control accident such as being hit from behind by another driver.

Ellen Langer, who first demonstrated the illusion of control, explained her findings in terms of a confusion between skill and chance situations. She proposed that people base their judgments of control on "skill cues". These are features of a situation that are usually associated with games of skill, such as competitiveness, familiarity and individual choice. When more of these skill cues are present, the illusion is stronger.
In 1998, Suzanne Thompson and colleagues argued that Langer's explanation was inadequate to explain all the variations in the effect. As an alternative, they proposed that judgments about control are based on a procedure that they called the "control heuristic". This theory proposes that judgments of control depend on two conditions; an intention to create the outcome, and a relationship between the action and outcome. In games of chance, these two conditions frequently go together. As well as an intention to win, there is an action, such as throwing a die or pulling a lever on a slot machine, which is immediately followed by an outcome. Even though the outcome is selected randomly, the control heuristic would result in the player feeling a degree of control over the outcome.
Self-regulation theory offers another explanation. To the extent that people are driven by internal goals concerned with the exercise of control over their environment, they will seek to reassert control in conditions of chaos, uncertainty or stress. One way of coping with a lack of real control is to falsely attribute oneself control of the situation.
The core self-evaluations (CSE) trait is a stable personality trait composed of locus of control, neuroticism, self-efficacy, and self-esteem. While those with high core self-evaluations are likely to believe that they control their own environment (i.e., internal locus of control), very high levels of CSE may lead to the illusion of control.

In 1988 Taylor and Brown have argued that positive illusions, including the illusion of control, are adaptive as they motivate people to persist at tasks when they might otherwise give up. This position is supported by Albert Bandura's claim in 1989 that "optimistic self-appraisals of capability, that are not unduly disparate from what is possible, can be advantageous, whereas veridical judgements can be self-limiting". His argument is essentially concerned with the adaptive effect of optimistic beliefs about control and performance in circumstances where control is possible, rather than perceived control in circumstances where outcomes do not depend on an individual's behavior.
In 1997 Bandura also suggested that: 
"In activities where the margins of error are narrow and missteps can produce costly or injurious consequences, personal well-being is best served by highly accurate efficacy appraisal."
Taylor and Brown argue that positive illusions are adaptive, since there is evidence that they are more common in normally mentally healthy individuals than in depressed individuals. However, in 1998 Pacini, Muir and Epstein showed that this may be because depressed people overcompensate for a tendency toward maladaptive intuitive processing by exercising excessive rational control in trivial situations, and note that the difference with non-depressed people disappears in more consequential circumstances.
There is also empirical evidence that high self-efficacy can be maladaptive in some circumstances. In a scenario-based study, Whyte et al. showed in 1997 that participants in whom they had induced high self-efficacy were significantly more likely to escalate commitment to a failing course of action. In 1998 Knee and Zuckerman challenged the definition of mental health used by Taylor and Brown and argue that lack of illusions is associated with a non-defensive personality oriented towards growth and learning and with low ego involvement in outcomes. They present evidence that self-determined individuals are less prone to these illusions.
In the late 1970s, Abramson and Alloy demonstrated that depressed individuals held a more accurate view than their non-depressed counterparts in a test which measured illusion of control. This finding held true even when the depression was manipulated experimentally. However, when replicating the findings Msetfi et al. (2005, 2007) found that the overestimation of control in nondepressed people only showed up when the interval was long enough, implying that this is because they take more aspects of a situation into account than their depressed counterparts. Also, Dykman et al. (1989) showed that depressed people believe they have no control in situations where they actually do, so their perception is not more accurate overall. Allan et al. (2007) has proposed that the pessimistic bias of depressives resulted in "depressive realism" when asked about estimation of control, because depressed individuals are more likely to say no even if they have control.
A number of studies have found a link between a sense of control and health, especially in older people. This link for older people having improved health because of a sense of control was discussed in a study conducted in a nursing home. As the residents at the nursing home were encouraged to make more choices for themselves, there was more sense of control over their daily lives. This increase in control increased their overall happiness and health compared to those not making as many decisions for themselves. It was even speculated that with results so promising could slow down or reverse cognitive decline that may occur with aging.
Fenton-O'Creevy et al. argue, as do Gollwittzer and Kinney in 1998, that while illusory beliefs about control may promote goal striving, they are not conducive to sound decision-making. Illusions of control may cause insensitivity to feedback, impede learning and predispose toward greater objective risk taking (since subjective risk will be reduced by illusion of control).

Psychologist Daniel Wegner argues that an illusion of control over external events underlies belief in psychokinesis, a supposed paranormal ability to move objects directly using the mind. As evidence, Wegner cites a series of experiments on magical thinking in which subjects were induced to think they had influenced external events. In one experiment, subjects watched a basketball player taking a series of free throws. When they were instructed to visualise him making his shots, they felt that they had contributed to his success.
A study published in 2003 examined traders working in the City of London's investment banks. They each watched a graph being plotted on a computer screen, similar to a real-time graph of a stock price or index. Using three computer keys, they had to raise the value as high as possible. They were warned that the value showed random variations, but that the keys might have some effect. In fact, the fluctuations were not affected by the keys. The traders' ratings of their success measured their susceptibility to the illusion of control. This score was then compared with each trader's performance. Those who were more prone to the illusion scored significantly lower on analysis, risk management and contribution to profits. They also earned significantly less.

The negativity bias, also known as the negativity effect, is a cognitive bias that, even when positive or neutral things of equal intensity occur, things of a more negative nature (e.g., unpleasant thoughts, emotions, social interactions, harmful/traumatic events) have a greater effect on a person's psychological state and processes than neutral or positive things. In other words, something very positive will generally have less of an impact on one's behavior and cognition than something equally emotional but negative. The negativity bias has been investigated within many different domains, including the formation of impressions and general evaluations; attention, learning, and memory; and decision-making and risk considerations.

Paul Rozin and Edward Royzman proposed four elements of the negativity bias in order to explain its manifestation: negative potency, steeper negative gradients, negativity dominance, and negative differentiation.
Negative potency refers to the notion that, while possibly of equal magnitude or emotionality, negative and positive items/events/etc. are not equally salient.  Rozin and Royzman note that this characteristic of the negativity bias is only empirically demonstrable in situations with inherent measurability, such as comparing how positively or negatively a change in temperature is interpreted.
With respect to positive and negative gradients, it seems that negative events are thought to be perceived as increasingly more negative than positive events are increasingly positive the closer one gets (spatially or temporally) to the affective event itself.  In other words, there is a steeper negative gradient than positive gradient.  For example, the negative experience of an impending dental surgery is perceived as increasingly more negative the closer one gets to the date of surgery than the positive experience of an impending party is perceived as increasingly more positive the closer one gets to the date of celebration  (assuming for the sake of this example that these events are equally positive and negative).  Rozin and Royzman argue that this characteristic is distinct from that of negative potency because there appears to be evidence of steeper negative slopes relative to positive slopes even when potency itself is low.
Negativity dominance describes the tendency for the combination of positive and negative items/events/etc. to skew towards an overall more negative interpretation than would be suggested by the summation of the individual positive and negative components.  Phrasing in more Gestalt-friendly terms, the whole is more negative than the sum of its parts.
Negative differentiation is consistent with evidence suggesting that the conceptualization of negativity is more elaborate and complex than that of positivity.  For instance, research indicates that negative vocabulary is more richly descriptive of the affective experience than that of positive vocabulary.  Furthermore, there appear to be more terms employed to indicate negative emotions than positive emotions. The notion of negative differentiation is consistent with the mobilization-minimization hypothesis, which posits that negative events, as a consequence of this complexity, require a greater mobilization of cognitive resources to deal with the affective experience and a greater effort to minimize the consequences.

Most of the early evidence suggesting a negativity bias stems from research on social judgments and impression formation, in which it became clear that negative information was typically more heavily weighted when participants were tasked with forming comprehensive evaluations and impressions of other target individuals.  Generally speaking, when people are presented with a range of trait information about a target individual, the traits are neither "averaged" nor "summed" to reach a final impression. When these traits differ in terms of their positivity and negativity, negative traits disproportionately impact the final impression.  This is specifically in line with the notion of negativity dominance (see "Explanations" above).
As an example, a famous study by Leon Festinger and colleagues investigated critical factors in predicting friendship formation; the researchers concluded that whether or not people became friends was most strongly predicted by their proximity to one another.  Ebbesen, Kjos, and Konecni, however, demonstrated that proximity itself does not predict friendship formation; rather, proximity serves to amplify the information that is relevant to the decision of either forming or not forming a friendship.  Negative information is just as amplified as positive information by proximity.  As negative information tends to outweigh positive information, proximity may predict a failure to form friendships even more so than successful friendship formation.
One explanation that has been put forth as to why such a negativity bias is demonstrated in social judgments is that people may generally consider negative information to be more diagnostic of an individual's character than positive information, that it is more useful than positive information in forming an overall impression.  This is supported by indications of higher confidence in the accuracy of one's formed impression when it was formed more on the basis of negative traits than positive traits.  People consider negative information to be more important to impression formation and, when it is available to them, they are subsequently more confident.
An oft-cited paradox, a dishonest person can sometimes act honestly while still being considered to be predominantly dishonest; on the other hand, an honest person who sometimes does dishonest things will likely be reclassified as a dishonest person.  It is expected that a dishonest person will occasionally be honest, but this honesty will not counteract the prior demonstrations of dishonesty. Honesty is considered more easily tarnished by acts of dishonesty. Honesty itself would then be not diagnostic of an honest nature, only the absence of dishonesty.
The presumption that negative information has greater diagnostic accuracy is also evident in voting patterns.  Voting behaviors have been shown to be more affected or motivated by negative information than positive: people tend to be more motivated to vote against a candidate because of negative information than they are to vote for a candidate because of positive information.  As noted by researcher Jill Klein, "character weaknesses were more important than strengths in determining...the ultimate vote".
This diagnostic preference for negative traits over positive traits is thought to be a consequence of behavioral expectations: there is a general expectation that, owing to social requirements and regulations, people will generally behave positively and exhibit positive traits. Contrastingly, negative behaviors/traits are more unexpected and, thus, more salient when they are exhibited. The relatively greater salience of negative events or information means they ultimately play a greater role in the judgment process.

Studies reported in a paper in the Journal of Experimental Psychology: General by Carey Morewedge (2009) found that people exhibit a negativity bias in attribution of external agency, such that they are more likely to attribute negative outcomes to the intentions of another person than similar neutral and positive outcomes. In laboratory experiments, Morewedge found that participants were more likely to believe that a partner had influenced the outcome of a gamble in when the participants lost money than won money, even when the probability of winning and losing money was held even. This bias is not limited to adults. Children also appear to be more likely to attribute negative events to intentional causes than similarly positive events.

As addressed by negative differentiation, negative information seems to require greater information processing resources and activity than does positive information; people tend to think and reason more about negative events than positive events. Neurological differences also point to greater processing of negative information: participants exhibit greater event-related potentials when reading about, or viewing photographs of, people performing negative acts that were incongruent with their traits than when reading about incongruent positive acts. This additional processing leads to differences between positive and negative information in attention, learning, and memory.

A number of studies have suggested that negativity is essentially an attention magnet. For example, when tasked with forming an impression of presented target individuals, participants spent longer looking at negative photographs than they did looking at positive photographs. Similarly, participants registered more eye blinks when studying negative words than positive words (blinking rate has been positively linked to cognitive activity). Also, people were found to show greater orienting responses following negative than positive outcomes, including larger increases in pupil diameter, heart rate, and peripheral arterial tone 
Importantly, this preferential attendance to negative information is evident even when the affective nature of the stimuli is irrelevant to the task itself. The automatic vigilance hypothesis has been investigated using a modified Stroop task.  Participants were presented with a series of positive and negative personality traits in several different colors; as each trait appeared on the screen, participants were to name the color as quickly as possible.  Even though the positive and negative elements of the words were immaterial to the color-naming task, participants were slower to name the color of negative traits than they were positive traits.  This difference in response latencies indicates that greater attention was devoted to processing the trait itself when it was negative.
Aside from studies of eye blinks and color naming, Baumeister and colleagues noted in their review of bad events versus good events  that there is also easily accessible, real-world evidence for this attentional bias: bad news sells more papers and the bulk of successful novels are full of negative events and turmoil. When taken in conjunction with the laboratory-based experiments, there is strong support for the notion that negative information generally has a stronger pull on attention than does positive information.

Learning and memory are direct consequences of attentional processing: the more attention is directed or devoted toward something, the more likely it is that it will be later learned and remembered.  Research concerning the effects of punishment and reward on learning suggests that punishment for incorrect responses is more effective in enhancing learning than are rewards for correct responses—learning occurs more quickly following bad events than good events.
Drs. Pratto and John addressed the effects of affective information on incidental memory as well as attention using their modified Stroop paradigm (see section concerning "Attention").  Not only were participants slower to name the colors of negative traits, they also exhibited better incidental memory for the presented negative traits than they did for the positive traits, regardless of the proportion of negative to positive traits in the stimuli set.
Intentional memory is also impacted by the stimuli's negative or positive quality.  When studying both positive and negative behaviors, participants tend to recall more negative behaviors during a later memory test than they do positive behaviors, even after controlling for serial position effects. There is also evidence that people exhibit better recognition memory and source memory for negative information.
When asked to recall a recent emotional event, people tend to report negative events more often than they report positive events, and this is thought to be because these negative memories are more salient than are the positive memories.  People also tend to underestimate how frequently they experience positive affect, in that they more often forget the positively emotional experiences than they forget negatively emotional experiences.

Studies of the negativity bias have also been related to research within the domain of decision-making, specifically as it relates to risk aversion or loss aversion.  When presented with a situation in which a person stands to either gain something or lose something depending on the outcome, potential costs were argued to be more heavily considered than potential gains. The greater consideration of losses (i.e. negative outcomes) is in line with the principle of negative potency as proposed by Rozin and Royzman. This issue of negativity and loss aversion as it relates to decision-making is most notably addressed by Drs. Daniel Kahneman's and Amos Tversky's prospect theory.
However, it is worth noting that Rozin and Royzman were never able to find loss aversion in decision making. They wrote, "in particular, strict gain and loss of money does not reliably demonstrate loss aversion". This is consistent with the findings of a recent review of more than 40 studies of loss aversion focusing on decision problems with equal sized gains and losses. In their review, Yechiam and Hochman (2013) did find a positive effect of losses on performance, autonomic arousal, and response time in decision tasks, which they suggested is due to the effect of losses on attention. This was labeled by them as loss attention.

Research points to a correlation between political affiliation and negativity bias, where conservatives are more sensitive to negative stimuli and therefore tend to lean towards right-leaning ideology which considers threat reduction and social-order to be its main focus.
Individuals with lower negativity bias tend to lean towards liberal political policies such as pluralism and are accepting of diverse social groups which by proxy could threaten social structure and cause greater risk of unrest.

While the majority of investigations into the negativity bias have primarily focused on adults, particularly undergraduate students, a limited number of studies involving infants have also indicated the presence of negativity biases.
Infants are thought to interpret ambiguous situations on the basis of how others around them react. When an adult (e.g. experimenter, mother) displays reactions of happiness, fear, or neutrality towards target toys, infants tend to approach the toy associated with the negative reaction significantly less than the neutral and positive toys. Furthermore, there was greater evidence of neural activity when the infants were shown pictures of the "negative" toy than when shown the "positive" and "neutral" toys.  Although recent work with 3-month-olds suggests a negativity bias in social evaluations, as well, there is also work suggesting a potential positivity bias in attention to emotional expressions in infants younger than 7 months.  A review of the literature conducted by Drs. Amrisha Vaish, Tobias Grossman, and Amanda Woodward suggests the negativity bias may emerge during the second half of an infant's first year, although the authors also note that research on the negativity bias and affective information has been woefully neglected within the developmental literature.

Some research indicates that older adults may display, at least in certain situations, a positivity bias or positivity effect.  Proposed by Dr. Laura Carstensen and colleagues, the socioemotional selectivity theory outlines a shift in goals and emotion regulation tendencies with advancing age, resulting in a preference for positive information over negative information.  Aside from the evidence in favor of a positivity bias, though, there have still been many documented cases of older adults displaying a negativity bias.

Choice-supportive bias or post-purchase rationalization is the tendency to retroactively ascribe positive attributes to an option one has selected and/or to demote the forgone options. It is part of cognitive science, and is a distinct cognitive bias that occurs once a decision is made. For example, if a person chooses option A instead of option B, they are likely to ignore or downplay the faults of option A while amplifying or ascribing new negative faults to option B. Conversely, they are also likely to notice and amplify the advantages of option A and not notice or de-emphasize those of option B.
What is remembered about a decision can be as important as the decision itself, especially in determining how much regret or satisfaction one experiences. Research indicates that the process of making and remembering choices yields memories that tend to be distorted in predictable ways.
In cognitive science, one predictable way that memories of choice options are distorted is that positive aspects tend to be remembered as part of the chosen option, whether or not they originally were part of that option, and negative aspects tend to be remembered as part of rejected options. Once an action has been taken, the ways in which we evaluate the effectiveness of what we did may be biased. It is believed this may influence our future decision-making. These biases may be stored as memories, which are attributions that we make about our mental experiences based on their subjective qualities, our prior knowledge and beliefs, our motives and goals, and the social context. True and false memories arise by the same mechanism because when the brain processes and stores information, it cannot tell the difference where they came from.

The tendency to remember one's choices as better than they actually were. In this respect, people tend to over attribute positive features to options they chose and negative features to options not chosen.

Experiments in cognitive science and social psychology have revealed a wide variety of biases in areas such as statistical reasoning, social attribution, and memory.
Choice-supportive memory distortion is thought to occur during the time of memory retrieval and was the result of the belief that, "I chose this option, therefore it must have been the better option."  Essentially,  after a choice is made people tend to adjust their attitudes to be consistent with, the decision they have already made. It is also possible that choice-supportive memories arise because an individual is only paying attention to certain pieces of information when making a decision or to post-choice cognitive dissonance. In addition, biases can also arise because they are closely related to the high level cognitive operations and complex social interactions.
Memory distortions may sometimes serve a purpose because it may be in our interest to not remember some details of an event or to forget others altogether.

The objective of a choice is generally to pick the best option. Thus, after making a choice, a person is likely to maintain the belief that the chosen option was better than the options rejected. Every choice has an upside and a downside. The process of making a decision mostly relies upon previous experiences. Therefore, a person will remember not only the decision made but also the reasoning behind making that decision.

Motivation may also play a role in this process because when a person remembers the option that they chose as being the best option, it should help reduce regret about their choice. This may represent a positive illusion that promotes well-being.

There are cases where an individual is not always in control of which options are received. People often end up with options that were not chosen but, instead were assigned by others, such as job assignments made by bosses, course instructors assigned by a registrar, or vacation spots selected by other family members. However, being assigned (random or not) to an option leads to a different set of cognitions and memory attributions that tend to favor the alternative (non-received) option and may emphasize regret and disappointment.
Assigned options: Making a choice or having a choice made for you by other people in your best interest can prompt memory attributions that support that choice.  Current experiments show no choice-supportive memory bias for assigned options. However, choices which are made on a person's behalf in their best interest do show a tendency for choice-supportive memory bias.
Random selection: People do not show choice-supportive biases when choices are made randomly for them.  This is because choice-supportive memory bias tends to arise during the act of making the decision.

Misattribution is a well-known commission error supported by researchers. It results in a type of choice-supportive bias when information is attributed to the wrong source. Consequently, the positive attributes of the forgone option are remembered as the positive attributes of the chosen option, or the negative attributes of the chosen option are remembered as the negative attributes of the foregone option. For example, if one had to choose between two pairs of trainers and the chosen pair fitted slightly tighter and the forgone option fitted perfectly, the chosen pair would be remembered as fitting perfectly whereas the forgone pair would be remembered as being slightly tighter (although this was not the case in reality)
While misattribution presupposes correct encoding and recall of the information in relation to a person's decision, the source of the information remains unclear or incorrect. Therefore it is not to be mistaken with completely false information.

Fact distortion results in a type of choice-supportive bias when the facts belonging to the chosen option are remembered in a distorted manner. The distortion refers to the objective values and/or features of the chosen option being misremembered in a more preferential way to their actual values. Alternatively, the forgone option can be misremembered as being significantly less preferential then their actual values. An example of fact distortion would be if you have to choose between buying one out of two cars which can both drive at a maximum speed of 130 mph, the foregone car would be remembered with a maximum speed of 100 mph, whereas the chosen car will be remembered with a maximum speed of 160 mph. Consequently, the facts concerning both cars have been distorted.
There are varieties of fact distortion that remain contested throughout the literature; as a result, there is no consensus on how it works. Some authors argue that the distortion mainly occurs when the favoured option is misremembered more preferentially, whereas other researchers argue that memory distortion does not occur or is only likely to take place during the post-decision stage. Overall, some studies have argued that holding the belief that distortion cannot take place (as soon as the decision is made) means that facts cannot be distorted.

Selective forgetting results in a form of choice-supportive bias when information is selectively forgotten. In this respect, the positive attributes of the chosen option and the negative attributes of the forgone option are retained in memory at a higher rate and the alternatives are displaced from memory at a faster rate. An example of selective forgetting would be correctly remembering that your chosen pair of trainers were aesthetically pleasing, but forgetting that they were slightly tight. This omission error may be related to the limited capacity of memory over time, otherwise known as transience.

False memory in the context of choice-supportive biases is when items that were not part of the original decision are remembered as being presented. If these entirely new items are positive, they will be remembered as belonging to the chosen option and if they are negative, they will be remembered as belonging to the forgone option. For example, a chosen pair of shoes might be remembered as good for running, although there was no information presented in respect to the shoes running capabilities.
This type of error is fundamentally different to the other types of misremembering in choice-supportive bias because it is not due to correct encoding and later confusion, but it is due to a completely false memory. This type of bias means that falsely remembered events can affect future attitudes and possibly decision-making.

Alignability in the context of choice-supportive bias refers to whether both options have directly comparable features. For example, an alignable characteristic could be whether both options can be measured in centimetres. In the context of decision making, alignability can influence choices or the ability to make a similarity judgement of a potential option. The alignment process enables a person to draw similarities and difference which impact their choice-supportive biases. Research to support this can be displayed by the following example: when given a choice between two brands of popcorn, participants were more likely to choose the one with the superior alignable differences, such as "pops in its own bag" compared with "requires a microwaveable bowl" than the one with superior non-alignable differences, such as "not likely to burn" compared with those containing "some citric acid"

The extent of the delay between encoding is a potential factor that could affect choice-supportive bias. If there is a larger delay between encoding (i.e. viewing the information about the options) and retrieval (i.e. memory tests) it is likely to result in more biased choices rather than the impact of the actual choice on choice-supportive bias. Some studies have found that the extent of false memories increases over time. Whereas other researchers have shown that a 2-day delay between making choices and assessment of memory resulted in reasonably high (86%) recognition accuracy. Therefore, these findings indicate that the influence of delays on choice-supportive bias remain varied, and the influence of delays could affect different types of memory distortions differently.

This factor refers to a person's perceived decisions concerning the choices they made, more specifically this includes memories that have been falsified to reflect a selected choice that the person did not actually make. Research illustrates that people favour the options they think they have chosen and remember the attributes of their "chosen choice" more vividly and favourably. Essentially, this influences assesses how one believes the choices they made affects their subsequent memories. As a result, people's memories are biased in favour of the option they thought they had selected rather than their actual choices.

Individual differences in choice-supportive bias affect the way a person remembers their options and the way they make decisions and therefore it may influence the degree to which a person engages in choice-supportive bias. Factors such as age and individual characteristics can influence an individual's cognitive abilities, personality and thus their overall choice-supportive biases. For example, it has been observed by correlations that people with better performance in tests of frontal or executive functioning were less prone to choice-supportive memory.

People's conception of who they are, can be shaped by the memories of the choices they make; the college favored over the one renounced, the job chosen over the one rejected, the candidate elected instead of another one not selected. Memories of chosen as well as forgone alternatives can affect one's sense of well-being. Regret for options not taken can cast a shadow, whereas satisfaction at having made the right choice can make a good outcome seem even better.

Choice-supportive bias often results in memories that depict the self in an overly favorable light. In general, cognitive biases loosen our grasp on reality because the line between reality and fantasy can become fuzzy if one's brain has failed to remember a particular event.
Positive illusions are generally mild and are important contributors to our sense of well being. However, we all need to be aware that they do exist as part of human nature.

Human beings are blessed with having an intelligent and complex mind, which allows us to remember our past, be able to optimize the present, and plan for the future. Remembering involves a complex interaction between the current environment, what one expects to remember, and what is retained from the past. The mechanisms of the brain that allow memory storage and retrieval serves us well most of the time, but occasionally gets us into trouble.

There is now abundant evidence that memory content can undergo systematic changes. After some period of time and if the memory is not used often, it may become forgotten.
Memory retention: It is recognized that retention is best for experiences that are pleasant, intermediate for experiences that are unpleasant, and worst for experiences that are neutral. Generic memories provide the basis for inferences that can bring about distortions. These distortions in memory do not displace an individual's specific memories, but they supplement and fill in the gaps when the memories are lost. It has been shown that a wide variety of strategic and systematic processes are used to activate different areas of the brain in order to retrieve information.
Credibility of a memory: People have a way to self-check memories, in which a person may consider the plausibility of the retrieved memory by asking themselves is this event even possible. For example, if a person remembers seeing a pig fly, they must conclude that it was from a dream because pigs cannot fly in the real world. Memory does not provide people with perfect reproductions of what happened, it only consists of constructions and reconstructions of what happened.

There is extensive evidence that the amygdala is involved in effectively influencing memory.  Emotional arousal, usually fear based, activates the amygdala and results in the modulation of memory storage occurring in other brain regions.
The forebrain is one of the targets of the amygdala. The forebrain receives input from amygdala and calculates the emotional significance of the stimulus, generates an emotional response, and transmits it to cerebral cortex. This can alter the way neurons respond to future input, and therefore cognitive biases, such as choice-supportive bias can influence future decisions.

Effects of stress-related hormones, such as epinephrine and glucocorticoids are mediated by influences involving the amygdala. It has been shown in experiments with rats that when they are given systemic injections of epinephrine while being trained to perform a task, they show an enhanced memory of performing the task. In effect the stronger the emotion that is tied to the memory, the more likely the individual is to remember. Therefore, if the memory is stored and retrieved properly it is less likely to be distorted.

A PET scan or fMRI can be used to identify different regions of the brain that are activated during specific memory retrieval.

True versus false memories: One study asked subjects to remember a series of events while being monitored by an fMRI to see which areas "light up". When an individual remembered a greater number of true memories than false memories, it showed a cluster spanning the right superior temporal gyrus and lateral occipital cortex. However, when the reverse occurred (when an individual remembered a greater number of false memories than true) the brain area that showed activation was the left insula.  These findings may provide some insight as to which areas of the brain are involved with storing memories and later retrieving them.

Studies now show that as people age, their process of memory retrieval changes. Although general memory problems are common to everyone because no memory is perfectly accurate, older adults are more likely than younger adults to show choice-supportive biases.

Normal aging may be accompanied by neuropathy in the frontal brain regions. Frontal regions help people encode or use specific memorial attributes to make source judgments, controls personality and the ability to plan for events. These areas can attribute to memory distortions and regulating emotion.

In general, older adults are more likely to remember emotional aspects of situations than are younger adults. For example, on a memory characteristic questionnaire, older adults rated remembered events as having more associated thoughts and feelings than did younger adults. As a person ages, regulating personal emotion becomes a higher priority, whereas knowledge acquisition becomes less of a powerful motive. Therefore, choice-supportive bias would arise because their focus was on how they felt about the choice rather than on the factual details of the options. Studies have shown that when younger adults are encouraged to remember the emotional aspect of a choice, they are more likely to show choice-supportive bias.  This may be related to older adults' greater tendency to show a positivity effect in memory.

Older adults rely more than younger adults on categorical or general knowledge about an event to recognize particular elements from the event. Older adults are also less likely to correctly remember contextual features of events, such as their color or location. This may be because older adults remember (or rely on) fewer source identifying characteristics than the young. Consequently, older adults must more often guess or base a response on less specific information, such as familiarity. As a result, if they can't remember something, they are more likely to fill in the missing gaps with things that are familiar to them.

Older adults are more reliant on gist-based retrieval. A number of studies suggest that using stereotypes or general knowledge to help remember an event is less cognitively demanding than relying on other types of memorial information and thus might require less reflective activity. This shift towards gist-based processes might occur as a compensation for age decrements in verbatim memory.

The episodic memory and inhibition accounts of age-related increases in false memories.  Inhibition of a memory may be related to an individual's hearing capacity and attention span. If the person cannot hear what is going on around them or is not paying much attention, the memory cannot be properly stored and therefore cannot be accurately retrieved.

Henkel and Mather tested the role of beliefs at the time of retrieval about which option was chosen by giving participants several hypothetical choices like deciding between two used cars.
After making several choices, participants left and were asked to return a week later. At that point, Henkel and Mather reminded them which option they had chosen for each choice and gave them a list of the features of the two options; some new positive and negative features were mixed in with the old features. Next, participants were asked to indicate whether each option was new, had been associated with the option they chose, or had been associated with the option they rejected. Participants favored whichever option Henkel and Mather had told them they had chosen in their memories. These findings show that beliefs at the time of retrieval about which option was chosen shape both which features are attributed to the options and how vividly they are remembered.

One study looked at the accuracy and distortion in memory for high school grades. The relation between accuracy and distortion of autobiographical memory content was examined by verifying 3,220 high school grades recalled by 99 freshman college students.  It was shown that most errors inflated the actual high school grade, meaning that these distortions are attributed to memory reconstructions in a positive and emotionally gratifying direction. In addition, their findings indicate that the process of distortion does not cause the actual unpleasant memory loss of getting the bad grade. This is because there was no correlation found between the percentage of accuracy recall and the degree of asymmetry, or distortion. This shows that the distortion in memories of high school grades arises after the content has been forgotten by another mechanism.

Many similar studies have been performed, such as a fifty-year study of remembering college grades. In this study one to 54 years after graduating, 276 alumni correctly recalled 3,025 of 3,967 college grades. The number of omission errors increased with the retention interval and better students made fewer errors. The accuracy of recall increased with confidence in recall. Eighty-one percent of errors of commission inflated the actual grade. These data suggested that distortions occur soon after graduation, remain constant during the retention interval, and are greater for better students and for courses students enjoyed most. Therefore, sometime in between when the memory is stored and when it is retrieved some time later, the distortion may arise.

Researchers have used written scenarios in which participants are asked to make a choice between two options. Later, on a memory test, participants are given a list of positive and negative features, some of which were in the scenario and some of which are new. A choice-supportive bias is seen when both correct and incorrect attributions tend to favor the chosen option, with positive features more likely to be attributed to the chosen option and negative features to the rejected option.
Deception: Henkel and Mather (2007) found that giving people false reminders about which option they chose in a previous experiment session led people to remember the option they were told they had chosen as being better than the other option. This reveals that choice-supportive biases arise in large part when remembering past choices, rather than being the result of biased processing at the time of the choice.

The Deese–Roediger–McDermott paradigm (DRM) consists of a participant listening to an experimenter read lists of thematically related words (e.g. table, couch, lamp, desk); then after some period of time the experimenter will ask if a word was presented in the list.  Participants often report that related but non-presented words (e.g. chair) were included in the encoding series, essentially suggesting that they 'heard' the experimenter say these non-presented words (or critical lures). Incorrect 'yes' responses to critical lures, often referred to as false memories, are remarkably high under standard DRM conditions.

The theory of cognitive dissonance proposes that people have a motivational drive to reduce dissonance. Choice-supportive bias is potentially related to the aspect of cognitive dissonance explored by Jack Brehm (1956) as postdecisional dissonance. Within the context of cognitive dissonance, choice-supportive bias would be seen as reducing the conflict between "I prefer X" and "I have committed to Y".

A study of the Lady Macbeth effect showed reduced choice-supportive bias by having participants engage in washing. However, the underlying general effect has not replicated in larger studies

A conjunction effect or Linda problem is a bias or mistake in reasoning where adding extra details (an "and" statement or logical conjunction; mathematical shorthand: 
) to a sentence makes it appear more likely. Logically, this is not possible, because adding more claims can make a true statement false, but cannot make false statements true: If A is true, then 
 might be false (if B is false). However, if A is false, then 
 will always be false, regardless of what B is. Therefore, 
 cannot be more likely than A.

The most often-cited example of this fallacy originated with Amos Tversky and Daniel Kahneman:
Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.
Which is more probable?
Linda is a bank teller.
Linda is a bank teller and is active in the feminist movement.
The majority of those asked chose option 2. However, this is logically impossible: if Linda is a bank teller active in the feminist movement, then she is a bank teller. Therefore, it is impossible for 2 to be true while 1 is false, so the probabilities are at most equal.
More generally, the probability of two events occurring together (that is, in conjunction) is always less than or equal to the probability of either one occurring itself. For two events A and B this inequality can be written as 
Tversky and Kahneman argue that most people get this problem wrong because they use a heuristic (an easily calculated) procedure called representativeness to make this kind of judgment: Option 2 seems more "representative" of Linda from the description of her, even though it is clearly mathematically less likely.
In other demonstrations, they argued that a specific scenario seemed more likely because of representativeness, but each added detail would actually make the scenario less and less likely. In this way it could be similar to the misleading vividness fallacy. More recently, Kahneman has argued that the conjunction fallacy is a type of extension neglect.

In some experimental demonstrations, the conjoint option is evaluated separately from its basic option. In other words, one group of participants is asked to rank-order the likelihood that Linda is a bank teller, a high school teacher, and several other options, and another group is asked to rank-order whether Linda is a bank teller and active in the feminist movement versus the same set of options (without "Linda is a bank teller" as an option). In this type of demonstration, different groups of subjects still rank-order Linda as a bank teller and active in the feminist movement more highly than Linda as a bank teller.
Separate evaluation experiments preceded the earliest joint evaluation experiments, and Kahneman and Tversky were surprised when the effect was observed even under joint evaluation.

While the Linda problem is the best-known example, researchers have developed dozens of problems that reliably elicit the conjunction fallacy.

Policy experts were asked to rate the probability that the Soviet Union would invade Poland, and the United States would break off diplomatic relations, all in the following year. They rated it on average as having a 4% probability of occurring. Another group of experts was asked to rate the probability simply that the United States would break off relations with the Soviet Union in the following year. They gave it an average probability of only 1%.
In an experiment conducted in 1980, respondents were asked the following:
Suppose Björn Borg reaches the Wimbledon finals in 1981. Please rank order the following outcomes from most to least likely.
Borg will win the match
Borg will lose the first set
Borg will lose the first set but win the match
Borg will win the first set but lose the match
On average, participants rated "Borg will lose the first set but win the match" more likely than "Borg will lose the first set". However, winning the match is only one of several potential eventual outcomes after having lost the first set. The first and the second outcome are thus more likely (as they only contain one condition) than the third and fourth outcome (which depend on two conditions).

Tversky and Kahneman followed up their original findings with a 1983 paper that looked at dozens of new problems, most of these with multiple variations. The following are a couple of examples.
Consider a regular six-sided die with four green faces and two red faces. The die will be rolled 20 times and the sequence of greens (G) and reds (R) will be recorded. You are asked to select one sequence, from a set of three, and you will win $25 if the sequence you choose appears on successive rolls of the die.
RGRRR
GRGRRR
GRRRRR
65% of participants chose the second sequence, though option 1 is contained within it and is shorter than the other options. In a version where the $25 bet was only hypothetical the results did not significantly differ. Tversky and Kahneman argued that sequence 2 appears "representative" of a chance sequence (compare to the clustering illusion).
A health survey was conducted in a representative sample of adult males in British Columbia of all ages and occupations.
Mr. F. was included in the sample. He was selected by chance from the list of participants.
Which of the following statements is more probable? (check one)
Mr. F. has had one or more heart attacks.
Mr. F. has had one or more heart attacks and he is over 55 years old.
The probability of the conjunctions is never greater than that of its conjuncts. Therefore, the first choice is more probable.

Critics such as Gerd Gigerenzer and Ralph Hertwig criticized the Linda problem on grounds such as the wording and framing. The question of the Linda problem may violate conversational maxims in that people assume that the question obeys the maxim of relevance. Gigerenzer argues that some of the terminology used have polysemous meanings, the alternatives of which he claimed were more "natural". He argues that one meaning of probable ("what happens frequently") corresponds to the mathematical probability people are supposed to be tested on, but other meanings ("what is plausible" and "whether there is evidence") do not. The term "and" has even been argued to have relevant polysemous meanings. Many techniques have been developed to control for this possible misinterpretation, but none of them has dissipated the effect.
Many variations in wording of the Linda problem were studied by Tversky and Kahneman. If the first option is changed to obey conversational relevance, i.e., "Linda is a bank teller whether or not she is active in the feminist movement" the effect is decreased, but the majority (57%) of the respondents still commit the conjunction error.  If the probability is changed to frequency format (see debiasing section below) the effect is reduced or eliminated. However, studies exist in which indistinguishable conjunction fallacy rates have been observed with stimuli framed in terms of probabilities versus frequencies.
The wording criticisms may be less applicable to the conjunction effect in separate evaluation. The "Linda problem" has been studied and criticized more than other types of demonstration of the effect (some described below).
In an incentivized experimental study, it has been shown that the conjunction fallacy decreased in those with greater cognitive ability, though it did not disappear. It has also been shown that the conjunction fallacy becomes less prevalent when subjects are allowed to consult with other subjects.
Still, the conjunction fallacy occurs even when people are asked to make bets with real money, and when they solve intuitive physics problems of various designs.

Drawing attention to set relationships, using frequencies instead of probabilities, or thinking diagrammatically are all methods that sharply reduce the error in some forms of the conjunction fallacy.
In one experiment the question of the Linda problem was reformulated as follows:
There are 100 persons who fit the description above (that is, Linda's). How many of them are:
Whereas previously 85% of participants gave the wrong answer (bank teller and active in the feminist movement), in experiments done with this questioning the proportion of incorrect answers is dramatically reduced (to ~20%). Participants were forced to use a mathematical approach and thus recognized the difference more easily.
However, in some tasks only based on frequencies, not on stories, that used clear logical formulations, conjunction fallacies continued to occur dominantly, with only few exceptions, when the observed pattern of frequencies resembled a conjunction.

In Episode 3 of Season 13 of Criminal Minds, SSA Dr. Spencer Reid exposes SSA Luke Alvez and SA Penelope Garcia to the Linda problem, saying that he is planning to discuss it in a seminar addressed to FBI agents.

Authority bias is the tendency to attribute greater accuracy to the opinion of an authority figure (unrelated to its content) and be more influenced by that opinion. An individual is more influenced by the opinion of this authority figure, believing their views to be more credible, and hence place greater emphasis on the authority figure's viewpoint and are more likely to obey them. This concept is considered one of the social cognitive biases or collective cognitive biases.
Humans generally have a deep-seated duty to authority and tend to comply when requested by an authority figure. Some scholars explain that individuals are motivated to view authority as deserving of their position and this legitimacy leads people to accept and obey the decisions that it makes. System justification theory articulates this phenomenon, particularly within its position that there is a psychological motivation for believing in the steadiness, stability and justness of the current social system.
Authority bias can be measured concerning respect for authority, where higher respect for authority positively correlates with the increased likelihood of exhibiting authority bias.

Authority bias, a term popularised by American psychologist Stanley Milgram, is defined as having an unreasonably high confidence in the belief that the information verified by a person with formal authority is correct, and therefore an individual is likely to be more influenced by them. Individuals in positions of authority are seen to be treated more favourably, where people believe their views with increased certainty, though the role of authority alone is not always significant enough to directly affect decision-making without this phenomenon being used in conjunction with other heuristics and biases.
The antonym of authority bias is blanket opposition to authority, disregarding their knowledge and believing authority figures to have inherently false claims. This relates to the view of anti-authoritarianism.
Authority bias has many explanations, rooted in the human need to obey authority figures. Namely, authority bias can be explained through evolutionary and social means.

Authority bias is greater when the authority figure in question is seen to be legitimate, that is, when they are accepted to be in an authorized position of relative power and have the right to demand obedience. Research has highlighted certain important characteristics that can mark one as a legitimate authority figure, strengthening authority bias, with individuals more likely to be influenced by and obey such figures. These markers can affect whether people regard an individual as an authority figure.

All societies are structured in a hierarchical manner, and often those who possess a higher social status are situated further up the hierarchy. The majority of people accept that, in order to maintain a co-operative, functioning society and avoid social chaos, a certain degree of individual choice must be relinquished to authority figures, often representing government institutions, whose role means they have greater status in the social hierarchy. For example, most people accept that, in the criminal justice system, the police have the right to exercise power over others and apprehend wrongdoers, as their role indicates their position in the social hierarchy and their authority. This is often instilled in children from a young age, with parents instructing them to defer to authority figures such as teachers and policemen and accept the opinions of authority figures as correct. There is also a tendency to attribute greater accuracy to and respect the opinions of experts such as doctors. For example, most people have complete confidence in their doctor treating them, as a trusted authority figure with a high social status. This normative behavior can be a useful shortcut, but blind acceptance of expert advice can raise issues if it becomes an automatic response. This was demonstrated by a study conducted by the psychiatrist Charles Hofling, who found that 21 out of 22 nurses would have given patients a potentially fatal dose of a drug when asked to by a doctor despite this instruction contravening official guidance forbidding the administration of the drug. Similar studies in other areas of society have demonstrated the problems that can arise as a result of unquestioningly accepting the opinions of those regarded as having higher status in the social hierarchy.

Outward appearance can signify an individual's social status and relative position in the social hierarchy, and consequently whether they are perceived as a legitimate authority figure. When examining authority bias, outward appearance, manifested in clothing, e.g., in a particular uniform, seems to have a profound effect on whether an individual is respected and obeyed as an authority figure. Research conducted by Bickman and colleagues found that passers-by were twice as likely to obey a confederate dressed as a security guard rather than a milkman when asked by the individual to complete tasks such as picking up litter or lending the confederate a coin for the parking meter. The effects of uniform on authority bias has been a fairly consistent and reliable finding, as other studies, such as those run by Milgram, have reached similar conclusions. In a variation of his original experiment, Milgram found that obedience levels dropped from 65% to 20% when the instructions to enable shocks were given by an ordinary member of the public wearing plain clothes rather than a researcher (signified by a white lab coat). In the real world, attention bias may also play a role, with people selectively paying more attention to the uniform as an important symbol of power, attributing greater accuracy to the opinions of the wearer and displaying more obedience to that figure in response. However, it is important to recognize that factors like uniform are situational and easily changed. Thus, an individual may masquerade as an authority figure and can elicit the authority bias response from others, even though their authority may not be truly legitimate. Though situational factors may point to an authority figure, the effects of authority bias may be stronger when such situational factors are combined with innate factors, such as gender.

Evidence also suggests that gender can be important in signifying an authority figure. A subset of authority bias, namely gender-authority bias, explains how, in particular, women may be more susceptible to authority bias than males. In addition to women being more influenced by authority figures than males, female authority figures may be less influential than their male counterparts. Research has shown that both men and women associated men with high authority roles and women with low authority roles, showing how gender can signify an authority figure and subsequently influence authority bias. As a result, confirmation bias may also play a role here in what people interpret to be a characteristic of an authority figure; as many associate higher-authority roles with males, confirmation bias suggests that people will tend to look for, favor, interpret or recall information that fits with this belief, perpetuating this bias, reinforcing the idea of males being more legitimate as authority figures. Gender-authority bias has been demonstrated across a variety of real-world sectors. In politics, research into leadership using the Implicit Association Test (IAT) found that female political leaders tend to face greater resistance to their authority in comparison to their male counterparts. Authority bias is therefore strengthened when an authority figure is male instead of female in politics. In finance, males are often favored as authority figures, being perceived to have greater control of resources, and able to make better decisions, demonstrated by the fact that they hold higher positions and on average earn 66% more than females in the finance industry. Research also suggests that women are more susceptible to authority bias, as they are more influenced than men by hearing fake news from an authority figure.

Prior to psychological research, the most common example of this phenomenon was when people obeyed Hitler during World War II, though such effects have been evident throughout history. This relates to pluralistic ignorance, in which authority figures are obeyed regardless of immorality.
The term “authority bias” was first mentioned in literature in reference to state authority bias, in which it simply indicated a preference for being pro-state or anti-state in the US federal election.
Nevertheless, the first-time authority bias was referenced in literature as a cognitive bias was a result of Milgram's experiment, in which it was used to explain obedience to authority figures. Whilst Milgram did not directly use the term "authority bias" in his 1963 paper, the obedience effect identified from his study became the primary example of authority bias.  Milgram's findings succeeded the reason why people during World War II obeyed Hitler; participants voluntarily submitted to the authority figure (the experimenter wearing a white lab coat, signifying professionalism). Authority bias is further strengthened through the use of uniforms to signify authority, initially investigated in Milgram's situational variable (where obedience decreased when the uniform of the experimenter was changed from a lab coat to everyday clothes), but further replicated through Bickman's infamous research into obedience, where security guards are more likely to be obeyed without question and thus contributing directly to authority bias.

Authority bias is used as a marketing strategy in order to increase the legitimacy of claims made about a product. A common example in advertising is where toothpaste companies such as Sensodyne promote the validity of their claims by ensuring the dentists wear lab coats, resulting in the consumer being more trustworthy of the product and consequently more likely to buy the product. Personalized advertising in relation to political voting attitudes can also rely on authority bias.

The expert halo effect is synonymous with authority bias in medicine, where the expert is seen as infallible. Issues arise in pharmaceutical settings, in which non-experts blindly follow expert's commands, resulting in the distribution of harmful drugs and inappropriate healthcare practices.
A further issue concerning the extent to which an authority figure is perceived to be providing accurate information is apparent in cases such as that of Willie Jackson. Forensic dentistry falsely proved Jackson to be guilty, yet the authority bias strengthened the doctor's standpoint in a court of law as they had expert authority bias.  Consequently, the negative effect of authority bias has led to wrongful convictions.

Authority bias is demonstrated in the case of the highest-paid persons' opinion (HIPPO) impact, which describes how employees and other stakeholders in the solution environment tend to go with the opinions and impressions of the highly paid people in an organization.

Evolution has established a dominance hierarchy in which it is an evolutionary advantage to obey authority figures, as figures of authority have a greater allocation of resources and other means of survival. The logical fallacy of ad verecundiam is evolutionary, highlighting that experts are more reliable due to a lack of opposing information, increasing trustworthiness.

In any society, a diverse and widely accepted system of authority allows the development of sophisticated structures for the production of resources, trade, expansion and social control. Notions of obedience and loyalty to the legitimate rule of others are generally accorded values in schools, the law, the military and in political systems. The strength of the bias to obey a legitimate authority figure comes from systemic socialization practices designed to instill the perception that such obedience constitutes correct behavior, and that genuine authority figures usually possess higher degrees of knowledge, wisdom and power. Different societies vary the terms of this dimension. As a result, authority bias can be rooted in the underlying social norms of society. Consequently, deference to authority can occur mindlessly as a kind of decision-making short cut.

Research support for the strength of authority bias is evident; however, the effect is not significant in some instances. Research is merely correlational, and hence other behavioural effects experienced in conjunction with authority bias strengthen its effects.

An individual exhibiting authority bias may also be subject to experiencing confirmation bias, which is the tendency to search for information that confirms one's own existing beliefs. Research suggests that greater authority is given to financial advisors who confirm one's existing opinions, implying that authority bias is strengthened when it coincides with confirmation bias.

The bandwagon effect is where people adopt the ideologies of those surrounding them. Society favours the opinions of authority figures, hence it is a majority view which others support.

Omission bias is the phenomenon in which people prefer omission (inaction) over commission (action), and tend to judge harm as a result of commission more negatively than harm as a result of omission. It can occur due to a number of processes, including psychological inertia, the perception of transaction costs, and the perception that commissions are more causal than omissions.
In social political terms the Universal Declaration of Human Rights establishes how basic human rights are to be assessed in article 2, as "without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status." criteria that are often subject to one or another form of omission bias.   It is controversial as to whether omission bias is a cognitive bias or is often rational. The bias is often showcased through the trolley problem and has also been described as an explanation for the endowment effect and status quo bias.

Taoism may gnomically promote inaction: "If you follow  the Way you shall do less each day. You shall do less and less until you do nothing at all. And if you do nothing at all, there is nothing that is left undone."
Spranca, Minsk and Baron extended the omission bias to judgments of morality of choices. In one scenario, John, a tennis player, would be facing a tough opponent the next day in a decisive match. John knows his opponent is allergic to a food substance. Subjects were presented with two conditions: John recommends the food containing the allergen to hurt his opponent's performance, or the opponent himself orders the allergenic food, and John says nothing. A majority of people judged that John's action of recommending the allergenic food as more immoral than John's inaction of not informing the opponent of the allergenic substance.
The effect has also held in real-world athletic arenas: NBA statistics showcased that referees called 50 percent fewer fouls in the final moments of close games.
An additional real-world example is when parents decide not to vaccinate their children because of the potential chance of death—even when the probability the vaccination will cause death is much less likely than death from the disease prevented.

The just-world fallacy, or just-world hypothesis, is the cognitive bias that assumes that "people get what they deserve" – that actions will necessarily have morally fair and fitting consequences for the actor. For example, the assumptions that noble actions will eventually be rewarded and evil actions will eventually be punished fall under this fallacy. In other words, the just-world fallacy is the tendency to attribute consequences to – or expect consequences as the result of – either a universal force that restores moral balance or a universal connection between the nature of actions and their results. This belief generally implies the existence of cosmic justice, destiny, divine providence, desert, stability, order, or the anglophone colloquial use of "karma". It is often associated with a variety of fundamental fallacies, especially in regard to rationalizing suffering on the grounds that the sufferers "deserve" it. This is called victim blaming.
This fallacy popularly appears in the English language in various figures of speech that imply guaranteed punishment for wrongdoing, such as: "you got what was coming to you", "what goes around comes around", "chickens come home to roost", "everything happens for a reason", and "you reap what you sow". This hypothesis has been widely studied by social psychologists since Melvin J. Lerner conducted seminal work on the belief in a just world in the early 1960s. Research has continued since then, examining the predictive capacity of the fallacy in various situations and across cultures, and clarifying and expanding the theoretical understandings of just-world beliefs.

Many philosophers and social theorists have observed and considered the phenomenon of belief in a just world, going back to at least as early as the Pyrrhonist philosopher Sextus Empiricus, writing c. 180 CE, who argued against this belief. Lerner's work made the just-world hypothesis a focus of research in the field of social psychology.

Lerner was prompted to study justice beliefs and the just-world fallacy in the context of social psychological inquiry into negative social and societal interactions. Lerner saw his work as extending Stanley Milgram's work on obedience. He sought to answer the questions of how regimes that cause cruelty and suffering maintain popular support, and how people come to accept social norms and laws that produce misery and suffering.
Lerner's inquiry was influenced by repeatedly witnessing the tendency of observers to blame victims for their suffering. During his clinical training as a psychologist, he observed treatment of mentally ill persons by the health care practitioners with whom he worked. Although Lerner knew them to be kindhearted, educated people, they often blamed patients for the patients' own suffering. Lerner also describes his surprise at hearing his students derogate (disparage, belittle) the poor, seemingly oblivious to the structural forces that contribute to poverty. The desire to understand the processes that caused these phenomena led Lerner to conduct his first experiments on what is now called the just-world fallacy.

In 1966, Lerner and his colleagues began a series of experiments that used shock paradigms to investigate observer responses to victimization. In the first of these experiments conducted at the University of Kansas, 72 female participants watched what appeared to be a confederate receiving electrical shocks for her errors during a learning task (learning pairs of nonsense syllables). Initially, these observing participants were upset by the victim's apparent suffering. But as the suffering continued and observers remained unable to intervene, the observers began to reject and devalue the victim. Rejection and devaluation of the victim was greater when the observed suffering was greater. But when participants were told the victim would receive compensation for her suffering, the participants did not derogate the victim. Lerner and colleagues replicated these findings in subsequent studies, as did other researchers.

To explain these studies' findings, it was theorized that there was a prevalent belief in a just world. A just world is one in which actions and conditions have predictable, appropriate consequences. These actions and conditions are typically individuals' behaviors or attributes. The specific conditions that correspond to certain consequences are socially determined by a society's norms and ideologies. Lerner presents the belief in a just world as functional: it maintains the idea that one can influence the world in a predictable way. Belief in a just world functions as a sort of "contract" with the world regarding the consequences of behavior. This allows people to plan for the future and engage in effective, goal-driven behavior. Lerner summarized his findings and his theoretical work in his 1980 monograph The Belief in a Just World: A Fundamental Delusion.
Lerner hypothesized that the belief in a just world is crucially important for people to maintain for their own well-being. But people are confronted daily with evidence that the world is not just: people suffer without apparent cause. Lerner explained that people use strategies to eliminate threats to their belief in a just world. These strategies can be rational or irrational. Rational strategies include accepting the reality of injustice, trying to prevent injustice or provide restitution, and accepting one's own limitations. Non-rational strategies include denial, withdrawal, and reinterpretation of the event.
There are a few modes of reinterpretation that could make an event fit the belief in a just world. One can reinterpret the outcome, the cause, and/or the character of the victim.  In the case of observing the injustice of the suffering of innocent people, one major way to rearrange the cognition of an event is to interpret the victim of suffering as deserving. Specifically, observers can blame victims for their suffering on the basis of their behaviors and/or their characteristics. Much psychological research on the belief in a just world has focused on these negative social phenomena of victim blaming and victim derogation in different contexts.
An additional effect of this thinking is that individuals experience less personal vulnerability because they do not believe they have done anything to deserve or cause negative outcomes. This is related to the self-serving bias observed by social psychologists.
Many researchers have interpreted just-world beliefs as an example of causal attribution. In victim blaming, the causes of victimization are attributed to an individual rather than to a situation. Thus, the consequences of belief in a just world may be related to or explained in terms of particular patterns of causal attribution.

Others have suggested alternative explanations for the derogation of victims. One suggestion is that derogation effects are based on accurate judgments of a victim's character. In particular, in relation to Lerner's first studies, some have hypothesized that it would be logical for observers to derogate an individual who would allow himself to be shocked without reason. A subsequent study by Lerner challenged this alternative hypothesis by showing that individuals are only derogated when they actually suffer; individuals who agreed to undergo suffering but did not were viewed positively.

Another alternative explanation offered for the derogation of victims early in the development of the just-world fallacy was that observers derogate victims to reduce their own feelings of guilt. Observers may feel responsible, or guilty, for a victim's suffering if they themselves are involved in the situation or experiment. In order to reduce the guilt, they may devalue the victim. Lerner and colleagues claim that there has not been adequate evidence to support this interpretation. They conducted one study that found derogation of victims occurred even by observers who were not implicated in the process of the experiment and thus had no reason to feel guilty.

Alternatively, victim derogation and other strategies may only be ways to alleviate discomfort after viewing suffering. This would mean that the primary motivation is not to restore a belief in a just world, but to reduce discomfort caused by empathizing.  Studies have shown that victim derogation does not suppress subsequent helping activity and that empathizing with the victim plays a large role when assigning blame. According to Ervin Staub, devaluing the victim should lead to lesser compensation if restoring belief in a just world was the primary motive; instead, there is virtually no difference in compensation amounts whether the compensation precedes or follows devaluation. Psychopathy has been linked to the lack of just-world maintaining strategies, possibly due to dampened emotional reactions and lack of empathy.

After Lerner's first studies, other researchers replicated these findings in other settings in which individuals are victimized. This work, which began in the 1970s and continues today, has investigated how observers react to victims of random calamities like traffic accidents, as well as rape and domestic violence, illnesses, and poverty. Generally, researchers have found that observers of the suffering of innocent victims tend to both derogate and blame victims for their suffering. Observers thus maintain their belief in a just world by changing their cognitions about the victims' character.
In the early 1970s, social psychologists Zick Rubin and Letitia Anne Peplau developed a measure of belief in a just world. This measure and its revised form published in 1975 allowed for the study of individual differences in just-world beliefs. Much of the subsequent research on the just-world hypothesis used these measurement scales.
These studies on victims of violence, illness, and poverty and others like them have provided consistent support for the link between observers' just-world beliefs and their tendency to blame victims for their suffering. As a result, the existence of the just-world hypothesis as a psychological phenomenon has become widely accepted.

Researchers have looked at how observers react to victims of rape and other violence. In a formative experiment on rape and belief in a just world by Linda Carli and colleagues, researchers gave two groups of subjects a narrative about interactions between a man and a woman. The description of the interaction was the same until the end; one group received a narrative that had a neutral ending and the other group received a narrative that ended with the man raping the woman. Subjects judged the rape ending as inevitable and blamed the woman in the narrative for the rape on the basis of her behavior, but not her characteristics. These findings have been replicated repeatedly, including using a rape ending and a "happy ending" (a marriage proposal).
Other researchers have found a similar phenomenon for judgments of battered partners. One study found that observers' labels of blame of female victims of relationship violence increase with the intimacy of the relationship. Observers blamed the perpetrator only in the least intimate case of violence, in which a male struck an acquaintance.

Researchers have employed the just-world fallacy to understand bullying. Given other research on beliefs in a just world, it would be expected that observers would derogate and blame bullying victims, but the opposite has been found: individuals high in just-world belief have stronger anti-bullying attitudes. Other researchers have found that strong belief in a just world is associated with lower levels of bullying behavior. This finding is in keeping with Lerner's understanding of belief in a just world as functioning as a "contract" that governs behavior. There is additional evidence that belief in a just world is protective of the well-being of children and adolescents in the school environment, as has been shown for the general population.

Other researchers have found that observers judge sick people as responsible for their illnesses. One experiment showed that persons suffering from a variety of illnesses were derogated on a measure of attractiveness more than healthy individuals were. In comparison to healthy people, victim derogation was found for persons presenting with indigestion, pneumonia, and stomach cancer. Moreover, derogation was found to be higher for those suffering from more severe illnesses, except for those presenting with cancer. Stronger belief in a just world has also been found to correlate with greater derogation of AIDS victims.

More recently, researchers have explored how people react to poverty through the lens of the just-world fallacy. Strong belief in a just world is associated with blaming the poor, with weak belief in a just world associated with identifying external causes of poverty including world economic systems, war, and exploitation.

Some research on belief in a just world has examined how people react when they themselves are victimized. An early paper by Dr. Ronnie Janoff-Bulman found that rape victims often blame their own behavior, but not their own characteristics, for their victimization. It was hypothesized that this may be because blaming one's own behavior makes an event more controllable.

Subsequent work on measuring belief in a just world has focused on identifying multiple dimensions of the belief. This work has resulted in the development of new measures of just-world belief and additional research. Hypothesized dimensions of just-world beliefs include belief in an unjust world, beliefs in immanent justice and ultimate justice, hope for justice, and belief in one's ability to reduce injustice.  Other work has focused on looking at the different domains in which the belief may function; individuals may have different just-world beliefs for the personal domain, the sociopolitical domain, the social domain, etc. An especially fruitful distinction is between the belief in a just world for the self (personal) and the belief in a just world for others (general). These distinct beliefs are differentially associated with positive mental health.

Researchers have used measures of belief in a just world to look at correlates of high and low levels of belief in a just world.
Limited studies have examined ideological correlates of the belief in a just world. These studies have found sociopolitical correlates of just-world beliefs, including right-wing authoritarianism and the Protestant work ethic. Studies have also found belief in a just world to be correlated with aspects of religiosity.
Studies of demographic differences, including gender and racial differences, have not shown systematic differences, but do suggest racial differences, with black people and African Americans having the lowest levels of belief in a just world.
The development of measures of just-world beliefs has also allowed researchers to assess cross-cultural differences in just-world beliefs. Much research conducted shows that beliefs in a just world are evident cross-culturally. One study tested beliefs in a just world of students in 12 countries. This study found that in countries where the majority of inhabitants are powerless, belief in a just world tends to be weaker than in other countries. This supports the theory of the just-world fallacy because the powerless have had more personal and societal experiences that provided evidence that the world is not just and predictable.
Belief in unjust world has been linked to increased self-handicapping, criminality, defensive coping, anger and perceived future risk. It may also serve as ego-protective belief for certain individuals by justifying maladaptive behavior.

Although much of the initial work on belief in a just world focused on its negative social effects, other research suggests that belief in a just world is good, and even necessary, for mental health. Belief in a just world is associated with greater life satisfaction and well-being and less depressive affect. Researchers are actively exploring the reasons why the belief in a just world might have this relationship to mental health; it has been suggested that such beliefs could be a personal resource or coping strategy that buffers stress associated with daily life and with traumatic events. This hypothesis suggests that belief in a just world can be understood as a positive illusion. In line with this perspective, recent research also suggests that belief in a just world may explain the known statistical association between religiosity/spirituality and psychological well-being. Some belief in a just world research has been conducted within the framework of primal world beliefs, and has found strong correlations between just world belief and beliefs that the world is safe, abundant and cooperative (among other qualities).
Some studies also show that beliefs in a just world are correlated with internal locus of control. Strong belief in a just world is associated with greater acceptance of and less dissatisfaction with negative events in one's life. This may be one way in which belief in a just world affects mental health. Others have suggested that this relationship holds only for beliefs in a just world for oneself. Beliefs in a just world for others are related instead to the negative social phenomena of victim blaming and victim derogation observed in other studies.
Belief in a just world has also been found to negatively predict the perceived likelihood of kin favoritism. The perspective of the individual plays an important role in this relationship, such that when people imagine themselves as mere observers of injustice, general belief in a just world will be the stronger predictor, and when they imagine themselves as victims of injustice, personal belief in a just world will be the stronger predictor. This further supports the distinction between general and personal belief in a just world.

More than 40 years after Lerner's seminal work on belief in a just world, researchers continue to study the phenomenon. Belief in a just world scales have been validated in several countries such as Iran, Russia, Brazil, and France. Work continues primarily in the United States, Europe, Australia, and Asia. Researchers in Germany have contributed disproportionately to recent research. Their work resulted in a volume edited by Lerner and German researcher Leo Montada titled Responses to Victimizations and Belief in a Just World.



"Illusory correlation" was originally coined by Chapman (1967) to describe people's tendencies to overestimate relationships between two groups when distinctive and unusual information is presented.  The concept was used to question claims about objective knowledge in clinical psychology through Chapmans' refutation of many clinicians' widely used Wheeler signs for homosexuality in Rorschach tests.

David Hamilton and Robert Gifford (1976) conducted a series of experiments that demonstrated how stereotypic beliefs regarding minorities could derive from illusory correlation processes. To test their hypothesis, Hamilton and Gifford had research participants read a series of sentences describing either desirable or undesirable behaviors, which were attributed to either Group A (the majority) or Group B (the minority). Abstract groups were used so that no previously established stereotypes would influence results. Most of the sentences were associated with Group A, and the remaining few were associated with Group B. The following table summarizes the information given.
Each group had the same proportions of positive and negative behaviors, so there was no real association between behaviors and group membership. Results of the study show that positive, desirable behaviors were not seen as distinctive so people were accurate in their associations. On the other hand, when distinctive, undesirable behaviors were represented in the sentences, the participants overestimated how much the minority group exhibited the behaviors.
A parallel effect occurs when people judge whether two events, such as pain and bad weather, are correlated. They rely heavily on the relatively small number of cases where the two events occur together. People pay relatively little attention to the other kinds of observation (of no pain or good weather).

Most explanations for illusory correlation involve psychological heuristics: information processing short-cuts that underlie many human judgments. One of these is availability: the ease with which an idea comes to mind. Availability is often used to estimate how likely an event is or how often it occurs. This can result in illusory correlation, because some pairings can come easily and vividly to mind even though they are not especially frequent.

Martin Hilbert (2012) proposes an information processing mechanism that assumes a noisy conversion of objective observations into subjective judgments. The theory defines noise as the mixing of these observations during retrieval from memory. According to the model, underlying cognitions or subjective judgments are identical with noise or objective observations that can lead to overconfidence or what is known as conservatism bias—when asked about behavior participants underestimate the majority or larger group and overestimate the minority or smaller group. These results are illusory correlations.

In an experimental study done by Eder, Fiedler and Hamm-Eder (2011), the effects of working-memory capacity on illusory correlations were investigated.  They first looked at the individual differences in working memory, and then looked to see if that had any effect on the formation of illusory correlations.  They found that individuals with higher working memory capacity viewed minority group members more positively than individuals with lower working memory capacity.  In a second experiment, the authors looked into the effects of memory load in working memory on illusory correlations.  They found that increased memory load in working memory led to an increase in the prevalence of illusory correlations.  The experiment was designed to specifically test working memory and not substantial stimulus memory.  This means that the development of illusory correlations was caused by deficiencies in central cognitive resources caused by the load in working memory, not selective recall.

Attention theory of learning proposes that features of majority groups are learned first, and then features of minority groups.  This results in an attempt to distinguish the minority group from the majority, leading to these differences being learned more quickly. The Attention theory also argues that, instead of forming one stereotype regarding the minority group, two stereotypes, one for the majority and one for the minority, are formed.

A study was conducted to investigate whether increased learning would have any effect on illusory correlations. It was found that educating people about how illusory correlation occurs resulted in a decreased incidence of illusory correlations.

Johnson and Jacobs (2003) performed an experiment to see how early in life individuals begin forming illusory correlations. Children in grades 2 and 5 were exposed to a typical illusory correlation paradigm to see if negative attributes were associated with the minority group. The authors found that both groups formed illusory correlations.
A study also found that children create illusory correlations.  In their experiment, children in grades 1, 3, 5, and 7, and adults all looked at the same illusory correlation paradigm.  The study found that children did create significant illusory correlations, but those correlations were weaker than the ones created by adults.  In a second study, groups of shapes with different colors were used. The formation of illusory correlation persisted showing that social stimuli are not necessary for creating these correlations.

Two studies performed by Ratliff and Nosek examined whether or not explicit and implicit attitudes affected illusory correlations.  In one study, Ratliff and Nosek had two groups: one a majority and the other a minority. They then had three groups of participants, all with readings about the two groups.  One group of participants received overwhelming pro-majority readings, one was given pro-minority readings, and one received neutral readings.  The groups that had pro-majority and pro-minority readings favored their respective pro groups both explicitly and implicitly.  The group that had neutral readings favored the majority explicitly, but not implicitly.  The second study was similar, but instead of readings, pictures of behaviors were shown, and the participants wrote a sentence describing the behavior they saw in the pictures presented.  The findings of both studies supported the authors' argument that the differences found between the explicit and implicit attitudes is a result of the interpretation of the covariation and making judgments based on these interpretations (explicit) instead of just accounting for the covariation (implicit).

Berndsen et al. (1999) wanted to determine if the structure of testing for illusory correlations could lead to the formation of illusory correlations.  The hypothesis was that identifying test variables as Group A and Group B might be causing the participants to look for differences between the groups, resulting in the creation of illusory correlations.  An experiment was set up where one set of participants were told the groups were Group A and Group B, while another set of participants were given groups labeled as students who graduated in 1993 or 1994.  This study found that illusory correlations were more likely to be created when the groups were Group A and B, as compared to students of the class of 1993 or the class of 1994.

Survivorship bias or survival bias is the logical error of concentrating on entities that passed a selection process while overlooking those that did not. This can lead to incorrect conclusions because of incomplete data.  
Survivorship bias is a form of sampling bias that can lead to overly optimistic beliefs because multiple failures are overlooked, such as when companies that no longer exist are excluded from analyses of financial performance. It can also lead to the false belief that the successes in a group have some special property, rather than just coincidence as in correlation "proves" causality.

The parapsychology researcher Joseph Banks Rhine believed he had identified the few individuals from hundreds of potential subjects who had powers of extra-sensory perception (ESP). His calculations were based on the improbability of these few subjects guessing the Zener cards shown to a partner by chance. A major criticism that surfaced against his calculations was the possibility of unconscious survivorship bias in subject selections. He was accused of failing to take into account the large effective size of his sample; that is, all the subjects he rejected as not being "strong telepaths" because they had failed at an earlier testing stage. Had he done this, he might have seen that from the large sample, one or two individuals would probably achieve—purely by chance—the track record of success he observed.
Writing about the Rhine case in Fads and Fallacies in the Name of Science, Martin Gardner explained that the experimenters had made such obvious mistakes not out of statistical naivety, but rather as a result of subtly disregarding poorly performing subjects. He said that, even without trickery of any kind, if a large enough sample were taken, there would always be a certain number of subjects who demonstrated improbable success. To illustrate this, he speculates about what would happen if one hundred professors of psychology read Rhine's work and decided to make their own tests; he said that survivor bias would winnow out the typically failed experiments, while encouraging the lucky successes to continue testing. 
He postulated that experiments confirming the null hypothesis (i.e., showing no result) would not be reported, but "[e]ventually, one experimenter remains whose subject has made high scores for six or seven successive sessions. Neither experimenter nor subject is aware of the other ninety-nine projects, and so both have a strong delusion that ESP is operating." He concludes: "The experimenter writes an enthusiastic paper, sends it to Rhine who publishes it in his magazine, and the readers are greatly impressed."
If sufficiently many scientists study a phenomenon, some will find statistically significant results by chance, and these are the experiments submitted for publication. Additionally, papers showing positive results may be more appealing to editors. This problem is known as positive results bias, a type of publication bias. To combat this, some editors now specifically call for the submission of "negative" scientific findings, where "nothing happened".
Survivorship bias is one of the research issues brought up in the provocative 2005 paper "Why Most Published Research Findings Are False", which shows that a large number of published medical research papers contain results that cannot be replicated.
One famous example of survivorship bias occurred in a study by Redelmeier and Singh, which was published in the Annals of Internal Medicine and purported to show that  Academy Award-winning actors and actresses lived almost four years longer than their less successful peers. The statistical method used to derive this statistically significant difference, however, gave winners an unfair advantage, because it credited winners' years of life before winning toward survival subsequent to winning. When the data was reanalyzed using methods that avoided this immortal time bias, the survival advantage was closer to one year and was not statistically significant.

In finance, survivorship bias can manifest as the tendency for failed companies to be excluded from performance studies because they no longer exist. It often causes the results of studies to skew higher because only companies that were successful enough to survive until the end of the period are included. For example, a mutual fund company's selection of funds today will include only those that are successful now. Many losing funds are closed and merged into other funds to hide poor performance. In theory, 70% of extant funds could truthfully claim to have performance in the first quartile of their peers, if the peer group includes funds that have closed.

Michael Shermer in Scientific American and Larry Smith of the University of Waterloo have described how advice about commercial success distorts perceptions of it by ignoring all of the businesses and college dropouts that failed. Journalist and author David McRaney observes that the "advice business is a monopoly run by survivors. When something becomes a non-survivor, it is either completely eliminated, or whatever voice it has is muted to zero". Alec Liu wrote in Vice that "for every Mark Zuckerberg, there's thousands of also-rans, who had parties no one ever attended, obsolete before we ever knew they existed."
In his book The Black Swan, financial writer Nassim Taleb called the data obscured by survivorship bias "silent evidence".

Diagoras of Melos was asked concerning paintings of those who had escaped shipwreck: "Look, you who think the gods have no care of human things, what do you say to so many persons preserved from death by their especial favour?", to which Diagoras replied: "Why, I say that their pictures are not here who were cast away, who are by much the greater number." A similar story is told about Diogenes of Sinope.
Susan Mumm has described how survival bias leads historians to study organisations that are still in existence more than those that have closed. This means large, successful organisations such as the Women's Institute, which were well organised and still have accessible archives for historians to work from, are studied more than smaller charitable organisations, even though these may have done a great deal of work.

Whether it be movie stars, athletes, musicians, or CEOs of multibillion-dollar corporations who dropped out of school, popular media often tells the story of the determined individual who pursues their dreams and beats the odds. There is much less focus on the many people that may be similarly skilled and determined, but fail to ever find success because of factors beyond their control or other (seemingly) random events. There is also a tendency to overlook resources and events that helped enable such success, that those who failed didn't have. 
For example, a 2013 study found that 91% of artists were undiscovered on social media, and just 1.1% were mainstream or mega-sized. The overwhelming majority of failures are not visible to the public eye, and only those who survive the selective pressures of their competitive environment are seen regularly.

During World War II, the statistician Abraham Wald took survivorship bias into his calculations when considering how to minimize bomber losses to enemy fire. The Statistical Research Group (SRG) at Columbia University, of which Wald was a member, examined the damage done to aircraft that had returned from missions and recommended adding armor to the areas that showed the least damage. The bullet holes in the returning aircraft represented areas where a bomber could take damage and still fly well enough to return safely to base. Therefore, Wald proposed that the Navy reinforce areas where the returning aircraft were unscathed, inferring that planes hit in those areas were the ones most likely to be lost. His work is considered seminal in the then nascent discipline of operational research.

In a study performed in 1987, it was reported that cats who fall from less than six stories, and are still alive, have greater injuries than cats who fall from higher than six stories. It has been proposed that this might happen because cats reach terminal velocity after righting themselves at about five stories, and after this point they relax, leading to less severe injuries in cats who have fallen from six or more stories. In 1996, The Straight Dope newspaper column proposed that another possible explanation for this phenomenon would be survivorship bias. Cats that die in falls are less likely to be brought to a veterinarian than injured cats, and thus many of the cats killed in falls from higher buildings are not reported in studies of the subject.

Large groups of organisms called clades that survive a long time are subject to various survivorship biases such as the "push of the past", generating the illusion that clades in general tend to originate with a high rate of diversification that then slows through time.

Survivorship bias can raise truth-in-advertising issues when the success rate advertised for a product or service is measured by reference to a population whose makeup differs from that of the target audience for the advertisement. This is especially important when
the advertisement either fails to disclose the relevant differences between the two populations, or describes them in insufficient detail; and
these differences result from the company's deliberate "pre-screening" of prospective customers to ensure that only customers with traits increasing their likelihood of success are allowed to purchase the product or service, especially when the company's selection procedures or evaluation standards are kept secret; and
the company offering the product or service charges a fee, especially one that is non-refundable or not disclosed in the advertisement, for the privilege of attempting to become a customer.
For example, the advertisements of online dating service eHarmony.com fail a truth in advertising test because they fail the first two prongs and pass the third, when all three must be passed:
they claim a success rate significantly higher than that of competing services while generally not disclosing that the rate is calculated with respect to a viewership subset of individuals who possess traits that increase their likelihood of finding and maintaining relationships and lack traits that pose obstacles to their doing so, and
the company deliberately selects for these traits by administering a lengthy pre-screening process designed to reject prospective customers who lack the former traits or possess the latter ones, but 
the company does not charge a fee for administration of its pre-screening test; thus its prospective customers face no "downside risk" other than wasting their time, expending the effort involved in completing the pre-screening process, and suffering disappointment.

Belief perseverance (also known as conceptual conservatism) is maintenance of a belief despite new information that firmly contradicts it.
Since rationality involves conceptual flexibility, belief perseverance is consistent with the view that human beings act at times in an irrational manner. Philosopher F.C.S. Schiller holds that belief perseverance "deserves to rank among the fundamental 'laws' of nature".
If beliefs are strengthened after others attempt to present evidence debunking them, this is known as a backfire effect. There are psychological mechanisms by which backfire effects could potentially occur, but the evidence on this topic is mixed, and backfire effects are very rare in practice.
A 2020 review of the scientific literature on backfire effects found that there have been widespread failures to replicate their existence, even under conditions that theoretically would be favorable to observing them. Due to the lack of reproducibility, as of 2020 most researchers believe that backfire effects either are unlikely to occur on the broader population level, or only occur in very specific circumstances, or do not exist.
For most people, corrections and fact-checking are very unlikely to have a negative effect, and there is no specific group of people in which backfire effects have been consistently observed.

According to Lee Ross and Craig A. Anderson, "beliefs are remarkably resilient in the face of empirical challenges that seem logically devastating".
The first study of belief perseverance was carried out by Festinger, Riecken, and Schachter. These psychiatrists spent time with members of a doomsday cult who believed the world would end on December 21, 1954. Despite the failure of the forecast, most believers continued to adhere to their faith. In When Prophecy Fails: A Social and Psychological Study of a Modern Group That Predicted the Destruction of the World (1956) and A Theory of Cognitive Dissonance (1957), Festinger proposed that human beings strive for internal psychological consistency to function mentally in the real world. A person who experiences internal inconsistency tends to become psychologically uncomfortable and is motivated to reduce the cognitive dissonance. They tend to make changes to justify the stressful behavior, either by adding new parts to the cognition causing the psychological dissonance (rationalization) or by avoiding circumstances and contradictory information likely to increase the magnitude of the cognitive dissonance (confirmation bias).
When asked to reappraise probability estimates in light of new information, subjects displayed a marked tendency to give insufficient weight to the new evidence. They refused to acknowledge the inaccurate prediction as a reflection of the overall validity of their faith. In some cases, subjects reported having a stronger faith in their religion than before.
Lee Ross and Craig A. Anderson led some subjects to the false belief that there existed a positive correlation between a firefighter's stated preference for taking risks and their occupational performance. Other subjects were told that the correlation was negative. The participants were then thoroughly debriefed and informed that there was no link between risk taking and performance. These authors found that post-debriefing interviews pointed to significant levels of belief perseverance.
In another study, subjects spent about four hours following instructions of a hands-on instructional manual.  At a certain point, the manual introduced a formula which led them to believe that spheres were 50 percent larger than they are. Subjects were then given an actual sphere and asked to determine its volume; first by using the formula, and then by filling the sphere with water, transferring the water to a box, and directly measuring the volume of the water in the box. In the last experiment in this series, all 19 subjects held a Ph.D. degree in a natural science, were employed as researchers or professors at two major universities, and carried out the comparison between the two volume measurements a second time with a larger sphere. All but one of these scientists clung to the spurious formula despite their empirical observations.

If beliefs are strengthened after others attempt to present evidence debunking them, this is known as a backfire effect (compare boomerang effect). For example, this would apply if providing information on the safety of vaccinations resulted in increased vaccination hesitancy. Types of backfire effects include: Familiarity Backfire Effect (from making myths more familiar), Overkill Backfire Effect (from providing too many arguments), and Worldview Backfire Effect (from providing evidence that threatens someone's worldview). There are a number of techniques to debunk misinformation, such as emphasizing the core facts and not the myth, or providing explicit warnings that the upcoming information is false, and providing alternative explanations to fill the gaps left by debunking the misinformation. However, more recent studies provided evidence that the backfire effects are not as likely as once thought.
There are psychological mechanisms by which backfire effects could potentially occur, but the evidence on this topic is mixed, and backfire effects are very rare in practice. A 2020 review of the scientific literature on backfire effects found that there have been widespread failures to replicate their existence, even under conditions that would be theoretically favorable to observing them. Due to the lack of reproducibility, as of 2020 most researchers believe that backfire effects are either unlikely to occur on the broader population level, or they only occur in very specific circumstances, or they do not exist. Brendan Nyhan, one of the researchers who initially proposed the occurrence of backfire effects, wrote in 2021 that the persistence of misinformation is most likely due to other factors.
For most people, corrections and fact-checking are very unlikely to have a negative impact, and there is no specific group of people in which backfire effects have been consistently observed. Presenting people with factual corrections has been demonstrated to have a positive effect in many circumstances. For example, this has been studied in the case of informing believers in 9/11 conspiracy theories about statements by actual experts and witnesses. One possibility is that criticism is most likely to backfire if it challenges someone's worldview or identity. This suggests that an effective approach may be to provide criticism while avoiding such challenges.
In many cases, when backfire effects have been discussed by the media or by bloggers, they have been over-generalized from studies on specific subgroups to incorrectly conclude that backfire effects apply to the entire population and to all attempts at correction.

Physicist Max Planck wrote that "the new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it". For example, the heliocentric theory of the great Greek astronomer, Aristarchus of Samos, had to be rediscovered about 1,800 years later, and even then undergo a major struggle before astronomers took its veracity for granted.
Belief persistence is frequently accompanied by intrapersonal cognitive processes. "When the decisive facts did at length obtrude themselves upon my notice," wrote the chemist Joseph Priestley, "it was very slowly, and with great hesitation, that I yielded to the evidence of my senses."

Students often "cling to ideas that form part of their world view even when confronted by information that does not coincide with this view." For example, students may spend months studying the Solar System and do well on related tests, but still believe that the Moon's perceived phases are produced by Earth's shadow. What they learned was not able to intrude on the beliefs they held prior to that knowledge.

The causes of belief perseverance remain unclear. Experiments in the 2010s suggest that neurochemical processes in the brain underlie the strong attentional bias of reward learning. Similar processes could underlie belief perseverance.
Peter Marris suggests that the process of abandoning a conviction is similar to the working out of grief. "The impulse to defend the predictability of life is a fundamental and universal principle of human psychology." Human beings possess "a deep-rooted and insistent need for continuity".
Philosopher of science Thomas Kuhn points to the resemblance between conceptual change and Gestalt perceptual shifts (e.g., the difficulty encountered in seeing the hag as a young lady). Hence, the difficulty of switching from one conviction to another could be traced to the difficulty of rearranging one's perceptual or cognitive field.

Zero-risk bias is a tendency to prefer the complete elimination of risk in a sub-part over alternatives with greater overall risk reduction. It often manifests in cases where decision makers address problems concerning health, safety, and the environment. Its effect on decision making has been observed in surveys presenting hypothetical scenarios.

Zero-risk bias is based on the way people feel better if a risk is eliminated instead of being merely mitigated. Scientists identified a zero-risk bias in responses to a questionnaire about a hypothetical cleanup scenario involving two hazardous sites X and Y, with X causing 8 cases of cancer annually and Y causing 4 cases annually. The respondents ranked three cleanup approaches: two options each reduced the total number of cancer cases by 6, while the third reduced the number by 5 while eliminating the cases at site Y. While the latter option featured the worst reduction overall, 42% of the respondents ranked it better than at least one of the other options. This conclusion resembled one from an earlier economics study that found people were willing to pay high costs to eliminate a risk. It has a normative justification since once risk is eliminated, people would have less to worry about and such removal of worry also has utility. It is also driven by our preference for winning much more than losing as well as the old instead of the new way, all of which cloud the way the world is viewed.
Multiple real-world policies have been said to be affected by this bias. In American federal policy, the Delaney clause outlawing cancer-causing additives from foods (regardless of actual risk) and the desire for perfect cleanup of Superfund sites have been alleged to be overly focused on complete elimination. Furthermore, the effort needed to implement zero-risk laws grew as technological advances enabled the detection of smaller quantities of hazardous substances. Limited resources were increasingly being devoted to low-risk issues.
Critics of the zero-risk bias model cite that it has the tendency to neglect overall risk reduction. For instance, when eliminating two side effects, it holds that the complete eradication of just one side-effect is preferable to lowering the overall risk.

Other biases might underlie the zero-risk bias. One is a tendency to think in terms of proportions rather than differences. A greater reduction in proportion of deaths is valued higher than a greater reduction in actual deaths. The zero-risk bias could then be seen as the extreme end of a broad bias about quantities as applied to risk. Framing effects can enhance the bias, for example, by emphasizing a large proportion in a small set, or can attempt to mitigate the bias by emphasizing total quantities.

In social psychology, illusory superiority is a cognitive bias wherein people overestimate their own qualities and abilities compared to others. Illusory superiority is one of many positive illusions, relating to the self, that are evident in the study of intelligence, the effective performance of tasks and tests, and the possession of desirable personal characteristics and personality traits. Overestimation of abilities compared to an objective measure is known as the overconfidence effect.
The term "illusory superiority" was first used by the researchers Van Yperen and Buunk, in 1991. The phenomenon is also known as the above-average effect, the superiority bias, the leniency error, the sense of relative superiority, the primus inter pares effect, and the Lake Wobegon effect, named after the fictional town where all the children are above average. The Dunning-Kruger effect is a form of illusory superiority shown by people on a task where their level of skill is low.
Most of the literature on illusory superiority is from studies on participants in the US. However, research that only investigates the effects in one specific population is severely limited as this may not be a true representation of human psychology. More recent research investigating self-esteem in other countries suggests that illusory superiority depends on culture. Some studies indicate that East Asians tend to underestimate their own abilities in order to improve themselves and get along with others.

Alicke and Govorun proposed the idea that, rather than individuals consciously reviewing and thinking about their own abilities, behaviors and characteristics and comparing them to those of others, it is likely that people instead have what they describe as an "automatic tendency to assimilate positively-evaluated social objects toward ideal trait conceptions". For example, if an individual evaluated themselves as honest, they would be likely to then exaggerate their characteristic towards their perceived ideal position on a scale of honesty. Importantly, Alicke noted that this ideal position is not always the top of the scale; for example, with honesty, someone who is always brutally honest may be regarded as rude—the ideal is a balance, perceived differently by different individuals.

Another explanation for how the better-than-average effect works is egocentrism. This is the idea that an individual places greater importance and significance on their own abilities, characteristics, and behaviors than those of others. Egocentrism is therefore a less overtly self-serving bias. According to egocentrism, individuals will overestimate themselves in relation to others because they believe that they have an advantage that others do not have, as an individual considering their own performance and another's performance will consider their performance to be better, even when they are in fact equal. Kruger (1999) found support for the egocentrism explanation in his research involving participant ratings of their ability on easy and difficult tasks. It was found that individuals were consistent in their ratings of themselves as above the median in the tasks classified as "easy" and below the median in the tasks classified as "difficult", regardless of their actual ability. In this experiment the better-than-average effect was observed when it was suggested to participants that they would be successful, but also a worse-than-average effect was found when it was suggested that participants would be unsuccessful.

Yet another explanation for the better-than-average effect is "focalism", the idea that greater significance is placed on the object that is the focus of attention. Most studies of the better-than-average effect place greater focus on the self when asking participants to make comparisons (the question will often be phrased with the self being presented before the comparison target—"compare yourself to the average person"). According to focalism this means that the individual will place greater significance on their own ability or characteristic than that of the comparison target. This also means that in theory if, in an experiment on the better-than-average effect, the questions were phrased so that the self and other were switched (e.g., "compare the average peer to yourself") the better-than-average effect should be lessened.
Research into focalism has concentrated primarily on optimistic bias rather than the better-than-average effect. However, two studies found a decreased effect of optimistic bias when participants were asked to compare an average peer to themselves, rather than themselves to an average peer.

A 2012 Psychological Bulletin suggests that illusory superiority, as well as other biases, can be explained by an  information-theoretic generative mechanism that assumes observation (a noisy conversion of objective evidence) into subjective estimates (judgment). The study suggests that the underlying cognitive mechanism is  similar to the noisy mixing of memories that cause the conservatism bias or overconfidence: re-adjustment of estimates of our own performance after our own performance are adjusted differently than the re-adjustments regarding estimates of others' performances. Estimates of the scores of others are even more conservative (more influenced by the previous expectation) than our estimates of our own performance (more influenced by the new evidence received after giving the test). The difference in the conservative bias of both estimates (conservative estimate of our own performance, and even more conservative estimate of the performance of others) is enough to create illusory superiority.
Since mental noise is a sufficient explanation that is much simpler and more straightforward than any other explanation involving heuristics, behavior, or social interaction, the Occam's razor principle argues in its favor as the underlying generative mechanism (it is the hypothesis which makes the fewest assumptions).

Selective recruitment is the notion that an individual selects their own strengths and the other's weaknesses when making peer comparisons, in order that they appear better on the whole. This theory was first tested by Weinstein (1980); however, this was in an experiment relating to optimistic bias, rather than the better-than-average effect. The study involved participants rating certain behaviors as likely to increase or decrease the chance of a series of life events happening to them. It was found that individuals showed less optimistic bias when they were allowed to see others' answers.
Perloff and Fetzer (1986) suggested that when making peer comparisons on a specific characteristic, an individual chooses a comparison target—the peer to whom he is being compared—with lower abilities. To test this theory, Perloff and Fetzer asked participants to compare themselves to specific comparison targets like a close friend, and found that illusory superiority decreased when they were told to envision a specific person rather than vague constructs like "the average peer". However, these results are not completely reliable and could be affected by the fact that individuals like their close friends more than an "average peer" and may as a result rate their friend as being higher than average, therefore the friend would not be an objective comparison target.

This idea, put forward by Giladi and Klar, suggests that when making comparisons any single member of a group will tend to evaluate themselves to rank above that group's statistical mean performance level or the median performance level of its members. For example, if an individual is asked to assess their own skill at driving compared to the rest of the group, they are likely to rate themself as an above-average driver. Furthermore, the majority of the group is likely to rate themselves as above average. Research has found this effect in many different areas of human performance and has even generalized it beyond individuals' attempts to draw comparisons involving themselves. Findings of this research therefore suggest that rather than individuals evaluating themselves as above average in a self-serving manner, the better-than-average effect is actually due to a general tendency to evaluate any single person or object as better than average.

The better-than-average effect may not have wholly social origins—judgments about inanimate objects suffer similar distortions.

The degree to which people view themselves as more desirable than the average person links to reduced activation in their orbitofrontal cortex and dorsal anterior cingulate cortex. This is suggested to link to the role of these areas in processing "cognitive control".

Illusory superiority has been found in individuals' comparisons of themselves with others in a variety of aspects of life, including performance in academic circumstances (such as class performance, exams and overall intelligence), in working environments (for example in job performance), and in social settings (for example in estimating one's popularity, or the extent to which one possesses desirable personality traits, such as honesty or confidence), and in everyday abilities requiring particular skill.
For illusory superiority to be demonstrated by social comparison, two logical hurdles have to be overcome. One is the ambiguity of the word "average". It is logically possible for nearly all of the set to be above the mean if the distribution of abilities is highly skewed. For example, the mean number of legs per human being is slightly lower than two because some people have fewer than two and almost none have more. Hence experiments usually compare subjects to the median of the peer group, since by definition it is impossible for a majority to exceed the median.
A further problem in inferring inconsistency is that subjects might interpret the question in different ways, so it is logically possible that a majority of them are, for example, more generous than the rest of the group each on "their own understanding" of generosity. This interpretation is confirmed by experiments which varied the amount of interpretive freedom. As subjects evaluated themselves on a specific, well-defined attribute, illusory superiority remains.

In a survey of faculty at the University of Nebraska–Lincoln, 68% rated themselves in the top 25% for teaching ability, and 94% rated themselves as above average.
In a similar survey, 87% of Master of Business Administration students at Stanford University rated their academic performance as above the median.
Illusory superiority has also explained phenomena such as the large amount of stock market trading (as each trader thinks they are the best, and most likely to succeed), and the number of lawsuits that go to trial (because, due to illusory superiority, many lawyers have an inflated belief that they will win a case).

In Kruger and Dunning's experiments, participants were given specific tasks (such as solving logic problems, analyzing grammar questions, and determining whether jokes were funny), and were asked to evaluate their performance on these tasks relative to the rest of the group, enabling a direct comparison of their actual and perceived performance.
Results were divided into four groups depending on actual performance and it was found that all four groups evaluated their performance as above average, meaning that the lowest-scoring group (the bottom 25%) showed a very large illusory superiority bias. The researchers attributed this to the fact that the individuals who were worst at performing the tasks were also worst at recognizing skill in those tasks. This was supported by the fact that, given training, the worst subjects improved their estimate of their rank as well as getting better at the tasks. The paper, titled "Unskilled and Unaware of It: How Difficulties in Recognizing One's Own Incompetence Lead to Inflated Self-Assessments", won an Ig Nobel Prize in 2000.
In 2003, Dunning and Joyce Ehrlinger, also of Cornell University, published a study that detailed a shift in people's views of themselves influenced by external cues. Cornell undergraduates were given tests of their knowledge of geography, some intended to positively affect their self-views, others intended to affect them negatively. They were then asked to rate their performance, and those given the positive tests reported significantly better performance than those given the negative.
Daniel Ames and Lara Kammrath extended this work to sensitivity to others, and the subjects' perception of how sensitive they were. Research by Burson, Larrick, and Klayman suggests that the effect is not so obvious and may be due to noise and bias levels.
Dunning, Kruger, and coauthors' 2008 paper on this subject comes to qualitatively similar conclusions after making some attempt to test alternative explanations.

Svenson (1981) surveyed 161 students in Sweden and the United States, asking them to compare their driving skills and safety to other people's. For driving skills, 93% of the U.S. sample and 69% of the Swedish sample put themselves in the top 50%; for safety, 88% of the U.S. and 77% of the Swedish put themselves in the top 50%.
McCormick, Walkey and Green (1986) found similar results in their study, asking 178 participants to evaluate their position on eight different dimensions of driving skills (examples include the "dangerous–safe" dimension and the "considerate–inconsiderate" dimension). Only a small minority rated themselves as below the median, and when all eight dimensions were considered together it was found that almost 80% of participants had evaluated themselves as being an above-average driver.
One commercial survey showed that 36% of drivers believed they were an above-average driver while texting or sending emails compared to other drivers; 44% considered themselves average, and 18% below average.

Subjects describe themselves in positive terms compared to other people, and this includes describing themselves as less susceptible to bias than other people. This effect is called the "bias blind spot" and has been demonstrated independently.

Illusory superiority that applies to IQ is known as the "Downing effect". This describes the tendency of people with a below-average IQ to overestimate their IQ, and of people with an above-average IQ to underestimate their IQ (similar trend to the Dunning-Kruger effect). This tendency was first observed by C. L. Downing, who conducted the first cross-cultural studies on perceived intelligence. His studies also showed that the ability to accurately estimate other people's IQs was proportional to one's own IQ (i.e., the lower the IQ, the less capable of accurately appraising other people's IQs). People with high IQs are better overall at appraising other people's IQs, but when asked about the IQs of people with similar IQs as themselves, they are likely to rate them as having higher IQs.
The disparity between actual IQ and perceived IQ has also been noted between genders by British psychologist Adrian Furnham, in whose work there was a suggestion that, on average, men are more likely to overestimate their intelligence by 5 points, while women are more likely to underestimate their IQ by a similar margin.

In Zuckerman and Jost's study, participants were given detailed questionnaires about their friendships and asked to assess their own popularity. Using social network analysis, they were able to show that participants generally had exaggerated perceptions of their own popularity, especially in comparison to their own friends.
Despite the fact that most people in the study believed that they had more friends than their friends, a 1991 study by sociologist Scott L. Feld on the friendship paradox shows that on average, due to sampling bias, most people have fewer friends than their friends have.

Researchers have also found illusory superiority in relationship satisfaction. For example, one study found that participants perceived their own relationships as better than others' relationships on average, but thought that the majority of people were happy with their relationships. It also found evidence that the higher the participants rated their own relationship happiness, the more superior they believed their relationship was—illusory superiority also increased their own relationship satisfaction. This effect was pronounced in men, whose satisfaction was especially related to the perception that one's own relationship was superior as well as to the assumption that few others were unhappy in their relationships. On the other hand, women's satisfaction was particularly related to the assumption that most people were happy with their relationship. One study found that participants became defensive when their spouse or partner were perceived by others to be more successful in any aspect of their life, and had the tendency to exaggerate their success and understate their spouse or partner's success.

One of the first studies that found illusory superiority was carried out in the United States by the College Board in 1976. A survey was attached to the SAT exams (taken by one million students annually), asking the students to rate themselves relative to the median of the sample (rather than the average peer) on a number of vague positive characteristics. In ratings of leadership, 70% of the students put themselves above the median. In ability to get on well with others, 85% put themselves above the median; 25% rated themselves in the top 1%.
A 2002 study on illusory superiority in social settings, with participants comparing themselves to friends and other peers on positive characteristics (such as punctuality and sensitivity) and negative characteristics (such as naivety or inconsistency). This study found that participants rated themselves more favorably than their friends, but rated their friends more favorably than other peers (but there were several moderating factors).
Research by Perloff and Fetzer, Brown, and Henri Tajfel and John C. Turner also found friends being rated higher than other peers. Tajfel and Turner attributed this to an "ingroup bias" and suggested that this was motivated by the individual's desire for a "positive social identity".

While illusory superiority has been found to be somewhat self-serving, this does not mean that it will predictably occur—it is not constant. The strength of the effect is moderated by many factors, the main examples of which have been summarized by Alicke and Govorun (2005).

The idea that ambiguity moderates illusory superiority has empirical research support from a study involving two conditions: in one, participants were given criteria for assessing a trait as ambiguous or unambiguous, and in the other participants were free to assess the traits according to their own criteria. It was found that the effect of illusory superiority was greater in the condition where participants were free to assess the traits.
The effects of illusory superiority have also been found to be strongest when people rate themselves on abilities at which they are totally incompetent. These subjects have the greatest disparity between their actual performance (at the low end of the distribution) and their self-rating (placing themselves above average). This Dunning–Kruger effect is interpreted as a lack of metacognitive ability to recognize their own incompetence.

The method used in research into illusory superiority has been found to have an implication on the strength of the effect found. Most studies into illusory superiority involve a comparison between an individual and an average peer, of which there are two methods: direct comparison and indirect comparison. A direct comparison—which is more commonly used—involves the participant rating themselves and the average peer on the same scale, from "below average" to "above average" and results in participants being far more self-serving. Researchers have suggested that this occurs due to the closer comparison between the individual and the average peer, however use of this method means that it is impossible to know whether a participant has overestimated themselves, underestimated the average peer, or both.
The indirect method of comparison involves participants rating themselves and the average peer on separate scales and the illusory superiority effect is found by taking the average peer score away from the individual's score (with a higher score indicating a greater effect). While the indirect comparison method is used less often it is more informative in terms of whether participants have overestimated themselves or underestimated the average peer, and can therefore provide more information about the nature of illusory superiority.

The nature of the comparison target is one of the most fundamental moderating factors of the effect of illusory superiority, and there are two main issues relating to the comparison target that need to be considered.
First, research into illusory superiority is distinct in terms of the comparison target because an individual compares themselves with a hypothetical average peer rather than a tangible person. Alicke et al. (1995) found that the effect of illusory superiority was still present but was significantly reduced when participants compared themselves with real people (also participants in the experiment, who were seated in the same room), as opposed to when participants compared themselves with an average peer. This suggests that research into illusory superiority may itself be biasing results and finding a greater effect than would actually occur in real life.
Further research into the differences between comparison targets involved four conditions where participants were at varying proximity to an interview with the comparison target: watching live in the same room; watching on tape; reading a written transcript; or making self-other comparisons with an average peer. It was found that when the participant was further removed from the interview situation (in the tape observation and transcript conditions) the effect of illusory superiority was found to be greater. Researchers asserted that these findings suggest that the effect of illusory superiority is reduced by two main factors—individuation of the target and live contact with the target.
Second, Alicke et al.'s (1995) studies investigated whether the negative connotations to the word "average" may have an effect on the extent to which individuals exhibit illusory superiority, namely whether the use of the word "average" increases illusory superiority. Participants were asked to evaluate themselves, the average peer and a person whom they had sat next to in the previous experiment, on various dimensions. It was found that they placed themselves highest, followed by the real person, followed by the average peer, however the average peer was consistently placed above the mean point on the scale, suggesting that the word "average" did not have a negative effect on the participant's view of the average peer.

Personality characteristics vary widely between people and have been found to moderate the effects of illusory superiority, one of the main examples of this is self-esteem. Brown (1986) found that in self-evaluations of positive characteristics participants with higher self-esteem showed greater illusory superiority bias than participants with lower self-esteem. Additionally, another study found that participants pre-classified as having high self-esteem tended to interpret ambiguous traits in a self-serving way, whereas participants pre-classified as having low self-esteem did not do this.

Psychology has traditionally assumed that generally accurate self-perceptions are essential to good mental health. This was challenged by a 1988 paper by Taylor and Brown, who argued that mentally healthy individuals typically manifest three cognitive illusions—illusory superiority, illusion of control, and optimism bias. This idea rapidly became very influential, with some authorities concluding that it would be therapeutic to deliberately induce these biases. Since then, further research has both undermined that conclusion and offered new evidence associating illusory superiority with negative effects on the individual.
One line of argument was that in the Taylor and Brown paper, the classification of people as mentally healthy or unhealthy was based on self-reports rather than objective criteria.  People prone to self-enhancement would exaggerate how well-adjusted they are. One study claimed that "mentally normal" groups were contaminated by "defensive deniers", who are the most subject to positive illusions. A longitudinal study found that self-enhancement biases were associated with poor social skills and psychological maladjustment. In a separate experiment where videotaped conversations between men and women were rated by independent observers, self-enhancing individuals were more likely to show socially problematic behaviors such as hostility or irritability. A 2007 study found that self-enhancement biases were associated with psychological benefits (such as subjective well-being) but also inter- and intra-personal costs (such as anti-social behavior).

In contrast to what is commonly believed, research has found that better-than-average effects are not universal. In fact, much recent research has found the opposite effect in many tasks, especially if they were more difficult.

Illusory superiority's relationship with self-esteem is uncertain. The theory that those with high self-esteem maintain this high level by rating themselves highly is not without merit—studies involving non-depressed college students found that they thought they had more control over positive outcomes compared to their peers, even when controlling for performance. Non-depressed students also actively rate peers below themselves as opposed to rating themselves higher. Students were able to recall a great deal more negative personality traits about others than about themselves.
In these studies there was no distinction made between people with legitimate and illegitimate high self-esteem, as other studies have found that absence of positive illusions  mainly coexist with high self-esteem and that determined individuals bent on growth and learning are less prone to these illusions. Thus it may be that while illusory superiority is associated with undeserved high self-esteem, people with legitimate high self-esteem do not necessarily exhibit it.

Normalcy bias, or normality bias, is a cognitive bias which leads people to disbelieve or minimize threat warnings. Consequently, individuals underestimate the likelihood of a disaster, when it might affect them, and its potential adverse effects. The normalcy bias causes many people to prepare inadequately for natural disasters, market crashes, and calamities caused by human error. About 80% of people reportedly display normalcy bias during a disaster.
The normalcy bias can manifest in response to warnings about disasters and actual catastrophes. Such events can range in scale from incidents such as traffic collisions to global catastrophic risk. The event may involve socially constructed phenomena such as loss of money in market crashes, or direct threats to continuity of life: as in natural disasters like a tsunami or violence in war.
Normalcy bias has also been called analysis paralysis, the ostrich effect, and by first responders, the negative panic. The opposite of normalcy bias is overreaction, or worst-case scenario bias, in which small deviations from normality are dealt with as signals of an impending catastrophe.

Amanda Ripley, author of The Unthinkable: Who Survives When Disaster Strikes – and Why, identifies common response patterns of people in disasters and explains that there are three phases of response: "denial, deliberation, and the decisive moment". With regard to the first phase, described as "denial", Ripley found that people were likely to deny that a disaster was happening. It takes time for the brain to process information and recognize that a disaster is a threat. In the "deliberation" phase, people have to decide what to do. If a person does not have a plan in place, this causes a serious problem because the effects of life-threatening stress on the body (e.g. tunnel vision, audio exclusion, time dilations, out-of-body experiences, or reduced motor skills) limit an individual's ability to perceive information and make plans. Ripley asserts that in the third and final phase, described as the "decisive moment", a person must act quickly and decisively. Failure to do so can result in injury or death. She explains that the faster someone can get through the denial and deliberation phases, the quicker they will reach the decisive moment and begin to take action.

Journalist David McRaney wrote that "Normalcy bias flows into the brain no matter the scale of the problem. It will appear whether you have days and plenty of warning or are blindsided with only seconds between life and death." It can manifest itself in phenomena such as car crashes. Car crashes occur very frequently, but the average individual experiences them only rarely, if ever. It also manifests itself in connection with events in world history. According to a 2001 study by sociologist Thomas Drabek, when people are asked to leave in anticipation of a disaster, most check with four or more sources of information before deciding what to do. The process of checking in, known as milling, is common in disasters.
It can explain why thousands of people refused to leave New Orleans as Hurricane Katrina approached and why at least 70% of 9/11 survivors spoke with others before evacuating. Officials at the White Star Line made insufficient preparations to evacuate passengers on the Titanic and people refused evacuation orders, possibly because they underestimated the odds of a worst-case scenario and minimized its potential impact. Similarly, experts connected with the Fukushima nuclear power plant were strongly convinced that a multiple reactor meltdown could never occur.
A website for police officers has noted that members of that profession have "all seen videos of officers who were injured or killed while dealing with an ambiguous situation, like the old one of a father with his young daughter on a traffic stop". In the video referred to, "the officer misses multiple threat cues...because the assailant talks lovingly about his daughter and jokes about how packed his minivan is. The officer only seems to react to the positive interactions, while seeming to ignore the negative signals. It's almost as if the officer is thinking, 'Well I've never been brutally assaulted before so it certainly won't happen now.' No one is surprised at the end of the video when the officer is violently attacked, unable to put up an effective defense." This professional failure, notes the website, is a consequence of normalcy bias.
Normalcy bias, David McRaney has written, "is often factored into fatality predictions in everything from ship sinkings to stadium evacuations". Disaster movies, he adds, "get it all wrong. When you and others are warned of danger, you don't evacuate immediately while screaming and flailing your arms." McRaney notes that in the book Big Weather, tornado chaser Mark Svenvold discusses "how contagious normalcy bias can be. He recalled how people often tried to convince him to chill out while fleeing from impending doom. Even when tornado warnings were issued, people assumed it was someone else's problem. Stake-holding peers, he said, would try to shame him into denial so they could remain calm. They didn't want him deflating their attempts at feeling normal".

The normalcy bias may be caused in part by the way the brain processes new data. Research suggests that even when the brain is calm, it takes 8–10 seconds to process new information. Stress slows the process, and when the brain cannot find an acceptable response to a situation, it fixates on a single and sometimes default solution that may or may not be correct. An evolutionary reason for this response could be that paralysis gives an animal a better chance of surviving an attack and predators are less likely to see prey that is not moving.

About 80% of people reportedly display normalcy bias in disasters. Normalcy bias has been described as "one of the most dangerous biases we have". The lack of preparation for disasters often leads to inadequate shelter, supplies, and evacuation plans. Even when all these things are in place, individuals with a normalcy bias often refuse to leave their homes.
Normalcy bias can cause people to drastically underestimate the effects of the disaster. Therefore, people think that they will be safe even though information from the radio, television, or neighbors gives them reasons to believe there is a risk. The normalcy bias causes a cognitive dissonance that people then must work to eliminate. Some manage to eliminate it by refusing to believe new warnings coming in and refusing to evacuate (maintaining the normalcy bias), while others eliminate the dissonance by escaping the danger. The possibility that some people may refuse to evacuate causes significant problems in disaster planning.

The negative effects of normalcy bias can be combated through the four stages of disaster response:
preparation, including publicly acknowledging the possibility of disaster and forming contingency plans.
warning, including issuing clear, unambiguous, and frequent warnings and helping the public to understand and believe them.
impact, the stage at which the contingency plans take effect and emergency services, rescue teams, and disaster relief teams work in tandem.
aftermath, reestablishing equilibrium after the fact, by providing both supplies and aid to those in need.

The affect heuristic is a  heuristic, a mental shortcut that allows people to make decisions and solve problems quickly and efficiently, in which current emotion—fear, pleasure, surprise, etc.—influences decisions. In other words, it is a type of heuristic in which emotional response, or "affect" in psychological terms, plays a lead role.  It is a subconscious process that shortens the decision-making process and allows people to function without having to complete an extensive search for information. It is shorter in duration than a mood, occurring rapidly and involuntarily in response to a stimulus. Reading the words "lung cancer" usually generates an affect of dread, while reading the words "mother's love" usually generates a feeling of affection and comfort.  The affect heuristic is typically used while judging the risks and benefits of something, depending on the positive or negative feelings that people associate with a stimulus. It is the equivalent of "going with your gut". If their feelings towards an activity are positive, then people are more likely to judge the risks as low and the benefits high. On the other hand, if their feelings towards an activity are negative, they are more likely to perceive the risks as high and benefits low.

The theory of affect heuristic is that a human being's affect can influence how he or she makes decisions. Research has shown that risk and benefits are negatively correlated in people's minds. This was found after researchers found that the inverse relationship between perceived risk and perceived benefit of an activity was linked to the strength of positive or negative affect associated with the activity as measured by rating the activity on bipolar scales (e.g. good/bad). This implies that people base their judgements of an activity or a technology not only on what they think about it, but also on how they feel about it.  The affect heuristic gained early attention in 1980 when Robert B. Zajonc argued that affective reactions to stimuli are often the first reaction which occur automatically and subsequently influencing the way in which we process and judge information. The affect heuristic received more recent attention when it was used to explain the unexpected negative correlation between benefit and risk perception. Finucane, Alhakami, Slovic and Johnson theorized in 2000 that a good feeling towards a situation (i.e., positive affect) would lead to a lower risk perception and a higher benefit perception, even when this is logically not warranted for that situation. This implies that a strong emotional response to a word or other stimulus might alter a person's judgment. He or she might make different decisions based on the same set of facts and might thus make an illogical decision. Overall, the affect heuristic is of influence in nearly every decision-making arena.

An alternative thought to the “gut feeling” response is Antonio Damasio's somatic marker hypothesis. It is the opinion that thought is made largely from images which include perceptual and symbolic representations. These images then become “marked” by positive or negative feelings linked directly or indirectly to somatic states. When a negative somatic marker is linked to an image of a future outcome, it sounds an alarm in the brain. When a positive marker is linked to an image, it becomes a signal of incentive. He hypothesized that somatic markers increase the accuracy of the decision process and the absence of these markers, mostly seen in people with certain types of brain damage, degrades the ability to make good decisions. This hypothesis arose when observing patients with damage to their prefrontal cortex who had severe impairments in personal and social decision-making despite their other abilities.

It has been argued by researchers that people use affect heuristics as a first response to an issue, they rely on spontaneous affective reactions which make it more efficient than having to research and analyze external information. Slovic, Finucane, Peters and MacGregor (2005) contrast two modes of thinking: the analytic system and the experiential system. The analytic system, also referred to as the rational system, is thought that is considered to be slow and requires effort; it requires consciousness, probabilities, logical reasoning, and substantial evidence. The experiential system is the exact opposite. It is intuitive and mostly automatic which makes it more convenient for people because it does not require effort or consciousness. It relies on images, metaphors, and narratives which are then used to estimate the probability of a hazard. This is due to the experience of affect, in other words, a “gut feeling.” Multiple studies including the one done by Miller and Ireland (2005) show how "gut feeling" or intuitive decisions affect various executives and managers of many companies. Many of the individuals studied use intuition as an effective approach to making important decisions. The experimenters' goal is to evaluate the risk and benefits of using intuition. Their results show that this is a troublesome decision tool. Affective reactions that accompany judgements are not necessarily voluntary, but are automatic responses. Zajonc states that “one might be able to control the expression of emotion, but not the experience of it itself.”  However, he also clarifies that feelings are not free of thought and that thoughts are not free of feeling. The experiential system also takes past experiences into account. In other words, if a person has already experienced a certain issue, he or she is more likely to take more precautions towards the issue.

Many studies have been done to further look into affect heuristics and many have found that these heuristics shape our attitudes and opinions towards our decisions, especially risk perception. These studies demonstrate how affect is an important characteristic of the decision-making process in many different domains and aspects as well as how it can lead to a strong conditioner of preference. As demonstrated below, affect is independent of cognition which indicate that there are conditions where affect does not require cognition.

The cause of affect does not necessarily have to be consciously perceived. A study conducted by Winkielman, Zajonc and Schwarz (1997) demonstrated the speed at which an affective reaction can influence judgements. To do this they used a subliminal priming paradigm where participants were "primed" through exposure to either a smiling face, a frowning face, or a neutral polygon presented at about 1⁄250  of a second. This was considered an amount of time where the nature of the stimuli could not be recalled. Participants were then exposed to an ideograph (e.g. a Chinese character) for two seconds and asked to rate the ideograph on a scale of liking. Researchers found that participants preferred the ideograph preceded with a smiling face as opposed to those preceded by a frowning face or neutral polygon despite the fact that the smiling face was only shown for 1⁄250 of a second.
The same experiment demonstrated the persistence of initial affect. During a second session, participations were primed with the same characters, but these characters were preceded by a different face that they were not previously exposed to (e.g. those previously exposed to the smiling face were now exposed to the neutral polygon). Participants continued to show preference for the characters based on the first association, even though the second exposure was preceded by a different affective stimulus.  In other words, the second priming was ineffective because the effects of the first priming still remained. If the participant liked a character following exposure to a smiling face, they would continue to like the character even when it was preceded by a frowning face during the second exposure. (The experimental outcome was statistically significant and adjusted for variables such as non-affective preference for certain characters).

Sometimes affective responses to certain stimuli are a result of a lack of sensitivity to other factors, for example, numbers. Slovic and Peters (2006) did a study on psychophysical numbing, the inability to discriminate change in a physical stimulus as the magnitude of the stimulus increases, and found that students more strongly supported an airport-safety measure that was expected to save a  high percentage of 150 lives at risk as opposed to a measure that was expected to save 150 lives. This is thought to have occurred because although saving 150 lives is good, it is somewhat harder to comprehend and thus the decision comes from the positive feeling associated with the higher percentage.

Research has been conducted in the influence that time plays in decision-making. In two experiments, Finucane, Alhakami, Slovic and Johnson (2000) studied the affect heuristic under time pressure and the influence that providing risk and benefit information has on the affect heuristic. The researchers compared individuals under no time pressure and those with time pressure. They predicted that individuals under time pressure would rely more heavily on their affect in order to be more efficient in their responses whereas those under no time pressure would use more logic in their decision-making. To do this, university students were randomly assigned to one of the two conditions (time pressure or no time pressure) and one of the two counterbalancing orders (risk judgements followed by benefit judgements or vice versa). They were then given a task in which they had to make judgements about the risk or benefit of certain activities and technologies. As predicted, individuals in the time-pressure condition took less time to make risk judgements than did individuals in the no time pressure condition. In the second experiment, students again had to make judgements about certain activities, but this time were given additional information about the risk and benefits. Information was framed as being high risk, low risk, high benefit or low benefit. The researchers found that this additional information did in fact influence their judgements.
Two similar studies were conducted by Wilson and Arvai in 2006, in which they also looked at the affect heuristic affects high and low risk options. These experiments examine the affect heuristic and the “evaluability hypothesis”, the joint evaluation when options are evaluated in a side-by-side comparison and separate evaluation where options are evaluated on their own. They take this concept and discuss how it relates to the affect heuristic by specifically looking at making traits of an option more or less meaningful in terms of the context of choice, more specifically, affect. To examine this relationship more closely, they conducted two experiments where participants received quantitative information about the nature of risks and were placed in one of two groups: affect-poor combined with high risks and affect-rich combined with low risks. In their first study, they looked how the influence of affect on evaluability in joint evaluations as compared to separate evaluations. To this, participants were asked to make choices about the affect-rich problem of crime and the affect-poor problem of deer overpopulation. Participants were asked to rate how they perceived crime and deer overpopulation by rating  on a scale from "very good" to "very bad." They found that participants ignored the quantitative information and focused on the affect characteristics.

Health campaigns often use “fear appeals” to grab the attention of their audience.  Fear appeals are a type of advertising that specifically uses methods of creating anxiety in the consumer which results in the consumer wanting to cure this fear by purchasing the product. In a study by Averbeck, Jones, and Robertson (2011), researchers look at how prior knowledge influences one's response to fear appeals. Surveys were distributed which manipulated prior knowledge as low or high and two different topics: sleep deprivation or spinal meningitis. Various scale were used to test how prior knowledge affects certain health-related issues. Researchers found that individuals who had prior knowledge in a certain subject exhibited less fear and were least likely to fall prey to the affect heuristic as opposed to individuals that did not have prior knowledge who exhibited more fear and were more likely to fall prey.
Another example of how fear appeals are used in marketing today is through the findings presented in the experiment by Schmitt and Blass (2008). They produced two versions of an anti-smoking film. One contained high fear arousal and one did not. Out of the participants (46 non-smoking students and 5 smoking students), those who viewed the high fear-arousal version expressed stronger anti-smoking behavioral intentions than those who viewed the low fear-arousal version.

Research has shown that Americans are aware of climate change, but do not consider it to be a serious problem due to the lack of an affective response. Many people report as not having experienced the consequences of climate change or that it is a long-term consequence that will not happen in the near future. Therefore, it is considered to be of lower priority and not much is done as a solution to global climate change.

Research on the affect heuristic had its origin in risk perception. Communicating risk is meant to improve the correspondence between the magnitude of the risk of an issue and the magnitude to which people respond to that risk. Affect, specifically negative affect, is an important method for increasing perceived risk considering its influences on perceived risk and thus has been utilized as essential for communicating risk to the public.
Raising risk awareness is thought to be increased when risk information is presented in the form of frequences (e.g. “Within 40 years there is a 33% probability of flood”) or probabilities (e.g. “Each year there is a 1% probability of flood). This method is thought to evoke an affective response which then increases the availability of risk which results in greater perceived risk. This demonstrates how the way in which information is presented influences the way in which people interpret the information, more specifically, potential risks. Research also shows that people's financial risk taking is affected by their emotional state,
The affect heuristic is certainly evident in product innovations we see in the market. The processes consumers use to weigh the potential risk and benefits associated with purchasing such innovations are in constant motion. A study by Slovic and King (2014) tries to explain this specific phenomenon. Their experiment addresses the extent to which feelings dominate early perceptions of new products. Participants were exposed to three innovations in pretest and posttest design. Through this study, they concluded that risks and benefits associated with innovations are related to the consumer's evaluations of the products.

Researchers have looked at the affective and experiential modes of thinking in terms of cancer prevention. Research has shown that affect plays a significant role in whether people choose to get screened for certain types of cancer. Current research is now looking into how to communicate the risks and benefits of cancer prevention and treatment options. So far research has shown that the way in which information is framed does play a role in the way in which the information is interpreted. Research has also shown that treatment options may not have significant meaning to patients unless it has an affective connection. It is for this reason that researchers are looking into using affective coding such as icon arrays to make numerical information easier to understand and process.

An experiment composed by Hine and Marks (2007) examines the role of affect heuristics in maintaining wood-burning behavior. The individuals analyzed in this study were 256 residents of a small Australian city where high levels of wood smoke pollution are present. With the negative effects of air pollution evident, their studies found that individuals who used wood heaters exhibited less support for wood smoke control policies. These individuals were aware that their wood heaters were part of the problem. Even with that awareness, their positive affections and emotions towards wood heating trumped all negative evidence for it.

Research has been done on how smiling can cause affective responses and thus influence our opinions of others. An experiment by LaFrance and Hecht (1995) investigated  whether a smiling target would elicit more leniency than those that do not. Participants judged a case of potential academic misconduct and were asked to rate a list of subjects. Materials included photos of a female target either showing a neutral expression, felt smile, false smile, or miserable smile. Researchers found that the student pictured as smiling received less punishment than did the student who did not smile despite the fact that the smiling student was not seen as less guilty. They did not find a significant difference between the different smiles. Smiling students were also rated as more trustworthy, honest, genuine, good, obedient, sincere, and admirable compared to the student that did not smile.
To the previous studies evidence, there is further evidence on the effect of smiling on a person’s perception. They contain it in the experiment by Delevati and Cesar (1994). Brazilian undergraduates perceived a slide of a male and female person. Smiling faces were portrayed and non-smiling faces were portrayed. The participants used 12 different adjectives to judge the portraits. Results showed those persons showing a smile received more favorable perceptions than those who did not. Generally speaking, a smiling person can produce warmer feelings in the perceiver than the non-smiling person.

Researchers have studied how one's memory load increases one's chances of using the affect heuristic. In a study by Shiv and Fedorikhin (1999), participants were asked to either memorize a two-digit number (low cognitive demand) or a seven-digit number (high cognitive demand). Participants were then asked to enter another room where they would report their number. On the way there, they were asked for their preference for two snacks: chocolate cake (more favorable affect, less favorable cognition) or fruit salad (less favorable affect, more favorable cognition). Researchers predicted that participants given the seven-digits to remember (high cognitive load) would reduce their deliberation process due to having to remember a large amount of information. This would increase the chances of these participants choosing the cake over the fruit salad due to it being the more affectively favorable option. This hypothesis proved true with participants choosing the chocolate cake 63% of the time when given a high cognitive load and only 41% when given a low cognitive load. In the same study they also tested the impulsiveness of the participants in moderating the effects of processing-resources of choice and at the time they were asked for their preference for the two snacks high cognitive demand chose the chocolate cake 84.2%. This provides evidence that people's decisions  can be influenced by affect heuristic in a relatively spontaneous manner from the stimulus, with little involvement of higher-order cognitive demand.

Another common situation involving affect heuristic is where a strong, emotional first impression can inform a decision, even if subsequent evidence weight cognitively against the original decision made. In a study by Sherman, Kim and Zajonc (1998), they investigated how long the induced effects of an affective response could last. Participants were asked to study Chinese characters and their English meanings. Half of the meanings were positive (e.g. beauty) and the other half negative (e.g. disease). Participants were then tested on these meanings, which was followed by a task in which they were given pairs of characters and asked to choose which character they preferred. Researchers found that participants preferred the character with a positive meaning.
In the same experiment, participants were given a new task where the characters were presented with a neutral meaning (e.g. linen) and participants were told that these were the true meanings of the character. The testing procedure was the same and despite exposing participants with the new meanings, their preferences in characters remained the same. Characters that were paired with positive meanings continued to be preferred.

While heuristics can be helpful in many situations, it can also lead to biases which can result in poor decision-making habits. Like other heuristics, the affect heuristic can provide efficient and adaptive responses, but relying on affect can also cause decisions to be misleading.

Studies have looked at how the affect influences smoking behavior. Smokers tend to act experientially in the sense that they give little conscious thought to the risks before they start. It is usually as a result of affective responses in the moment that occur when seeing others partake in the behavior. Epstein (1995) found that there has been quite a bit of manipulation of consumers when it comes to packaging and marketing products. This is especially the case with tobacco companies. Research has shown that cigarette advertisements were designed to increase the positive affect associated with smoking and decrease the perceptions of risk. Therefore, seeing this advertisement could lead people astray to start smoking because of its induced appeal. In a study by Slovic et al. (2005), he released a survey to smokers in which he asked “If you had it to do all over again, would you start smoking?” and more than 85% of adult smokers and about 80% of young smokers (between the ages of 14 and 22) answered “No.” He found that most smokers, especially those that start at a younger age, do not take the time and think about how their future selves will perceive the risks associated with smoking. Essentially, smokers give little conscious thought to smoking before they start and it is usually after they have started smoking and have become addicted that they learn new information about health risk.

The hot hand (also known as the hot hand phenomenon or hot hand fallacy) is the putative tendency for an athlete to have streaks of success higher than their average performance. The concept is often applied to sports and skill-based tasks in general and originates from basketball, where a shooter is assumed to be more likely to score if their previous attempts were successful – i.e., while having the "hot hand". Researchers for many years did not find evidence for a "hot hand" in practice, dismissing it as fallacious. However, later research questioned whether the belief is indeed a fallacy. Some recent studies using modern statistical analysis have observed evidence for the "hot hand" in some sporting activities; however, other recent studies have not observed evidence of the "hot hand". Moreover, evidence suggests that only a small subset of players may show a "hot hand" and, among those who do, the magnitude (i.e., effect size) of the "hot hand" tends to be small.

The fallacy was first described in a 1985 paper by Thomas Gilovich,  Amos Tversky, and Robert Vallone. The "Hot Hand in Basketball" study questioned the hypothesis that basketball players have "hot hands", which the paper defined as the claim that players are more likely to make a successful shot if their previous shot was successful. The study looked at the inability of respondents to properly understand randomness and random events; much like innumeracy can impair a person's judgement of statistical information, the hot hand fallacy can lead people to form incorrect assumptions regarding random events. The three researchers provide an example in the study regarding the "coin toss"; respondents expected even short sequences of heads and tails to be approximately 50% heads and 50% tails. The study proposed two biases that are created by the kind of thought pattern applied to the coin toss: it could lead an individual to believe that the probability of heads or tails increases after a long sequence of either has occurred (known as the gambler's fallacy); or it could cause an individual to reject randomness due to a belief that a streak of either outcome is not representative of a random sample.
The first study was conducted via a questionnaire of 100 basketball fans from the colleges of Cornell and Stanford. The other looked at the individual records of players from the 1980–81 Philadelphia 76ers. The third study analyzed free-throw data and the fourth study was of a controlled shooting experiment. The reason for the different studies was to gradually eliminate external factors around the shot. For example, in the first study there is the factor of how the opposing team's defensive strategy and shot selection would interfere with the shooter. The second and third take out the element of shot selection, and the fourth eliminates the game setting and the distractions and other external factors mentioned before. The studies primarily found that the outcomes of both field goal and free throw attempts are independent of each other. In the later studies involving the controlled shooting experiment the results were the same; evidently, the researchers concluded that the sense of being "hot" does not predict hits or misses.

Gilovich offers two different explanations for why people believe hot hands exist. The first is that a person may be biased towards looking for streaks before watching a basketball game. This bias would then affect their perceptions and recollection of the game (confirmation bias). The second explanation deals with people's inability to recognize chance sequences. People expect chance sequences to alternate between the options more than they actually do. Chance sequences can seem too lumpy, and are thus dismissed as non-chance (clustering illusion).
There are many proposed explanations for why people are susceptible to the hot-hand fallacy. Alan D. Castel, and others investigated the idea that age would alter an individual's belief in the fallacy. To test this idea researchers conducted a cross-sectional study where they sampled 455 participants ranging in age from 22 to 90 years old. These participants were given a questionnaire preceded by a prompt that said in college and professional basketball games no players make 100% of their attempted shots. Then the questionnaire asked two important questions: (1) Does a basketball player have a better chance of making a shot after having just made the last two or three shots than after having missed the last two or three shots? (2) Is it important to pass the ball to someone who has just made several shots in a row?
The main interest of the questionnaire was to see if a participant answered yes to the first question, implying that they believed in the hot-hand fallacy. The results showed that participants over 70 years of age were twice as likely to believe the fallacy than adults 40–49, confirming that the older individuals relied more on heuristic-based processes. Older adults are more likely to remember positive information, making them more sensitive to gains and less to losses than younger adults.
One study looked at the root of the hot-hand fallacy as being from an inability to appropriately judge sequences. The study compiled research from dozens of behavioral and cognitive studies that examined the hot-hand and gambler's fallacies with random mechanisms and skill-generated streaks. In terms of judging random sequences the general conclusion was that people do not have a statistically correct concept of random. It concluded that human beings are built to see patterns in sensory and conceptual data of all types.

In 2018 Miller and Sanjurjo published a new analysis of the original research of Gilovich, Tversky, and Vallone (GTV) and in contrast concluded that there is "significant evidence of streak shooting". Miller and Sanjurjo concluded that there is indeed a statistical basis for the hot hand phenomenon in the hit pattern of the Philadelphia 76ers.
GTV assumed that there is only evidence of a hot hand if the probability of a hit is higher after a streak of hits than the probability of a hit after a streak of misses. This cannot be observed in the hit pattern of the 76ers. The aforementioned probabilities are not significantly different. Therefore, GTV concluded that there is no sign of a hot hand phenomenon. However, Miller and Sanjurjo show that GTV's assumption is wrong and, in fact, the expected rate of hits after a streak of hits should be lower than the rate of hits after a streak of misses. Thus, an equal rate of hits to misses after a streak is a sign of a hot hand.
Miller and Sanjurjo stated that GTV introduced a sampling bias because they start counting after a series of hits/misses. Miller and Sanjurjo show analytically for a series of one hit (and empirically for bigger streaks) that this introduces a bias towards more misses, given that the number following samples is small enough (e.g. less than 100 for a fair coin). According to Miller and Sanjurjo: "it is incorrect to expect a consistent 50 percent (Bernoulli i.i.d.) shooter who has taken 100 shots to make half of the shots that immediately follow a streak of three hits".

A study reported that a belief in the hot-hand fallacy affects a player's perceptions of success.

More recent research has questioned the earlier findings, instead finding support for the belief of a hot hand phenomenon.
A 2003 paper from researchers at Monash University noted that Gilovich et al. did not examine the statistical power of their own experiments. By performing power analysis on the 1985 data, the researchers concluded that even if the Philadelphia 76ers did shoot in streaks, it is highly unlikely that Gilovich, Vallone and Tversky would have discovered that fact.
A paper from October 2011 by Yaari and Eisenmann, a large dataset of more than 300,000 NBA free throws were found to show "strong evidence" for the "hot hand" phenomenon at the individual level. They analyzed all free throws taken during five regular NBA seasons from 2005 to 2010. They found that there was a significant increase in players' probabilities of hitting the second shot in a two-shot series compared to the first one. They also found that in a set of two consecutive shots, the probability of hitting the second shot is greater following a hit than following a miss on the previous one.
In November 2013, researchers at Stanford University used data from Major League Baseball and found that there was "strong evidence" that the hot hand existed in ten different statistical categories.
In 2014, a paper from three Harvard graduates presented at the Sloan Sports Analytics Conference, which used advanced statistics that for the first time could control for variables in basketball games such as the player's shot location and a defender's position, showed a "small yet significant hot-hand effect."
In 2015, an examination of the 1985 study by Joshua Miller and Adam Sanjurjo found flaws in the methodology of the 1985 study and showed that, in fact, the hot hands may exist. The researchers said that instead it may be attributable to a misapplication of statistical techniques. The authors concluded that people were right to believe that the hot hand exists in basketball.
A 2021 study, using data from NBA Three-Point Contests over the period 1986–2020, found "considerable evidence of hot hand shooting in and across individuals".
However, other recent studies have not observed evidence of the "hot hand". Moreover, evidence suggests that only a small subset of players may show a "hot hand" and, among those who do, the magnitude (i.e., effect size) of the "hot hand" tends to be small.

There are places other than sport that can be affected by the hot-hand fallacy. A study conducted by Joseph Johnson et al. examined the characteristics of an individual's buying and selling behavior as it pertained to the hot hand and gambler's heuristic. Both of these occur when a consumer misunderstands random events in the market and is influenced by a belief that a small sample is able to represent the underlying process. To examine the effect of the hot hand and gambler's heuristic on the buying and selling behaviors of consumers, three hypotheses were made. Hypothesis one stated that consumers that were given stocks with positive and negative trends in earning would be more likely to buy a stock that was positive when it was first getting started but would become less likely to do so as the trend lengthened. Hypothesis two was that consumers would be more likely to sell a stock with negative earnings as the trend length initially increased but would decrease as the trend length increased more. Finally, the third hypothesis was that consumers in the buy condition show stronger preferences for the winning stock over the losing stock than consumers in the sell condition show for the losing stock over the winning stock. A consequence of the third hypothesis is that on average, consumers buy winners and sell losers.
The results of the experiment did not support the first hypothesis but did support hypotheses two and three, suggesting that the use of these heuristics is dependent on buying or selling and the length of the sequence. In summary, buyers for both short and long trends and sellers for short trends would fall under the influence of the hot-hand fallacy. The opposite would be in accordance with the gambler's fallacy which has more of an influence on longer sequences of numerical information.

A study was conducted to examine the difference between the hot-hand and gambler's fallacy. The gambler's fallacy is the expectation of a reversal following a run of one outcome. Gambler's fallacy occurs mostly in cases in which people feel that an event is random, such as rolling a pair of dice on a craps table or spinning the roulette wheel. It is caused by the false belief that the random numbers of a small sample will balance out the way they do in large samples; this is known as the law of small numbers heuristic. The difference between this and the hot-hand fallacy is that with the hot-hand fallacy an individual expects a run to continue. There is a much larger aspect of the hot hand that relies on the individual. This relates to a person's perceived ability to predict random events, which is not possible for truly random events. The fact that people believe that they have this ability is in line with the illusion of control.
In this study, the researchers wanted to test if they could manipulate a coin toss, and counter the gambler's fallacy by having the participant focus on the person tossing the coin. In contrast, they attempted to initiate the hot-hand fallacy by centering the participant's focus on the person tossing the coin as a reason for the streak of either heads or tails. In either case the data should fall in line with sympathetic magic, whereby they feel that they can control the outcomes of random events in ways that defy the laws of physics, such as being "hot" at tossing a specific randomly determined outcome.
They tested this concept under three different conditions. The first was person focused, where the person who tossed the coin mentioned that she was tossing a lot of heads or tails. Second was a coin focus, where the person who tossed the coin mentioned that the coin was coming up with a lot of heads or tails. Finally there was a control condition in which there was nothing said by the person tossing the coin. The participants were also assigned to different groups, one in which the person flipping the coin changed and the other where the person remained the same.
The researchers found the results of this study to match their initial hypothesis that the gambler's fallacy could in fact be countered by the use of the hot hand and people's attention to the person who was actively flipping the coin. It is important to note that this counteraction of the gambler's fallacy only happened if the person tossing the coin remained the same. This study shed light on the idea that the gambler's and hot hand fallacies at times fight for dominance when people try to make predictions about the same event.

The curse of knowledge, also called the curse of expertise or expert's curse, is a cognitive bias that occurs when a person who has specialized knowledge assumes that others share in that knowledge.
For example, in a classroom setting, teachers may struggle if they cannot put themselves in the position of the student. A knowledgeable professor might no longer remember the difficulties that a student faces when learning a new subject. This curse of knowledge also explains the danger behind thinking about student learning based on what seems best to faculty members, as opposed to what has been verified with students.

The term "curse of knowledge" was coined in a 1989 Journal of Political Economy article by economists Colin Camerer, George Loewenstein, and Martin Weber. The aim of their research was to counter the "conventional assumptions in such (economic) analyses of asymmetric information in that better-informed agents can accurately anticipate the judgement of less-informed agents".
Such research drew from Baruch Fischhoff's work in 1975 surrounding hindsight bias, a cognitive bias that knowing the outcome of a certain event makes it seem more predictable. Research conducted by Fischhoff revealed that participants did not know that their outcome knowledge affected their responses, and, if they did know, they could still not ignore or defeat the effects of the bias. Study participants could not accurately reconstruct their previous, less knowledgeable states of mind, which directly relates to the curse of knowledge. This poor reconstruction was theorized by Fischhoff to be because the participant was "anchored in the hindsightful state of mind created by receipt of knowledge". This receipt of knowledge returns to the idea of the curse proposed by Camerer, Loewenstein, and Weber: a knowledgeable person cannot accurately reconstruct what a person, be it themselves or someone else, without the knowledge would think, or how they would act. In his paper, Fischhoff questions the failure to empathize with ourselves in less knowledgeable states, and notes that how well people manage to reconstruct perceptions of lesser informed others is a crucial question for historians and "all human understanding".
This research led the economists Camerer, Loewenstein, and Weber to focus on the economic implications of the concept and question whether the curse harms the allocation of resources in an economic setting. The idea that better-informed parties may suffer losses in a deal or exchange was seen as something important to bring to the sphere of economic theory. Most theoretical analyses of situations where one party knew less than the other focused on how the lesser-informed party attempted to learn more information to minimize information asymmetry. However, in these analyses, there is an assumption that better-informed parties can optimally exploit their information asymmetry when they, in fact, cannot. People cannot utilize their additional, better information, even when they should in a bargaining situation.
For example, two people are bargaining over dividing money or provisions. One party may know the size of the amount being divided while the other does not. However, to fully exploit their advantage, the informed party should make the same offer regardless of the amount of material to be divided. But informed parties actually offer more when the amount to be divided is larger. Informed parties are unable to ignore their better information, even when they should.

A 1990 experiment by a Stanford University graduate student, Elizabeth Newton, illustrated the curse of knowledge in the results of a simple task. A group of subjects were asked to "tap" out well known songs with their fingers, while another group tried to name the melodies. When the "tappers" were asked to predict how many of the "tapped" songs would be recognized by listeners, they would always overestimate. The curse of knowledge is demonstrated here as the "tappers" are so familiar with what they were tapping that they assumed listeners would easily recognize the tune.
Susan Birch and Paul Bloom similarly found a curse of knowledge in a study involving Yale University students. Participants were told one sister (Denise) moved her sister's (Vicki's) violin without Vicki knowing. When participants are told where Denise placed the violin, they are more likely to think Vicki will first look for the violin in that new location. The curse of knowledge was less pronounced when Denise put the violin in a less plausible place. However, a replication study conducted in 2014 found that this finding was not reliably reproducible across seven experiments with large sample sizes, and the true effect size of this phenomenon was less than half of that reported in the original findings. Therefore, it is suggested that "the influence of plausibility on the curse of knowledge in adults appears to be small enough that its impact on real-life perspective-taking may need to be reevaluated."
The curse of knowledge has been found in other domains too. For instance, in data visualisation, when participants are told a background story with a certain graph, they notice the part of the graph that corresponds with this story. They then believe others, who have not heard this background story, will notice the same pattern.
Two potential reason are given for the bias. One is that people who know the answer find it difficult to disregard (inhibit) that information when considering other people's knowledge. The second potential reason is that people are more familiar with the general topic area. In one experiment, students exposed to questions on an earlier date, but not the answers, still estimates more people would know the answers than students who had not seen the answers before.
Related to this finding is the phenomenon experienced by players of charades: the actor may find it frustratingly hard to believe that their teammates keep failing to guess the secret phrase, known only to the actor, conveyed by pantomime.

The curse of knowledge is a difficult bias to correct. For instance, the cognitive bias does not reduce when you tell people about it or ask them to think more about the other's perspective. Financial incentives were equally ineffective in reducing the bias.

In the Camerer, Loewenstein, and Weber article, it is mentioned that the setting closest in structure to the market experiments done would be underwriting, a task in which well-informed experts price goods that are sold to a less-informed public.
Investment bankers value securities, experts taste cheese, store buyers observe jewelry being modeled, and theater owners see movies before they are released. They then sell those goods to a less-informed public. If they suffer from the curse of knowledge, high-quality goods will be overpriced and low-quality goods underpriced relative to optimal, profit-maximizing prices; prices will reflect characteristics (e.g., quality) that are unobservable to uninformed buyers.
The curse of knowledge has a paradoxical effect in these settings. By making better-informed agents think that their knowledge is shared by others, the curse helps alleviate the inefficiencies that result from information asymmetries (a better informed party having an advantage in a bargaining situation), bringing outcomes closer to complete information. In such settings, the curse on individuals may actually improve social welfare ("you get what you pay for").

Economists Camerer, Loewenstein, and Weber first applied the curse of knowledge phenomenon to economics, in order to explain why and how the assumption that better-informed agents can accurately anticipate the judgments of lesser-informed agents is not inherently true. They also sought to support the finding that sales agents who are better informed about their products may, in fact, be at a disadvantage against other, less-informed agents when selling their products. The reason is said to be that better-informed agents fail to ignore the privileged knowledge that they possess and are thus "cursed" and unable to sell their products at a value that more naïve agents would deem acceptable.

The curse of knowledge could contribute to the difficulty of teaching. The curse of knowledge means that it could be potentially ineffective, if not harmful, to think about how students are viewing and learning material by asking the perspective of the teacher as opposed to what has been verified by students. The teacher already has the knowledge that they are trying to impart, but the way that knowledge is conveyed may not be the best for those who do not already possess the knowledge.
The curse of expertise may be counterproductive for learners acquiring new skills. This is important because the predictions of experts can influence educational equity and training  as well as the personal development of young people, not to mention the allocation of time and resources to scientific research and crucial design decisions. Effective teachers must predict the issues and misconceptions that people will face when learning a complex new skill or understanding an unfamiliar concept. This should also encompass the teachers' recognizing their own or each other's bias blind spots.
Decoding the Disciplines is another way of coping with the curse of knowledge in educational settings. It intends to increase student learning by narrowing the gap between expert and novice thinking resulting from the curse of knowledge. The process seeks to make explicit the tacit knowledge of experts and to help students master the mental actions they need for success in particular disciplines.

Two related biases are the false consensus bias and the hindsight bias.
People typically overestimate how many people hold the same opinions as them. This is called the false consensus effect. This is especially true for strongly held opinion. In software design, the mantra "You are not the user" reflects attempts to counter this bias, as software developers may think their experience of a user interface is representative of the final user.
The hindsight bias is the tendency of people to overestimate how well they could have predicted the future, given knowledge of what has happened. It can be seen as a 'special case' of the curse of knowledge, now applied to a past self, rather than to others. What makes it difficult to estimate other people's knowledge may also make it difficult to assess your own prior knowledge.

The difficulty experienced people may encounter is exemplified fictionally by Dr. Watson in discourses with the insightful detective Sherlock Holmes.
The xkcd comic "Average Familiarity" features two geochemists discussing the phenomenon.

In psychology, reactance is an unpleasant motivational reaction to offers, persons, rules, regulations, advice, recommendations, information, and messages that are perceived to threaten or eliminate specific behavioral freedoms. Reactance occurs when an individual feels that an agent is attempting to limit their choice of response or range of alternatives.
Reactance can occur when someone is heavily pressured into accepting a certain view or attitude. Reactance can encourage an individual to adopt or strengthen a view or attitude which is indeed contrary to that which was intended — which is to say, to a response of noncompliance — and can also increase resistance to persuasion. Some individuals might employ reverse psychology in a bid to exploit reactance for their benefit, in an attempt to influence someone to choose the opposite of what is being requested. Reactance can occur when an individual senses that someone is trying to compel them to do something; often the individual will offer resistance and attempt to extricate themselves from the situation.
Some individuals are naturally high in reactance, a personality characteristic called trait reactance.

Psychological reactance is "an unpleasant motivational arousal that emerges when people experience a threat to or loss of their free behaviors." An individual's freedom to select when and how to conduct their behavior, and the level to which they are aware of the relevant freedom—and are able to determine behaviors necessary to satisfy that freedom—affect the generation of psychological reactance. It is assumed that if a person's behavioral freedom is threatened or reduced, they become motivationally aroused. The fear of loss of further freedoms can spark this arousal and motivate them to re-establish the threatened freedom. Because this motivational state is a result of the perceived reduction of one's freedom of action, it is considered a counterforce, and thus is called "psychological reactance".
There are four important elements to reactance theory: perceived freedom, threat to freedom, reactance, and restoration of freedom. Freedom is not an abstract consideration, but rather a feeling associated with real behaviors, including actions, emotions, and attitudes.
Reactance also explains denial as it is encountered in addiction counselling. According to William R. Miller, "Research demonstrates that a counselor can drive resistance (denial) levels up and down dramatically according to his or her personal counseling style". Use of a "respectful, reflective approach" described in motivational interviewing and applied as motivational enhancement therapy, rather than by argumentation, the accusation of "being in denial",  and direct confrontations, lead to the motivation to change and avoid the resistance and denial, or reactance, elicited by strong direct confrontation.

The theory of psychological reactance specifies what is considered a freedom, how said freedom can be taken away or threatened, and how the psychological reactance will manifest itself. Reactance theory aims to understand motive behind behaviors when freedom is threatened or eliminated. In this theory, with the removal of freedom, an individual will attempt to restore said freedom. Reactance in this case is now the manifestation of the behaviors aimed to restore freedom. When the freedom is completely eliminated, reactance becomes maximal, as the lost freedom becomes more desirable. 
Reactance theory assumes there are "free behaviors" individuals perceive and can take part in at any given moment. For a behavior to be free, the individual must have the relevant physical and psychological abilities to partake in it, and must know they can engage in it at the moment, or in the near future.
"Behavior" includes any imaginable act. More specifically, behaviors may be explained as "what one does (or doesn't do)", "how one does something", or "when one does something". It is not always clear, to an observer, or the individuals themselves, if they hold a particular freedom to engage in a given behavior. When a person has such a free behavior they are likely to experience reactance whenever that behavior is restricted, eliminated, or threatened with elimination.
There are several rules associated with free behaviors and reactance:
Other core concepts of the theory are justification and legitimacy. A possible effect of justification is a limitation of the threat to a specific behavior or set of behaviors. For example, if Mr. Doe states that he is interfering with Mrs. Smith's expectations because of an emergency, this keeps Mrs Smith from imagining that Mr. Doe will interfere on future occasions as well. Likewise, legitimacy may point to a set of behaviors threatened since there will be a general assumption that an illegitimate interference with a person's freedom is less likely to occur. With legitimacy there is an additional implication that a person's freedom is equivocal.

In the phenomenology of reactance, there is no assumption that a person will be aware of reactance. When persons become aware of reactance, they will feel a higher level of self-direction in relationship to their own behaviour. In other words, they will feel that if they are able to do what they want, then they do not have to do what they do not want. In this case, when the freedom is in question, that person alone is the director of their own behaviour.
When considering the direct re-establishment of freedom, the greater the magnitude of reactance, the more the individual will try to re-establish the freedom that has been lost or threatened. When a freedom is threatened by a social pressure, then reactance will lead a person to resist that pressure. Also, when there are restraints against a direct re-establishment of freedom, there can be attempts at re-establishment by implication whenever possible.
Freedom can and may be reestablished by a social implication. When an individual has lost free behavior because of a social threat, then the participation in a free-like behavior by a similar person will allow the individual to re-establish their freedom.
Reactance is a motivational state that is aimed at re-establishment of a threatened or eliminated freedom. In short, the level of reactance has a direct relationship with the importance of the eliminated or threatened freedom, and the proportion of free behaviours eliminated or threatened.

A number of studies have looked at psychological reactance, providing empirical evidence for the behaviour; some key studies are discussed below.
Brehm's 1981 study, "Psychological reactance and the attractiveness of unobtainable objects: sex differences in children's responses to an elimination of freedom", examined the differences in sex and age in a child's view of the attractiveness of obtained and unobtainable objects. The study reviewed how well children respond in these situations and determined if the children being observed thought the "grass was greener on the other side". It also determined how well the child made peace with the world if they devalued what they could not have. This work concluded that when a child cannot have what they want, they experience emotional consequences of not getting it.
In this study the results were duplicated from a previous study by Hammock and J. Brehm (1966). The male subjects wanted what they could not obtain, however the female subjects did not conform to the theory of reactance. Although their freedom to choose was taken away, it had no overall effect on them.
Silvia's 2005 study "Deflecting reactance: The role of similarity in increasing compliance and reducing resistance" concluded that one way to increase the activity of a threatened freedom is to censor it, or provide a threatening message toward the activity. In turn a "boomerang effect" occurs, in which people choose forbidden alternatives. This study also shows that social influence has better results when it does not threaten one's core freedoms. Two concepts revealed in this study are that a communicator may be able to increase the positive force towards compliance by increasing their credibility, and that increasing the positive communication force and decreasing the negative communication force simultaneously should increase compliance.
Miller and colleagues concluded in their 2006 study, "Identifying principal risk factors for the initiation of adolescent smoking behaviors: The significance of psychological reactance", that psychological reactance is an important indicator in adolescent smoking initiation. Peer intimacy, peer individuation, and intergenerational individuation are strong predictors of psychological reactance. The overall results of the study indicate that children think that they are capable of making their own decisions, although they are not aware of their own limitations. This is an indicator that adolescents will experience reactance to authoritative control, especially the proscriptions and prescriptions of adult behaviors that they view as hedonically relevant.
Latané and Darley's 1968 study demonstrated how the nonreactance of others can influence an individual's own, even in potentially health-hazardous situations. Male subjects were placed in a room into which smoke was pumped, and the subjects were tested for whether or not they reported the smoke to experimenters. When individually tested, 75% of participants reported smoke coming into the room, compared to 10% when there were two nonreactive others. In the third group, three individuals were grouped together and were generally watching each other for reactions. In this third group, there was a 38% report of smoke. When seeing others nonreactive to the smoke, the individual would interpret the smoke as something not harmful.
A number of studies have also looked into media use and reactance. For instance, an experiment that exposed participants to immigration messages that was threatening (for example via group stereotypes and uncivil language) resulted in very large reactance, in the form of anger and counterarguing.

They formed several conclusions about reactance. Firstly reactance is mostly cognitive; this allows reactance to be measurable by self-report techniques. Also, in support of previous research, they conclude reactance is in part related to an anger response. This verifies Brehm's description that during the reactance experience one tends to have hostile or aggressive feelings, often aimed more at the source of a threatening message than at the message itself. Finally, within reactance, both cognition and affect are intertwined; Dillard and Shen suggest they are so intertwined that their effects on persuasion cannot be distinguished from each other.
Dillard and Shen's research indicates reactance can effectively be studied using established self-report methods. Furthermore, it provided a better understanding of reactance theory and its relationship to persuasive health communication.
Miller and colleagues conducted their 2007 study Psychological reactance and promotional health messages: the effects of controlling language, lexical concreteness, and the restoration of freedom at the University of Oklahoma, with the primary goal being to measure the effects of controlling language in promotional health messages. Their research revisited the notion of restoring freedom by examining the use of a short postscripted message tagged on the end of a promotional health appeal. The results of the study indicated that more concrete messages generate greater attention than less concrete (more abstract) messages. Also, the source of concrete messages can be seen as more credible than the source of abstract messages. They concluded that the use of more concrete, low-controlling language, and the restoration of freedom through the inclusion of a choice-emphasizing postscript, may offer the best solution to reducing ambiguity and reactance created by overtly persuasive health appeals.

The frequency illusion (also known as the Baader–Meinhof phenomenon) is a cognitive bias in which a person notices a specific concept, word, or product more frequently after recently becoming aware of it.
The name "Baader–Meinhof phenomenon" was coined in 1994 by Terry Mullen in a letter to the St. Paul Pioneer Press. The letter describes how, after mentioning the name of the German militant group Baader–Meinhof once, he kept noticing it. This led to other readers sharing their own experiences of the phenomenon, leading it to gain recognition. It was not until 2005, when Stanford linguistics professor Arnold Zwicky wrote about this effect on his blog, that the name "frequency illusion" was coined.

Several possible causes behind frequency illusion have been put forth. However, the consensus seems to be that the main processes behind this illusion are other cognitive biases and attention-related effects that interact with frequency illusion. Zwicky considered this illusion a result of two psychological processes: selective attention and confirmation bias.

The main cause behind frequency illusion, and other related illusions and biases, seems to be selective attention. Selective attention refers to the process of selecting and focusing on selective objects while ignoring distractions. This means that people have the unconscious cognitive ability to filter for what they are focusing on.
Selective attention is always at play whenever frequency illusion occurs. Since selective attention directs focus to the information they are searching for, their experience of frequency illusion will also focus on the same stimuli. The process of frequency illusion is inseparable from selective attention, due to the cause-and-effect relationship between the two, so the "frequent" object, phrase, or idea has to be selective.
This means that a particularly triggering or emotive stimulus could catch someone's attention, possibly more than a mundane task they are preoccupied with.

Confirmation bias is a cognitive bias that always interacts with frequency illusion. This bias refers to the tendency of seeking evidence that confirms one's beliefs or hypotheses, while sometimes overlooking evidence to the contrary. Confirmation bias takes effect in the later stages of selective attention, when the individual has already started noticing the specific stimulus. By focusing on this specific stimulus, the individual notices it more, therefore confirming their suspicions of it occurring more frequently, even though in reality the frequency has not changed. In essence, confirmation bias occurs when the individual affected by frequency illusion starts looking for reassurance of this increased frequency, believing their theories to be confirmed as they focus only on the supporting evidence.

Recency illusion is another selective attention effect that tends to accompany frequency illusion. This illusion occurs when an individual notices something recently, leading them to be convinced that it originated recently as well. This phenomenon amplifies frequency illusion since it leads the person to become more aware of recent stimuli and increases the chances of them focusing on it in the near future. Similar to frequency illusion, recency illusion is also a result of selective attention, and can be overcome by fact-checking.

More relevant to frequency estimations but still a possible cause of the frequency illusion, the split-category effect is the phenomenon in which, when events are split into smaller subcategories, this can increase the predicted frequency of occurrence. An example of this is asking a person to predict the number of dogs in a country or asking them to estimate the number of Beagles, Labradors, Poodles, and French Bulldogs. Based on this effect, the sum of the latter would be larger than the former. The split-category effect could be causing frequency illusion in people – after subcategorizing an object, phrase, or idea, they might be likelier to notice these subcategories, leading them to believe the main category's frequency of occurrence has increased.

The concept of cognitive information processing, including phenomena such as frequency illusion, suggests that regressive frequency judgements arise from discrepancies in cognitive processing. This occurs when stimulus information is not accurately processed or becomes obscured by errors or inconsistencies, leading to reduced variability in how individuals perceive the frequencies of events compared to what is actually observed. Similar to participants in a conditioning experiment learning reinforcement patterns of certain stimuli, individuals become attentive to differences from an equal distribution in frequency. With time, this inefficient learning can distort frequency perception, causing overestimation of less common events and resulting in a flattening of subjective frequency distributions.
Numerous studies have documented the phenomenon of frequency illusion. In a research by Begg et al, two experiments were carried out. The first aimed to investigate how repeating words in different contexts affects frequency estimates, while the second assessed the perceived frequency of different item types that were presented differently at the start. Results showed that frequency estimates are influenced by contexts, especially if they are semantically related, with contextual variety strongly correlating with frequency estimation. The second experiment found that certain factors, like emotions or vivid qualities of items, can lead individuals to overestimate the perception of frequency of occurrences. This research provides empirical evidence for the frequency illusion phenomenon while emphasizing the role of contextual factors and emotional salience in shaping frequency perceptions.

According to the information-loss account, frequency illusions arise due to unsystematic error in processing skewed distributions. This means that people may mistakenly believe that certain events or phenomena happen more often than they actually do because of inaccuracies or biases in how they process information. Specifically, this can lead to a regression effect in accuracy of frequency estimates. This regression effect is more pronounced for smaller sample sizes, resulting in less reliable or accurate estimates of minority statistics and less common occurrences.

Potential misutilization of frequency illusions in problem-solving or diagnosis contexts has been noted by researchers. This cognitive bias can lead individuals to discount rarer causes or events, attributing their perception solely to increased awareness. However, the "Mongoose Phenomenon" challenges conventional views on frequency illusions in decision-making. Instead, it suggests that overlooked events may not be as uncommon as perceived. This highlights the limitations of relying solely on increased awareness to explain perceived frequency.
Moreover, comparisons to Occam's razor versus Hickam's dictum in medicine underscores the need for caution when applying frequency illusions. This encourages a more nuanced and critical approach to decision-making processes to prevent potential harm or oversight that may arise from relying on oversimplified interpretations of frequency illusions.

The natural frequency hypothesis posits that humans are evolutionarily adapted to process information in terms of frequencies rather than single-event probabilities. Proponents argue that this preference for frequency formats stems from evolutionary principles as our ancestors relied on specific event memories to make judgements under uncertainty, as they couldn't inherently observe the probability of individual events. This perspective proposes that human cognition has evolved to analyze counts of specific events, making individuals prone to the frequency illusion and leading them to perceive increased occurrences of recently encountered events. Presenting information in natural frequency formats may mitigate certain cognitive illusions, including frequency illusions, by offering a more accurate understanding of event occurrences.

Frequency illusion is common in the linguistic field. Zwicky, who coined the term frequency illusion, is a linguist himself. He gave the example of how linguists "working on innovative uses of 'all,' especially the quotative use," believed their friends used the quotative "all" in conversation frequently. However, when the linguists actually transcribed these conversations, the number of times they used the quotative "all" was found to be significantly lower compared to their expectations. This is most relevant when commenting on modern linguistic trends such as young people using specific phrases. When the phrases' actual frequency of use in the past is examined, however, it is revealed that they are much more frequent throughout history than initially predicted.
Frequency illusion has also been commonly observed in prescriptive language publications, suggesting that prescriptive authors heavily rely on frequency statements and their alignment with empirical linguistic data. A study empirically investigating the illusion found that frequency statements commonly used in prescriptive publication actually constitute instances of frequency illusions, as proposed by Zwicky. Comparison of statements to linguistic sources such as dictionaries shows that they often don't match actual language usage patterns. Discrepancy between prescriptive language norms and empirical linguistic data highlights the need for increased awareness and scrutiny of language prescriptions advocated in such publications.

In the field of medicine, frequency illusion could help doctors, radiologists, and medical professionals detect diseases. Rare diseases or conditions can often get overlooked by those in the medical field due to an unfamiliarity with the condition. For instance, during the peak of the COVID-19 pandemic, doctors worldwide would observe discoloration of toes in patients and quickly conclude that it was a sign of COVID-19 due to concurrent timing. This is because the skin is considered to reflect underlying health conditions during this period. However, later research revealed lower incidence among the patients, demonstrating a misinterpretation influenced by frequency illusion. 
Medical researchers suggest that based on frequency illusion, medical professionals, especially those in training, could be primed to notice rarer patterns and lesions, which would lead them to detect rare diseases and conditions with higher accuracy. Even in situations where medical professionals are equipped with extensive medical equipments, the ability to recognise a condition lies in their abilities to distinguish the particular medical condition. Therefore, increasing salience of specific rare diseases enables healthcare providers to leverage the frequency illusion, enhancing diagnostic accuracy and patient care.

Frequency illusion is used by the marketing industry to make this cognitive bias work in their favour. Generally, this is achieved by introducing a product through ads and familiarizing consumers with it. As a result of frequency illusion, once the consumer notices the product, they start paying more attention to it. Frequently noticing this product on social media, in conversations, and in real life leads them to believe that the product is more popular – or in more frequent use – than it actually is. Either due to a desire to conform or simply to own the product, the consumer eventually makes the purchase. This phenomenon is a marketing trick that increases the likelihood of the consumer buying the product.

An experimental setup in the lab, simulating an economy and shopping experience for research participants, reveals a tendency in perception biased towards aggregate inflation rates. This phenomenon is notably influenced by the inflation rates of frequently purchased goods. One example of this is an empirical study which found that Swedish women perceived a higher overall rate of inflation than their male counterparts when food price inflation was higher than general inflation. As women are responsible for the major share of the food purchases within Swedish households, this implies a bias formed from frequent exposure to specific price changes. 
Implications of people's tendency to be affected by frequency illusion can greatly influence economic behavior and decision-making which may affect consumption, investment and policy-making decisions.

Presence of frequency illusions have implications in research, wherein this cognitive bias can lead to erroneous conclusions. Researchers may inadvertently draw conclusions regarding broader trends based on limited local data. This tendency can arise when there are significant gaps in sampling coverage, resulting in inaccurate assessments of changes or trends across a larger area or population.

Automation bias is the propensity for humans to favor suggestions from automated decision-making systems and to ignore contradictory information made without automation, even if it is correct. Automation bias stems from the social psychology literature that found a bias in human-human interaction that showed that people assign more positive evaluations to decisions made by humans than to a neutral object. The same type of positivity bias has been found for human-automation interaction, where the automated decisions are rated more positively than neutral. 
This type of bias has become a growing problem for decision making as intensive care units, nuclear power plants, and aircraft cockpits have increasingly integrated computerized system monitors and decision aids to mostly factor out possible human error. Errors of automation bias tend to occur when decision-making is dependent on computers or other automated aids and the human is in an observatory role but able to make decisions. Examples of automation bias range from urgent matters like flying a plane on automatic pilot to such mundane matters as the use of spell-checking programs.

An operator's trust in the system can also lead to different interactions with the system, including system use, misuse, disuse, and abuse.
The tendency toward overreliance on automated aids is known as "automation misuse". Misuse of automation can be seen when a user fails to properly monitor an automated system, or when the automated system is used when it should not be. This is in contrast to disuse, where the user does not properly utilize the automation either by turning it off or ignoring it. Both misuse and disuse can be problematic, but automation bias is directly related to misuse of the automation through either too much trust in the abilities of the system, or defaulting to using heuristics. Misuse can lead to lack of monitoring of the automated system or blind agreement with an automation suggestion, categorized by two types of errors, errors of omission and errors of commission, respectively.
Automation use and disuse can also influence stages of information processing: information acquisition, information analysis, decision making and action selection, and action implementation.
For example, information acquisition, the first step in information processing, is the process by which a user registers input via the senses. An automated engine gauge might assist the user with information acquisition through simple interface features—such as highlighting changes in the engine's performance—thereby directing the user's selective attention. When faced with issues originating from an aircraft, pilots may tend to overtrust an aircraft's engine gauges, losing sight of other possible malfunctions not related to the engine. This attitude is a form of automation complacency and misuse. If, however, the pilot devotes time to interpret the engine gauge, and manipulate the aircraft accordingly, only to discover that the flight turbulence has not changed, the pilot may be inclined to ignore future error recommendations conveyed by an engine gauge—a form of automation complacency leading to disuse.

Automation bias can take the form of commission errors, which occur when users follow an automated directive without taking into account other sources of information. Conversely, omission errors occur when automated devices fail to detect or indicate problems and the user does not notice because they are not properly monitoring the system.
Errors of omission have been shown to result from cognitive vigilance decrements, while errors of commission result from a combination of a failure to take information into account and an excessive trust in the reliability of automated aids. Errors of commission occur for three reasons: (1) overt redirection of attention away from the automated aid; (2) diminished attention to the aid; (3) active discounting of information that counters the aid's recommendations. Omission errors occur when the human decision-maker fails to notice an automation failure, either due to low vigilance or overtrust in the system. For example, a spell-checking program incorrectly marking a word as misspelled and suggesting an alternative would be an error of commission, and a spell-checking program failing to notice a misspelled word would be an error of omission. In these cases, automation bias could be observed by a user accepting the alternative word without consulting a dictionary, or a user not noticing the incorrectly misspelled word and assuming all the words are correct without reviewing the words.
Training that focused on the reduction of automation bias and related problems has been shown to lower the rate of commission errors, but not of omission errors.

The presence of automatic aids, as one source puts it, "diminishes the likelihood that decision makers will either make the cognitive effort to seek other diagnostic information or process all available information in cognitively complex ways." It also renders users more likely to conclude their assessment of a situation too hastily after being prompted by an automatic aid to take a specific course of action.
According to one source, there are three main factors that lead to automation bias. First, the human tendency to choose the least cognitive approach to decision-making, which is called the cognitive miser hypothesis. Second, the tendency of humans to view automated aids as having an analytical ability superior to their own. Third, the tendency of humans to reduce their own effort when sharing tasks, either with another person or with an automated aid.
Other factors leading to an over-reliance on automation and thus to automation bias include inexperience in a task (though inexperienced users tend to be most benefited by automated decision support systems), lack of confidence in one's own abilities, a lack of readily available alternative information, or desire to save time and effort on complex tasks or high workloads. It has been shown that people who have greater confidence in their own decision-making abilities tend to be less reliant on external automated support, while those with more trust in decision support systems (DSS) were more dependent upon it.

One study, published in the Journal of the American Medical Informatics Association, found that the position and prominence of advice on a screen can impact the likelihood of automation bias, with prominently displayed advice, correct or not, is more likely to be followed; another study, however, seemed to discount the importance of this factor. According to another study, a greater amount of on-screen detail can make users less "conservative" and thus increase the likelihood of automation bias. One study showed that making individuals accountable for their performance or the accuracy of their decisions reduced automation bias.

"The availability of automated decision aids," states one study by Linda Skitka, "can sometimes feed into the general human tendency to travel the road of least cognitive effort."

One study also found that when users are made aware of the reasoning process employed by a decision support system, they are likely to adjust their reliance accordingly, thus reducing automation bias.

The performance of jobs by crews instead of individuals acting alone does not necessarily eliminate automation bias. One study has shown that when automated devices failed to detect system irregularities, teams were no more successful than solo performers at responding to those irregularities.

Training that focuses on automation bias in aviation has succeeded in reducing omission errors by student pilots.

It has been shown that automation failure is followed by a drop in operator trust, which in turn is succeeded by a slow recovery of trust. The decline in trust after an initial automation failure has been described as the first-failure effect. By the same token, if automated aids prove to be highly reliable over time, the result is likely to be a heightened level of automation bias. This is called "learned carelessness."

In cases where system confidence information is provided to users, that information itself can become a factor in automation bias.

Studies have shown that the more external pressures are exerted on an individual's cognitive capacity, the more he or she may rely on external support.

Although automation bias has been the subject of many studies, there continue to be complaints that automation bias remains ill-defined and that reporting of incidents involving automation bias is unsystematic.
A review of various automation bias studies categorized the different types of tasks where automated aids were used as well as what function the automated aids served. Tasks where automated aids were used were categorized as monitoring tasks, diagnosis tasks, or treatment tasks. Types of automated assistance were listed as Alerting automation, which track important changes and alert the user, Decision support automation, which may provide a diagnosis or recommendation, or Implementation automation, where the automated aid performs a specified task.

The concept of automation bias is viewed as overlapping with automation-induced complacency, also known more simply as automation complacency. Like automation bias, it is a consequence of the misuse of automation and involves problems of attention. While automation bias involves a tendency to trust decision-support systems, automation complacency involves insufficient attention to and monitoring of automation output, usually because that output is viewed as reliable. "Although the  concepts  of complacency  and automation bias have been discussed separately as if they were independent," writes one expert, "they share several commonalities, suggesting they reflect different aspects of the same  kind of automation misuse." It has been proposed, indeed, that the concepts of complacency and automation bias be combined into a single "integrative concept" because these two concepts "might represent different manifestations of overlapping automation-induced phenomena" and because "automation-induced complacency and automation bias represent closely linked theoretical concepts that show considerable overlap with respect to the underlying processes."
Automation complacency has been defined as "poorer detection of system malfunctions under automation compared with under manual control." NASA's Aviation Safety Reporting System (ASRS) defines complacency as "self-satisfaction that  may result in non-vigilance based on an unjustified assumption of satisfactory system state." Several studies have indicated that it occurs most often when operators are engaged in both manual and automated tasks at the same time. In turn, the operators' perceptions of the automated system's reliability can influence the way in which the operator interacts with the system. Endsley (2017) describes how high system reliability can lead users to disengage from monitoring systems, thereby increasing monitoring errors, decreasing situational awareness, and interfering with an operator's ability to re-assume control of the system in the event performance limitations have been exceeded. This complacency can be sharply reduced when automation reliability varies over time instead of remaining constant, but is not reduced by experience and practice. Both expert and inexpert participants can exhibit automation bias as well as automation complacency. Neither of these problems can be easily overcome by training.
The term "automation complacency" was first used in connection with aviation accidents or incidents in which pilots, air-traffic controllers, or  other workers failed to check systems sufficiently, assuming that everything was fine when, in reality, an accident was about to occur. Operator complacency, whether or not automation-related, has long been recognized as a leading factor in air accidents.
As such, perceptions of reliability, in general, can result in a form of automation irony, in which more automation can decrease cognitive workload but increase the opportunity for monitoring errors. In contrast, low automation can increase workload but decrease the opportunity for monitoring errors. Take, for example, a pilot flying through inclement weather, in which continuous thunder interferes with the pilot's ability to understand information transmitted by an air traffic controller (ATC). Despite how much effort is allocated to understanding information transmitted by ATC, the pilot's performance is limited by the source of information needed for the task. The pilot therefore has to rely on automated gauges in the cockpit to understand flight path information. If the pilot perceives the automated gauges to be highly reliable, the amount of effort needed to understand ATC and automated gauges may decrease. Moreover, if the automated gauges are perceived to be highly reliable, the pilot may ignore those gauges to devote mental resources for deciphering information transmitted by ATC. In so doing, the pilot becomes a complacent monitor, thereby running the risk of missing critical information conveyed by the automated gauges. If, however, the pilot perceives the automated gauges to be unreliable, the pilot will now have to interpret information from ATC and automated gauges simultaneously. This creates scenarios in which the operator may be expending unnecessary cognitive resources when the automation is in fact reliable, but also increasing the odds of identifying potential errors in the weather gauges should they occur. To calibrate the pilot's perception of reliability, automation should be designed to maintain workload at appropriate levels while also ensuring the operator remains engaged with monitoring tasks. The operator should be less likely to disengage from monitoring when the system's reliability can change as compared to a system that has consistent reliability (Parasuraman, 1993).
To some degree, user complacency offsets the benefits of automation, and when an automated system's reliability level falls below a certain level, then automation will no longer be a net asset. One 2007 study suggested that this automation occurs when the reliability level reaches approximately 70%. Other studies have found that automation with a reliability level below 70% can be of use to persons with access to the raw information sources, which can be combined with the automation output to improve performance.
Death by GPS, wherein the deaths of individuals is in part caused by following inaccurate GPS directions, is another example of automation complacency.

Automation bias has been examined across many research fields. It can be a particularly major concern in aviation, medicine, process control, and military command-and-control operations.

At first, discussion of automation bias focused largely on aviation. Automated aids have played an increasing role in cockpits, taking a growing role in the control of such flight tasks as determining the most fuel-efficient routes, navigating, and detecting and diagnosing system malfunctions. The use of these aids, however, can lead to less attentive and less vigilant information seeking and processing on the part of human beings. In some cases, human beings may place more confidence in the misinformation provided by flight computers than in their own skills.
An important factor in aviation-related automation bias is the degree to which pilots perceive themselves as responsible for the tasks being carried out by automated aids. One study of pilots showed that the presence of a second crewmember in the cockpit did not affect automation bias. A 1994 study compared the impact of low and high levels of automation (LOA) on pilot performance, and concluded that pilots working with a high level spent less time reflecting independently on flight decisions.
In another study, all of the pilots given false automated alerts that instructed them to shut off an engine did so, even though those same pilots insisted in an interview that they would not respond to such an alert by shutting down an engine, and would instead have reduced the power to idle. One 1998 study found that pilots with approximately 440 hours of flight experience detected more automation failures than did nonpilots, although both groups showed complacency effects. A 2001 study of pilots using a cockpit automation system, the Engine-indicating and crew-alerting system (EICAS), showed evidence of complacency. The pilots detected fewer engine malfunctions when using the system than when performing the task manually.
In a 2005 study, experienced air-traffic controllers used high-fidelity simulation of an ATC (Free Flight) scenario that involved the detection of conflicts among "self-separating" aircraft. They had access to an automated device that identified potential conflicts several minutes ahead of time. When the device failed near the end of the simulation process, considerably fewer controllers detected the conflict than when the situation was handled manually. Other studies have produced similar findings.
Two studies of automation bias in aviation discovered a higher rate of commission errors than omission errors, while another aviation study found 55% omission rates and 0% commission rates. Automation-related omissions errors are especially common during the cruise phase. When a China Airlines flight lost power in one engine, the autopilot attempted to correct for this problem by lowering the left wing, an action that hid the problem from the crew. When the autopilot was disengaged, the airplane rolled to the right and descended steeply, causing extensive damage. The 1983 shooting down of a Korean Airlines 747 over Soviet airspace occurred because the Korean crew "relied on automation that had been inappropriately set up, and they never checked their progress manually."

Clinical decision support systems (CDSS) are designed to aid clinical decision-making. They have the potential to effect a great improvement in this regard, and to result in improved patient outcomes. Yet while CDSS, when used properly, bring about an overall improvement in performance, they also cause errors that may not be recognized owing to automation bias. One danger is that the incorrect advice given by these systems may cause users to change a correct decision that they have made on their own. Given the highly serious nature of some of the potential consequences of automation  bias in the health-care field, it is especially important to be aware of this problem when it occurs in clinical settings.
Sometimes automation bias in clinical settings is a major problem that renders CDSS, on balance, counterproductive; sometimes it is  minor problem, with the benefits outweighing the damage done. One study found more automation bias among older users, but it was noted that could be a result not of age but of experience. Studies suggest, indeed, that familiarity with CDSS often leads to desensitization and habituation effects. Although automation bias occurs more often among persons who are inexperienced in a given task, inexperienced users exhibit the most performance improvement when they use CDSS. In one study, the use of CDSS improved clinicians' answers by 21%, from 29% to 50%, with 7% of correct non-CDSS answers being changed incorrectly.
A 2005 study found that when primary-care physicians used electronic sources such as PubMed, Medline, and Google, there was a "small to medium" increase in correct answers, while in an equally small percentage of instances the physicians were misled by their use of those sources, and changed correct to incorrect answers.
Studies in 2004 and 2008 that involved the effect of automated aids on diagnosis of breast cancer found clear evidence of automation bias involving omission errors. Cancers diagnosed in 46% of cases without automated aids were discovered in only 21% of cases with automated aids that failed to identify the cancer.

Automation bias can be a crucial factor in the use of intelligent decision support systems for military command-and-control operations. One 2004 study found that automation bias effects have contributed to a number of fatal military decisions, including friendly-fire killings during the Iraq War. Researchers have sought to determine the proper level of automation for decision support systems in this field.

Automation complacency is also a challenge for automated driving systems in which the human only has to monitor the system or act as a fallback driver. This is for example discussed in the report of National Transportation Safety Board about the fatal accident between an UBER test vehicle and pedestrian Elaine Herzberg.

Automation bias can be mitigated by redesigning automated systems to reduce display prominence, decrease information complexity or couch assistance as supportive rather than directive information. Training users on automated systems which introduce deliberate errors more effectively reduces automation bias than just telling them errors can occur. Excessively checking and questioning automated assistance can increase time pressure and task complexity, thus reducing benefit, so some automated decision support systems balance positive and negative effects rather than attempt to eliminate negative effects.

The observer-expectancy effect is a form of reactivity in which a researcher's cognitive bias causes them to subconsciously influence the participants of an experiment. Confirmation bias can lead to the experimenter interpreting results incorrectly because of the tendency to look for information that conforms to their hypothesis, and overlook information that argues against it. It is a significant threat to a study's internal validity, and is therefore typically controlled using a double-blind experimental design.
It may include conscious or unconscious influences on subject behavior including creation of demand characteristics that influence subjects, and altered or selective recording of experimental results themselves.

The experimenter may introduce cognitive bias into a study in several ways‍—‍in the observer-expectancy effect, the experimenter may subtly communicate their expectations for the outcome of the study to the participants, causing them to alter their behavior to conform to those expectations. Such observer bias effects are near-universal in human data interpretation under expectation and in the presence of imperfect cultural and methodological norms that promote or enforce objectivity.
The classic example of experimenter bias is that of "Clever Hans", an Orlov Trotter horse claimed by his owner von Osten to be able to do arithmetic and other tasks. As a result of the large public interest in Clever Hans, philosopher and psychologist Carl Stumpf, along with his assistant Oskar Pfungst, investigated these claims. Ruling out simple fraud, Pfungst determined that the horse could answer correctly even when von Osten did not ask the questions. However, the horse was unable to answer correctly when either it could not see the questioner, or if the questioner themselves was unaware of the correct answer: When von Osten knew the answers to the questions, Hans answered correctly 89% of the time. However, when von Osten did not know the answers, Hans guessed only 6% of questions correctly. Pfungst then proceeded to examine the behaviour of the questioner in detail, and showed that as the horse's taps approached the right answer, the questioner's posture and facial expression changed in ways that were consistent with an increase in tension, which was released when the horse made the final, correct tap. This provided a cue that the horse had learned to use as a reinforced cue to stop tapping.
Experimenter-bias also influences human subjects. As an example, researchers compared performance of two groups given the same task (rating portrait pictures and estimating how successful each individual was on a scale of −10 to 10), but with different experimenter expectations. In one group, ("Group A"), experimenters were told to expect positive ratings while in another group, ("Group B"), experimenters were told to expect negative ratings. Data collected from Group A was a significant and substantially more optimistic appraisal than the data collected from Group B. The researchers suggested that experimenters gave subtle but clear cues with which the subjects complied.

Double blind techniques may be employed to combat bias by causing the experimenter and subject to be ignorant of which condition data flows from.
It might be thought that, due to the central limit theorem of statistics, collecting more independent measurements will improve the precision of estimates, thus decreasing bias. However, this assumes that the measurements are statistically independent. In the case of experimenter bias, the measures share correlated bias: simply averaging such data will not lead to a better statistic but may merely reflect the correlations among the individual measurements and their non-independent nature.

Affective forecasting, also known as hedonic forecasting or the hedonic forecasting mechanism, is the prediction of one's affect (emotional state) in the future. As a process that influences preferences, decisions, and behavior, affective forecasting is studied by both psychologists and economists, with broad applications.

In The Theory of Moral Sentiments (1759), Adam Smith observed the personal challenges, and social benefits, of hedonic forecasting errors:
[Consider t]he poor man's son, whom heaven in its anger has visited with ambition, when he begins to look around him, admires the condition of the rich …. and, in order to arrive at it, he devotes himself for ever to the pursuit of wealth and greatness…. Through the whole of his life he pursues the idea of a certain artificial and elegant repose which he may never arrive at, for which he sacrifices a real tranquillity that is at all times in his power, and which, if in the extremity of old age he should at last attain…, he will find to be in no respect preferable to that humble security and contentment which he had abandoned for it. It is then, in the last dregs of life, his body wasted with toil and diseases, his mind galled and ruffled by the memory of a thousand injuries and disappointments..., that he begins at last to find that wealth and greatness are mere trinkets of frivolous utility….
[Yet] it is well that nature imposes upon us in this manner. It is this deception which rouses and keeps in continual motion the industry of mankind.
In the early 1990s, Kahneman and Snell began research on hedonic forecasts, examining its impact on decision making. The term "affective forecasting" was later coined by psychologists Timothy Wilson and Daniel Gilbert. Early research tended to focus solely on measuring emotional forecasts, while subsequent studies began to examine the accuracy of forecasts, revealing that people are surprisingly poor judges of their future emotional states. For example, in predicting how events like winning the lottery might affect their happiness, people are likely to overestimate future positive feelings, ignoring the numerous other factors that might contribute to their emotional state outside of the single lottery event. Some of the cognitive biases related to systematic errors in affective forecasts are focalism, hot-cold empathy gap, and impact bias.

While affective forecasting has traditionally drawn the most attention from economists and psychologists, their findings have in turn generated interest from a variety of other fields, including happiness research, law, and health care. Its effect on decision-making and well-being is of particular concern to policy-makers and analysts in these fields, although it also has applications in ethics. For example, one's tendency to underestimate one's ability to adapt to life-changing events has led to legal theorists questioning the assumptions behind tort damage compensation. Behavioral economists have incorporated discrepancies between forecasts and actual emotional outcomes into their models of different types of utility and welfare. This discrepancy also concerns healthcare analysts, in that many important health decisions depend upon patients' perceptions of their future quality of life.

Affective forecasting can be divided into four components: predictions about valence (i.e. positive or negative), the specific emotions experienced, their duration, and their intensity. While errors may occur in all four components, research overwhelmingly indicates that the two areas most prone to bias, usually in the form of overestimation, are duration and intensity. Immune neglect is a form of impact bias in response to negative events, in which people fail to predict how much their recovery will be hastened by their psychological immune system. The psychological immune system is a metaphor "for that system of defenses that helps you feel better when bad things happen", according to Gilbert. On average, people are fairly accurate about predicting which emotions they will feel in response to future events. However, some studies indicate that predicting specific emotions in response to more complex social events leads to greater inaccuracy. For example, one study found that while many women who imagine encountering gender harassment predict feelings of anger, in reality, a much higher proportion report feelings of fear. Other research suggests that accuracy in affective forecasting is greater for positive affect than negative affect, suggesting an overall tendency to overreact to perceived negative events. Gilbert and Wilson posit that this is a result of the psychological immune system.
While affective forecasts take place in the present moment, researchers also investigate its future outcomes. That is, they analyze forecasting as a two-step process, encompassing a current prediction as well as a future event. Breaking down the present and future stages allow researchers to measure accuracy, as well as tease out how errors occur. Gilbert and Wilson, for example, categorize errors based on which component they affect and when they enter the forecasting process. In the present phase of affective forecasting, forecasters bring to mind a mental representation of the future event and predict how they will respond emotionally to it. The future phase includes the initial emotional response to the onset of the event, as well as subsequent emotional outcomes, for example, the fading of the initial feeling.
When errors occur throughout the forecasting process, people are vulnerable to biases. These biases disable people from accurately predicting their future emotions. Errors may arise due to extrinsic factors, such as framing effects, or intrinsic ones, such as cognitive biases or expectation effects. Because accuracy is often measured as the discrepancy between a forecaster's present prediction and the eventual outcome, researchers also study how time affects affective forecasting. For example, the tendency for people to represent distant events differently from close events is captured in the construal level theory.
The finding that people are generally inaccurate affective forecasters has been most obviously incorporated into conceptualizations of happiness and its successful pursuit, as well as decision making across disciplines. Findings in affective forecasts have stimulated philosophical and ethical debates, for example, on how to define welfare. On an applied level, findings have informed various approaches to healthcare policy, tort law, consumer decision making, and measuring utility (see below sections on economics, law, and health).
Newer and conflicting evidence suggests that intensity bias in affective forecasting may not be as strong as previous research indicates. Five studies, including a meta-analysis, recover evidence that overestimation in affective forecasting is partly due to the methodology of past research. Their results indicate that some participants misinterpreted specific questions in affective forecasting testing. For example, one study found that undergraduate students tended to overestimate experienced happiness levels when participants were asked how they were feeling in general with and without reference to the election, compared to when participants were asked how they were feeling specifically in reference to the election. Findings indicated that 75%-81% of participants who were asked general questions misinterpreted them.  After clarification of tasks, participants were able to more accurately predict the intensity of their emotions

Because forecasting errors commonly arise from literature on cognitive processes, many affective forecasting errors derive from and are often framed as cognitive biases, some of which are closely related or overlapping constructs (e.g. projection bias and empathy gap). Below is a list of commonly cited cognitive processes that contribute to forecasting errors.

One of the most common sources of error in affective forecasting across various populations and situations is impact bias, the tendency to overestimate the emotional impact of a future event, whether in terms of intensity or duration. The tendencies to overestimate intensity and duration are both robust and reliable errors found in affective forecasting.
One study documenting impact bias examined college students participating in a housing lottery.  These students predicted how happy or unhappy they would be one year after being assigned to either a desirable or an undesirable dormitory. These college students predicted that the lottery outcomes would lead to meaningful differences in their own level of happiness, but follow-up questionnaires revealed that students assigned to desirable or undesirable dormitories reported nearly the same levels of happiness. Thus, differences in forecasts overestimated the impact of the housing assignment on future happiness.
Some studies specifically address "durability bias," the tendency to overestimate the length of time future emotional responses will last. Even if people accurately estimate the intensity of their future emotions, they may not be able to estimate their duration. Durability bias is generally stronger in reaction to negative events. This is important because people tend to work toward events they believe will cause lasting happiness, and according to durability bias, people might be working toward the wrong things. Similar to impact bias, durability bias causes a person to overemphasize where the root cause of their happiness lies.
Impact bias is a broad term and covers a multitude of more specific errors. Proposed causes of impact bias include mechanisms like immune neglect,  focalism, and misconstruals. The pervasiveness of impact bias in affective forecasts is of particular concern to healthcare specialists, in that it affects both patients' expectations of future medical events as well as patient-provider relationships. (See health.)

Previously formed expectations can alter emotional responses to the event itself, motivating forecasters to confirm or debunk their initial forecasts. In this way, the self-fulfilling prophecy can lead to the perception that forecasters have made accurate predictions. Inaccurate forecasts can also become amplified by expectation effects. For example, a forecaster who expects a movie to be enjoyable will, upon finding it dull, like it significantly less than a forecaster who had no expectations.

Major life events can have a huge impact on people's emotions for a very long time but the intensity of that emotion tends to decrease with time, a phenomenon known as emotional evanescence. When making forecasts, forecasters often overlook this phenomenon. Psychologists have suggested that emotion does not decay over time predictably like radioactive isotopes but that the mediating factors are more complex. People have psychological processes that help dampen emotions. Psychologists have proposed that surprising, unexpected, or unlikely events cause more intense emotional reactions. Research suggests that people are unhappy with randomness and chaos and that they automatically think of ways to make sense of an event when it is surprising or unexpected. This sense-making helps individuals recover from negative events more quickly than they would have expected. This is related to immune neglect in that when these unwanted acts of randomness occur people become upset and try to find meaning or ways to cope with the event. The way that people try to make sense of the situation can be considered a coping strategy made by the body. This idea differs from immune neglect due to the fact that this is more of a momentary idea. Immune neglect tries to cope with the event before it even happens.
One study documents how sense-making processes decrease emotional reactions. The study found that a small gift produced greater emotional reactions when it was not accompanied by a reason than when it was, arguably because the reason facilitated the sense-making process, dulling the emotional impact of the gift. Researchers have summarized that pleasant feelings are prolonged after a positive situation if people are uncertain about the situation.
People fail to anticipate that they will make sense of events in a way that will diminish the intensity of the emotional reaction. This error is known as ordinization neglect. For example, ("I will be ecstatic for many years if my boss agrees to give me a raise") an employee might believe, especially if the employee believes the probability of a raise was unlikely. Immediately after having the request approved, the employee may be thrilled but with time the employees make sense of the situation (e.g., "I am a very hard worker and my boss must have noticed this") thus dampening the emotional reaction.

A variant of immune neglect also proposed by Gilbert and Wilson is the region-beta paradox, where recovery from more intense suffering is faster than recovery from less intense experiences because of the engagement of coping systems. This complicates forecasting, leading to errors. Contrarily, accurate affective forecasting can also promote the region-beta paradox. For example, Cameron and Payne conducted a series of studies in order to investigate the relationship between affective forecasting and the collapse of compassion phenomenon, which refers to the tendency for people's compassion to decrease as the number of people in need of help increases. Participants in their experiments read about either 1 or a group of 8 children from Darfur. These researchers found that people who are skilled at regulating their emotions tended to experience less compassion in response to stories about 8 children from Darfur compared to stories about only 1 child. These participants appeared to collapse their compassion by correctly forecasting their future affective states and proactively avoiding the increased negative emotions resulting from the story. In order to further establish the causal role of proactive emotional regulation in this phenomenon, participants in another study read the same materials and were encouraged to either reduce or experience their emotions. Participants instructed to reduce their emotions reported feeling less upset for 8 children than for 1, presumably because of the increased emotional burden and effort required for the former (an example of the region-beta paradox). These studies suggest that in some cases accurate affective forecasting can actually promote unwanted outcomes such as the collapse of compassion phenomenon by way of the region-beta paradox.

Research suggests that the accuracy of affective forecasting for positive and negative emotions is based on the distance in time of the forecast. Finkenauer, Gallucci, van Dijk, and Pollman discovered that people show greater forecasting accuracy for positive than negative affect when the event or trigger being forecast is more distant in time. Contrarily, people exhibit greater affective forecasting accuracy for negative affect when the event/trigger is closer in time. The accuracy of an affective forecast is also related to how well a person predicts the intensity of his or her emotions. In regard to forecasting both positive and negative emotions, Levine, Kaplan, Lench, and Safer have recently shown that people can in fact predict the intensity of their feelings about events with a high degree of accuracy. This finding is contrary to much of the affective forecasting literature currently published, which the authors suggest is due to a procedural artifact in how these studies were conducted.
Another important affective forecasting bias is fading affect bias, in which the emotions associated with unpleasant memories fade more quickly than the emotion associated with positive events.

Focalism (or the "focusing illusion") occurs when people focus too much on certain details of an event, ignoring other factors. Research suggests that people have a tendency to exaggerate aspects of life when focusing their attention on it. A well-known example originates from a paper by Kahneman and Schkade, who coined the term "focusing illusion" in 1998. They found that although people tended to believe that someone from the Midwest would be more satisfied if they lived in California, results showed equal levels of life satisfaction in residents of both regions. In this case, concentrating on the easily observed difference in weather bore more weight in predicting satisfaction than other factors. There are many other factors that could have contributed to the desire to move to the Midwest, but the focal point for their decisions was weather. Various studies have attempted to "defocus" participants, meaning instead of focusing on that one factor, they tried to make the participants think of other factors or look at the situation through a different lens. There were mixed results dependent upon the methods used. One successful study asked people to imagine how happy a winner of the lottery and a recently diagnosed HIV patient would be. The researchers were able to reduce the amount of focalism by exposing participants to detailed and mundane descriptions of each person's life, meaning that the more information the participants had on the lottery winner and the HIV patient the less they were able to only focus on few factors, these participants subsequently estimated similar levels of happiness for the HIV patient as well as the lottery-winner. As for the control participants, they made unrealistically disparate predictions of happiness. This could be due to the fact that the more information that is available, the less likely it is one will be able to ignore contributory factors.

Affective forecasters often rely on memories of past events. When people report memories of past events they may leave out important details, change things that occurred, and even add things that have not happened. This suggests the mind constructs memories based on what actually happened, and other factors including the person's knowledge, experiences, and existing schemas.  Using highly available, but unrepresentative memories, increases the impact bias. Baseball fans, for example, tend to use the best game they can remember as the basis for their affective forecast of the game they are about to see. Commuters are similarly likely to base their forecasts of how unpleasant it would feel to miss a train on their memory of the worst time they missed the train  Various studies indicate that retroactive assessments of past experiences are prone to various errors, such as duration neglect or decay bias. People tend to overemphasize the peaks and ends of their experiences when assessing them (peak/end bias), instead of analyzing the event as a whole. For example, in recalling painful experiences, people place greater emphasis on the most discomforting moments as well as the end of the event, as opposed to taking into account the overall duration. Retroactive reports often conflict with present-moment reports of events, further pointing to contradictions between the actual emotions experienced during an event and the memory of them. In addition to producing errors in forecasts about the future, this discrepancy has incited economists to redefine different types of utility and happiness (see the section on economics).
Another problem that can arise with affective forecasting is that people tend to remember their past predictions inaccurately. Meyvis, Ratner, and Levav predicted that people forget how they predicted an experience would be beforehand, and thought their predictions were the same as their actual emotions. Because of this, people do not realize that they made a mistake in their predictions, and will then continue to inaccurately forecast similar situations in the future.  Meyvis et al. ran five studies to test whether or not this is true. They found in all of their studies, when people were asked to recall their previous predictions they instead write how they currently feel about the situation. This shows that they do not remember how they thought they would feel, and makes it impossible for them to learn from this event for future experiences.

When predicting future emotional states people must first construct a good representation of the event. If people have a lot of experience with the event then they can easily picture the event. When people do not have much experience with the event they need to create a representation of what the event likely contains. For example, if people were asked how they would feel if they lost one hundred dollars in a bet, gamblers are more likely to easily construct an accurate representation of the event. "Construal level theory" theorizes that distant events are conceptualized more abstractly than immediate ones. Thus, psychologists suggest that a lack of concrete details prompts forecasters to rely on more general or idealized representations of events, which subsequently leads to simplistic and inaccurate predictions. For example, when asked to imagine what a 'good day' would be like for them in the near future, people often describe both positive and negative events. When asked to imagine what a 'good day' would be like for them in a year, however, people resort to more uniformly positive descriptions. Gilbert and Wilson call bringing to mind a flawed representation of a forecasted event the misconstrual problem. Framing effects, environmental context, and heuristics (such as schemas) can all affect how a forecaster conceptualizes a future event. For example, the way options are framed affects how they are represented: when asked to forecast future levels of happiness based on pictures of dorms they may be assigned to, college students use physical features of the actual buildings to predict their emotions. In this case, the framing of options highlighted visual aspects of future outcomes, which overshadowed more relevant factors to happiness, such as having a friendly roommate.

Projection bias is the tendency to falsely project current preferences onto a future event. When people are trying to estimate their emotional state in the future they attempt to give an unbiased estimate. However, people's assessments are contaminated by their current emotional state. Thus, it may be difficult for them to predict their emotional state in the future, an occurrence known as mental contamination. For example, if a college student was currently in a negative mood because he just found out he failed a test, and if the college student forecasted how much he would enjoy a party two weeks later, his current negative mood may influence his forecast. In order to make an accurate forecast the student would need to be aware that his forecast is biased due to mental contamination, be motivated to correct the bias, and be able to correct the bias in the right direction and magnitude.
Projection bias can arise from empathy gaps (or hot/cold empathy gaps), which occur when the present and future phases of affective forecasting are characterized by different states of physiological arousal, which the forecaster fails to take into account. For example, forecasters in a state of hunger are likely to overestimate how much they will want to eat later, overlooking the effect of their hunger on future preferences. As with projection bias, economists use the visceral motivations that produce empathy gaps to help explain impulsive or self-destructive behaviors, such as smoking.
An important affective forecasting bias related to projection bias is personality neglect. Personality neglect refers to a person's tendency to overlook their personality when making decisions about their future emotions. In a study conducted by Quoidbach and Dunn, students' predictions of their feelings about future exam scores were used to measure affective forecasting errors related to personality. They found that college students who predicted their future emotions about their exam scores were unable to relate these emotions to their own dispositional happiness. To further investigate personality neglect, Quoidbach and Dunn studied happiness in relation to neuroticism. People predicted their future feelings about the outcome of the 2008 US presidential election between Barack Obama and John McCain. Neuroticism was correlated with impact bias, which is the overestimation of the length and intensity of emotions. People who rated themselves as higher in neuroticism overestimated their happiness in response to the election of their preferred candidate, suggesting that they failed to relate their dispositional happiness to their future emotional state.
The term "projection bias" was first introduced in the 2003 paper "Projection Bias in Predicting Future Utility" by Loewenstein, O'Donoghue and Rabin.

The novelty of new products oftentimes overexcites consumers and results in the negative consumption externality of impulse buying. To counteract such, George Loewenstein recommends offering "cooling off"  periods for consumers. During such, they would have a few days to reflect on their purchase and appropriately develop a longer-term understanding of the utility they receive from it. This cooling-off period could also benefit the production side by diminishing the need for a salesperson to "hype" certain products. Transparency between consumers and producers would increase as "sellers will have an incentive to put buyers in a long-run average mood rather than an overenthusiastic state". By implementing Loewentstein's recommendation, firms that understand projection bias should minimize information asymmetry; such would diminish the negative consumer externality that comes from purchasing an undesirable good and relieve sellers from extraneous costs required to exaggerate the utility of their product.

Projection bias influences the life cycle of consumption. The immediate utility obtained from consuming particular goods exceeds the utility of future consumption. Consequently, projection bias causes "a person to (plan to) consume too much early in life and too little late in life relative to what would be optimal".  Graph 1 displays decreasing expenditures as a percentage of total income from 20 to 54. The period following where income begins to decline can be explained by retirement. According to Loewenstein's recommendation, a more optimal expenditure and income distribution is displayed in Graph 2. Here, income is left the same as in Graph 1, but expenditures are recalculated by taking the average percentage of expenditures in terms of income from ages 25 to 54 (77.7%) and multiplying such by income to arrive at a theoretical expenditure. The calculation is only applied to this age group because of unpredictable income before 25 and after 54 due to school and retirement.

When buying food, people often wrongly project what they will want to eat in the future when they go shopping, which results in food waste.

Generally, affect is a potent source of motivation. People are more likely to pursue experiences and achievements that will bring them more pleasure than less pleasure. In some cases, affective forecasting errors appear to be due to forecasters' strategic use of their forecasts as a means to motivate them to obtain or avoid the forecasted experience. Students, for example, might predict they would be devastated if they failed a test as a way to motivate them to study harder for it. The role of motivated reasoning in affective forecasting has been demonstrated in studies by Morewedge and Buechel (2013). Research participants were more likely to overestimate how happy they would be if they won a prize, or achieved a goal, if they made an affective forecast while they could still influence whether or not they achieved it than if they made an affective forecast after the outcome had been determined (while still in the dark about whether they knew if they won the prize or achieved the goal).

Economists share psychologists' interests in affective forecasting insomuch as it affects the closely related concepts of utility, decision making, and happiness.

Research in affective forecasting errors complicates conventional interpretations of utility maximization, which presuppose that to make rational decisions, people must be able to make accurate forecasts about future experiences or utility. Whereas economics formerly focused largely on utility in terms of a person's preferences (decision utility), the realization that forecasts are often inaccurate suggests that measuring preferences at a time of choice may be an incomplete concept of utility. Thus, economists such as Daniel Kahneman, have incorporated differences between affective forecasts and later outcomes into corresponding types of utility. Whereas a current forecast reflects expected or predicted utility, the actual outcome of the event reflects experienced utility. Predicted utility is the "weighted average of all possible outcomes under certain circumstances."  Experienced utility refers to the perceptions of pleasure and pain associated with an outcome. Kahneman and Thaler provide an example of "the hungry shopper," in which case the shopper takes pleasure in the purchase of food due to their current state of hunger. The usefulness of such purchasing is based on their current experience and their anticipated pleasure in fulfilling their hunger.

Affective forecasting is an important component of studying human decision making. Research in affective forecasts and economic decision making include investigations of durability bias in consumers and predictions of public transit satisfaction. In relevance to the durability bias in consumers, a study was conducted by Wood and Bettman, that showed that people make decisions regarding the consumption of goods based on the predicted pleasure, and the duration of that pleasure, that the goods will bring them.  Overestimation of such pleasure, and its duration, increases the likelihood that the good will be consumed. Knowledge on such an effect can aid in the formation of marketing strategies of consumer goods. Studies regarding the predictions of public transit satisfaction reveal the same bias. However, with a negative impact on consumption, due to their lack of experience with public transportation, car users predict that they will receive less satisfaction with the use of public transportation than they actually experience. This can lead them to refrain from the use of such services, due to inaccurate forecasting. Broadly, the tendencies people have to make biased forecasts deviate from rational models of decision making. Rational models of decision making presume an absence of bias, in favor of making comparisons based on all relevant and available information. Affective forecasting may cause consumers to rely on the feelings associated with consumption rather than the utility of the good itself. One application of affective forecasting research is in economic policy. The knowledge that forecasts, and therefore, decisions, are affected by biases as well as other factors (such as framing effects), can be used to design policies that maximize the utility of people's choices. This approach is not without its critics, however, as it can also be seen to justify economic paternalism.
Prospect theory describes how people make decisions. It differs from expected utility theory in that it takes into account the relativity of how people view utility and incorporates loss aversion, or the tendency to react more strongly to losses rather than gains. Some researchers suggest that loss aversion is in itself an affective forecasting error since people often overestimate the impact of future losses.

Economic definitions of happiness are tied to concepts of welfare and utility, and researchers are often interested in how to increase levels of happiness in the population. The economy has a major influence on the aid that is provided through welfare programs because it provides funding for such programs. 
Many welfare programs are focused on providing assistance with the attainment of basic necessities such as food and shelter. This may be due to the fact that happiness and well-being are best derived from personal perceptions of one's ability to provide these necessities. This statement is supported by research that states after basic needs have been met, income has less of an impact on perceptions of happiness. Additionally, the availability of such welfare programs can enable those that are less fortunate to have additional discretionary income. Discretionary income can be dedicated to enjoyable experiences, such as family outings, and in turn, provides an additional dimension to their feelings and experience of happiness. Affective forecasting provides a unique challenge to answering the question regarding the best method for increasing levels of happiness, and economists are split between offering more choices to maximize happiness, versus offering experiences that contain more objective or experienced utility. Experienced utility refers to how useful an experience is in its contribution to feelings of happiness and well-being.  Experienced utility can refer to both material purchases and experiential purchases. Studies show that experiential purchases, such as a bag of chips, result in forecasts of higher levels of happiness than material purchases, such as the purchase of a pen. This prediction of happiness as a result of a purchase experience exemplifies affective forecasting. It is possible that an increase in choices, or means, of achieving desired levels of happiness will be predictive of increased levels of happiness. For example, if one is happy with their ability to provide themselves with both a choice of necessities and a choice of enjoyable experiences they are more likely to predict that they will be happier than if they were forced to choose between one or the other. Also, when people are able to reference multiple experiences that contribute to their feelings of happiness, more opportunities for comparison will lead to a forecast of more happiness. Under these circumstances, both the number of choices and the quantity of experienced utility have the same effect on affective forecasting, which makes it difficult to choose a side of the debate on which method is most effective in maximizing happiness.
Applying findings from affective forecasting research to happiness also raises methodological issues: should happiness measure the outcome of an experience or the satisfaction experienced as a result of the choice made based upon a forecast? For example, although professors may forecast that getting tenure would significantly increase their happiness, research suggests that in reality, happiness levels between professors who are or are not awarded tenure are insignificant. In this case happiness is measured in terms of the outcome of an experience.  Affective forecasting conflicts such as this one have also influenced theories of hedonic adaptation, which compares happiness to a treadmill, in that it remains relatively stable despite forecasts.

Similar to how some economists have drawn attention to how affective forecasting violates assumptions of rationality, legal theorists point out that inaccuracies in, and applications of, these forecasts have implications in law that have remained overlooked. The application of affective forecasting, and its related research, to legal theory reflects a wider effort to address how emotions affect the legal system. In addition to influencing legal discourse on emotions, and welfare, Jeremy Blumenthal cites additional implications of affective forecasting in tort damages, capital sentencing and sexual harassment.

Jury awards for tort damages are based on compensating victims for pain, suffering, and loss of quality of life. However, findings in affective forecasting errors have prompted some to suggest that juries are overcompensating victims since their forecasts overestimate the negative impact of damages on the victims' lives. Some scholars suggest implementing jury education to attenuate potentially inaccurate predictions, drawing upon research that investigates how to decrease inaccurate affective forecasts.

During the process of capital sentencing, juries are allowed to hear victim impact statements (VIS) from the victim's family. This demonstrates affective forecasting in that its purpose is to present how the victim's family has been impacted emotionally and, or, how they expect to be impacted in the future. These statements can cause juries to overestimate the emotional harm, causing harsh sentencing, or underestimate harm, resulting in inadequate sentencing. The time frame in which these statements are present also influences affective forecasting. By increasing the time gap between the crime itself and sentencing (the time at which victim impact statements are given), forecasts are more likely to be influenced by the error of immune neglect (See Immune neglect) Immune neglect is likely to lead to underestimation of future emotional harm, and therefore results in inadequate sentencing. As with tort damages, jury education is a proposed method for alleviating the negative effects of forecasting error.

In cases involving sexual harassment, judgements are more likely to blame the victim for their failure to react in a timely fashion or their failure to make use of services that were available to them in the event of sexual harassment. This is because prior to the actual experience of harassment, people tend to overestimate their affective reactions as well as their proactive reactions in response to sexual harassment. This exemplifies the focalism error (See Focalism) in which forecasters ignore alternative factors that may influence one's reaction, or failure to react. For example, in their study, Woodzicka and LaFrance studied women's predictions of how they would react to sexual harassment during an interview. Forecasters overestimated their affective reactions of anger, while underestimating the level of fear they would experience. They also overestimated their proactive reactions. In Study 1, participants reported that they would refuse to answer questions of a sexual nature and, or, report the question to the interviewer's supervisor. However, in Study 2, of those who had actually experienced sexual harassment during an interview, none of them displayed either proactive reaction. If juries are able to recognize such errors in forecasting, they may be able to adjust such errors. Additionally, if juries are educated on other factors that may influence the reactions of those who are victims of sexual harassment, such as intimidation, they are more likely to make more accurate forecasts, and less likely to blame victims for their own victimization.

Affective forecasting has implications in health decision making and medical ethics and policy. Research in health-related affective forecasting suggests that nonpatients consistently underestimate the quality of life associated with chronic health conditions and disability. The so-called "disability paradox" states the discrepancy between self-reported levels of happiness amongst chronically ill people versus the predictions of their happiness levels by healthy people. The implications of this forecasting error in medical decision making can be severe, because judgments about future quality of life often inform health decisions. Inaccurate forecasts can lead patients, or more commonly their health care agent, to refuse life-saving treatment in cases when the treatment would involve a drastic change in lifestyle, for example, the amputation of a leg. A patient, or health care agent, who falls victim to focalism would fail to take into account all the aspects of life that would remain the same after losing a limb. Although Halpern and Arnold suggest interventions to foster awareness of forecasting errors and improve medical decision making amongst patients, the lack of direct research in the impact of biases in medical decisions provides a significant challenge.
Research also indicates that affective forecasts about future quality of life are influenced by the forecaster's current state of health. Whereas healthy individuals associate future low health with low quality of life, less healthy individuals do not forecast necessarily low quality of life when imagining having poorer health. Thus, patient forecasts and preferences about their own quality of life may conflict with public notions. Because a primary goal of healthcare is maximizing quality of life, knowledge about patients' forecasts can potentially inform policy on how resources are allocated.
Some doctors suggest that research findings in affective forecasting errors merit medical paternalism. Others argue that although biases exist and should support changes in doctor-patient communication, they do not unilaterally diminish decision-making capacity and should not be used to endorse paternalistic policies. This debate captures the tension between medicine's emphasis on protecting the autonomy of the patient and an approach that favors intervention in order to correct biases.

Individuals who recently have experienced an emotionally charged life event will display the impact bias. The individual predicts they will feel happier than they actually feel about the event. Another factor that influences overestimation is focalism which causes individuals to concentrate on the current event. Individuals often fail to realize that other events will also influence how they currently feel.  Lam et al. (2005) found that the perspective that individuals take influences their susceptibility to biases when making predictions about their feelings.
A perspective that overrides impact bias is mindfulness. Mindfulness is a skill that individuals can learn to help them prevent overestimating their feelings. Being mindful helps the individual understand that they may currently feel negative emotions, but the feelings are not permanent. The Five Factor Mindfulness Questionnaire (FFMQ) can be used to measure an individual's mindfulness. The five factors of mindfulness are observing, describing, acting with awareness, non-judging of inner experience, and non-reactivity to inner experience. The two most important factors for improving forecasts are observing and acting with awareness.  The observing factor assesses how often an individual attends to their sensations, emotions, and outside environment. The ability to observe allows the individual to avoid focusing on one single event, and be aware that other experiences will influence their current emotions. Acting with awareness requires assessing how individuals tend to current activities with careful consideration and concentration. Emanuel, Updegraff, Kalmbach, and Ciesla (2010) stated that the ability to act with awareness reduces the impact bias because the individual is more aware that other events co-occur with the present event. Being able to observe the current event can help individuals focus on pursuing future events that provide long-term satisfaction and fulfillment.

The ostrich effect, also known as the ostrich problem, was originally coined by Dan Galai and Orly Sade. The name comes from the common (but false) legend that ostriches bury their heads in the sand to avoid danger. This effect is a cognitive bias where people tend to "bury their head in the sand" and avoid potentially negative but useful information, such as feedback on progress, to avoid psychological discomfort.

There is neuroscientific evidence of the ostrich effect. Tali Sharot investigated the differences in positive and negative information when updating existing beliefs. Consistent with the ostrich effect, participants presented with negative information were more likely to avoid updating their beliefs. Moreover, they found that the part of the brain responsible for this cognitive bias was the left IFG -  by disrupting this part of the brain with TMS, participants were more likely to accept the negative information provided.

An everyday example of the ostrich effect in a financial context is people avoiding checking their bank account balance after spending a lot of money. The studies below explore the ostrich effect through investors in financial markets.
Galai and Sade studied investors' decision-making in Israel's capital market. They found that investors prefer financial investments where the risk is unreported over those with a similar risk-return profile but with frequently reported risks, saying that investors are willing to pay a premium for "the bliss of ignorance".
Later, Niklas Karlsson studied investors' decision-making in Swedish and US markets. They determined that investors from both countries looked up their portfolios more when the market index was increasing (positive information) and less when the index was decreasing (negative information).

There are known negative implications of the ostrich effect in healthcare. For example, people with diabetes avoid monitoring their blood sugar levels.
Ritesh Banerjee and Giulio Zanella highlighted the ostrich effect in avoiding preventive screening, studying women working at a company to understand how a woman's propensity to get annual mammograms changes after a co-worker is diagnosed with breast cancer. The company had on-site mammograms and removed all barriers to getting them, such as cost and long queues. 70% of eligible women took up the company's offer of an annual mammogram. However, surprisingly, in the presence of a co-worker diagnosed with breast cancer, women "spatially closer to her in the workplace" are 8% less likely to get a screening. Highlighting that in the presence of potentially negative information, people tend to avoid the chance to receive it.

Research has found that when people feel uninformed about a pressing matter, they may exhibit the ostrich effect. The ostrich effect may explain why people sometimes avoid tackling climate change or energy depletion.
Steven Shepherd and Aaron Kay presented participants with a passage. One group read that the US would have oil for 240 more years (positive information), while the other read that supplies would diminish in 40 years (negative information). Afterwards, participants completed a questionnaire to gauge their interest in learning about energy depletion. Those who read that energy depletion was an urgent problem and that oil would run out in 40 years were more likely to avoid learning about the issue.

Cognitive dissonance is a state of psychological discomfort that arises when an individual holds two or more conflicting beliefs. Betty Chang found that when participants ranked reasons on why they did not monitor progress, the main reason was that "information on goal progress would demand a change in beliefs". This statement shows that when confronted with information that contradicts their beliefs, individuals may experience cognitive dissonance and avoid seeking it to reduce discomfort. This avoidance is the ostrich effect. The opposite, seeking information consistent with your beliefs, is a cognitive bias termed confirmation bias.

Chang also found that some participants exhibited the ostrich effect because they did not trust the information provided. Lack of trust is especially true for negative information; Daniel R. Ilgen found that people are more likely to trust positive feedback than negative feedback. Additionally, Kenneth G. DeBono and Richard J. Harnish found that the information's trustability depends on the perceived expertise of the information provider. The higher the perceived expertise, the more likely people trust it.

Loss aversion is the tendency for people to feel the pain of losses more strongly than the pleasure of equivalent gains. Panidi (2015) looked at the link between loss aversion and the ostrich effect - loss aversion was measured through lottery choices, and the ostrich effect was measured through preventive medical testing. The study found that higher loss aversion decreases the chance of the decision to do a preventive medical test. Demonstrating that the higher the loss aversion in an individual, the more likely they are to display the ostrich effect by avoiding information on diagnosis.

Nachum Sicherman showed that the sample and demographic moderate the extent that investors exhibited the ostrich effect. In a sample of 100,000, Sicherman found that 79% of investors showed the ostrich effect while 21% had "anti-ostrich behavior", such as the meerkat effect.
The researchers argued that Gherzi sample size of 617 investors was too small, one potential reason that most investors exhibited the meerkat effect rather than the ostrich effect. Sicherman also showed that the ostrich effect appeared more in "men, older investors and wealthier investors".

Another moderator for the ostrich effect that has yet to be specifically studied but has been theorised is cultural differences. Culture may impact the ostrich effect as the underlying causes of the ostrich effect are all influenced by culture. Eysuko Hoshino-Browne showed that cognitive dissonance is resolved in different manners in collectivistic cultures compared to individualistic cultures. Furthermore, Mei Wang shows that loss aversion is higher in individualistic cultures, and Hazel R. Markus and Shinobu Kitayama (1991) found that collectivistic cultures tend to trust negative feedback and reject positive ones. Individualism appears more in western culture, hinting at the ostrich effect being higher in western cultures. The studies on the ostrich effect are predominantly conducted on western cultures; therefore, future studies must test for potential cultural differences in the ostrich effect.

Social proof (or informational social influence) is a psychological and social phenomenon wherein people copy the actions of others in choosing how to behave in a given situation. The term was coined by Robert Cialdini in his 1984 book Influence: Science and Practice.
Social proof is used in ambiguous social situations where people are unable to determine the appropriate mode of behavior, and is driven by the assumption that the surrounding people possess more knowledge about the current situation.
The effects of social influence can be seen in the tendency of large groups to conform. This is referred to in some publications as the herd behavior. Although social proof reflects a rational motive to take into account the information possessed by others, formal analysis shows that it can cause people to converge too quickly upon a single distinct choice, so that decisions of even larger groups of individuals may be grounded in very little information (see information cascades).
Social proof is one type of conformity.  When a person is in a situation where they are unsure of the correct way to behave, they will often look to others for clues concerning the correct behavior. When "we conform because we believe that others' interpretation of an ambiguous situation is more accurate than ours and will help us choose an appropriate course of action", it is informational social influence.  This is contrasted with normative social influence wherein a person conforms to be liked or accepted by others.
Social proof often leads not only to public compliance (conforming to the behavior of others publicly without necessarily believing it is correct) but also private acceptance (conforming out of a genuine belief that others are correct). Social proof is more powerful when being accurate is more important and when others are perceived as especially knowledgeable.

Uncertainty is a major factor that encourages the use of social proof. One study found that when evaluating a product, consumers were more likely to incorporate the opinions of others through the use of social proof when their own experiences with the product were ambiguous, leaving uncertainty as to the correct conclusion that they should make.

Similarity also motivates the use of social proof; when a person perceives themselves as similar to the people around them, they are more likely to adopt and perceive as correct the observed behavior of these people. This has been noted in areas such as the use of laugh tracks, where participants will laugh longer and harder when they perceive the people laughing to be similar to themselves.
Social proof is also one of Robert Cialdini's  six principles of persuasion, (along with reciprocity, commitment/consistency, authority, liking, and scarcity) which maintains that people are especially likely to perform certain actions if they can relate to the people who performed the same actions before them. One experiment which exemplifies this claim was conducted by researchers who joined a door-to-door charity campaign, who found that if a list of prior donators was longer, the next person solicited was more likely to donate as well. This trend was even more pronounced when the names on the donor list were people that the prospective donor knew, such as friends and neighbors. Cialdini's principle also asserts that peer power is effective because people are more likely to respond to influence tactics applied horizontally rather than vertically, so people are more likely to be persuaded by a colleague than a superior.

The most famous study of social proof is Muzafer Sherif's 1935 experiment.  In this experiment subjects were placed in a dark room and asked to look at a dot of light about 15 feet away.  They were then asked how much, in inches, the dot of light was moving.  In reality it was not moving at all, but due to the autokinetic effect it appeared to move.  How much the light appears to move varies from person to person but is generally consistent over time for each individual.  A few days later a second part of the experiment was conducted. Each subject was paired with two other subjects and asked to give out loud their estimate of how much the light was moving. Even though the subjects had previously given different estimates, the groups would come to a common estimate.  To rule out the possibility that the subjects were simply giving the group answer to avoid looking foolish while still believing their original estimate was correct, Sherif had the subjects judge the lights again by themselves after doing so in the group.  They maintained the group's judgment.  Because the movement of the light is ambiguous the participants were relying on each other to define reality.
Another study looked at informational social influence in eyewitness identification.  Subjects were shown a slide of the "perpetrator".  They were then shown a slide of a line-up of four men, one of whom was the perpetrator they had seen, and were asked to pick him out.  The task was made difficult to the point of ambiguity by presenting the slides very quickly. The task was done in a group that consisted of one actual subject and three confederates (a person acting as a subject but actually working for the experimenter).  The confederates answered first and all three gave the same wrong answer.  In a high-importance condition of the experiment, subjects were told that they were participating in a real test of eyewitness identification ability that would be used by police departments and courts, and that their scores would establish the norm for performance.  In a low-importance condition, subjects were told that the slide task was still being developed and that the experimenters had no idea what the norm for performance was—they were just looking for useful hints to improve the task. It was found that when subjects thought the task was of high importance, they were more likely to conform, giving the confederate's wrong answer 51% of the time, as opposed to 35% of the time in the low-importance condition.

The strength of social proof also varies across different cultures. For instance, studies have shown that subjects in collectivist cultures conform to others' social proof more often than those in individualist cultures. Although this trend seems reoccurring, there is evidence which suggests that these results are a simplification, and that an independent subject's personal individualistic-collectivist tendency also makes an impact upon their decisions. Additional variables, such as the subject's sense of social responsibility, need to be taken into account to better understand the mechanisms of social proof across cultures; for example, more collectivist individuals will often have an increased compulsion to help others because of their prominent awareness of social responsibility, and this in turn will increase the likelihood they will comply to requests, regardless of their peers' previous decisions.

Social proof has been proposed as an explanation for copycat suicide, where suicide rates increase following media publication about suicides. One study using agent-based modeling showed that copycat suicides are more likely when there are similarities between the person involved in the publicized suicide and the potential copycats. In addition, research performed by David Phillips between 1947 and 1968 further supports the existence of copycat suicides.

The adoption and popularity of tulips in the Netherlands and Germany after they had been imported from Constantinople by Conrad Gessner in 1559 has been seen as an example of a fashion spread by social proof.

Theaters sometimes use specially planted audience members who are instructed to give ovations at pre-arranged times. Usually, these people are the ones who clap initially, and the rest of the audience follows. Such ovations may be perceived by non-expert audience members as signals of the performance's quality.
In television shows, television studios have discovered that they can increase the perceived "funniness" of a show by playing a pre-recorded laugh track at key moments. They have found that even though viewers find a laugh track annoying, they perceive shows that use canned laughter to be funnier than those that do not.

In e-commerce, social proof takes the form of positive testimonials from previous customers. Showcasing these testimonials is one of the tactics that is found to be effective in encouraging potential customers to sign up.

Social proof is exploited on social networks such as X (formerly Twitter), Facebook, Instagram and YouTube. The number of followers, fans, views, likes, favorites and even comments that a user has made, positively affects how other users perceive them. A user on X (formerly Twitter) with a million followers is perceived as more trustworthy and reputable than a similar user with a thousand followers, resulting in faster growth of followers and higher engagement and click-through-rates.

Herd behavior is the behavior of individuals in a group acting collectively without centralized direction. Herd behavior occurs in animals in herds, packs, bird flocks, fish schools, and so on, as well as in humans. Voting, demonstrations, riots, general strikes, sporting events, religious gatherings, everyday decision-making, judgement, and opinion-forming, are all forms of human-based herd behavior.
Raafat, Chater and Frith proposed an integrated approach to herding, describing two key issues, the mechanisms of transmission of thoughts or behavior between individuals and the patterns of connections between them.  They suggested that bringing together diverse theoretical approaches of herding behavior illuminates the applicability of the concept to many domains, ranging from cognitive neuroscience to economics.

A group of animals fleeing from a predator shows the nature of herd behavior, for example in 1971, in the oft-cited article "Geometry for the Selfish Herd", evolutionary biologist W. D. Hamilton asserted that each individual group member reduces the danger to itself by moving as close as possible to the center of the fleeing group. Thus the herd appears as a unit in moving together, but its function emerges from the uncoordinated behavior of self-serving individuals.

Asymmetric aggregation of animals under panic conditions has been observed in many species, including humans, mice, and ants. Theoretical models have demonstrated symmetry-breaking similar to observations in empirical studies. For example, when panicked individuals are confined to a room with two equal and equidistant exits, a majority will favor one exit while the minority will favor the other.
Possible mechanisms for this behavior include Hamilton's selfish herd theory, neighbor copying, or the byproduct of communication by social animals or runaway positive feedback.
Characteristics of escape panic include:
Individuals attempt to move faster than normal.
Interactions between individuals become physical.
Exits become arched and clogged.
Escape is slowed by fallen individuals serving as obstacles.
Individuals display a tendency towards mass or copied behavior.
Alternative or less used exits are overlooked.

The philosophers Søren Kierkegaard and Friedrich Nietzsche were among the first to criticize what they referred to as "the crowd" (Kierkegaard) and "herd morality" and the "herd instinct" (Nietzsche) in human society. Modern psychological and economic research has identified herd behavior in humans to explain the phenomenon of large numbers of people acting in the same way at the same time. The British surgeon Wilfred Trotter popularized the "herd behavior" phrase in his book, Instincts of the Herd in Peace and War (1914). In The Theory of the Leisure Class, Thorstein Veblen explained economic behavior in terms of social influences such as "emulation", where some members of a group mimic other members of higher status. In "The Metropolis and Mental Life" (1903), early sociologist George Simmel referred to the "impulse to sociability in man", and sought to describe "the forms of association by which a mere sum of separate individuals are made into a 'society' ". Other social scientists explored behaviors related to herding, such as Sigmund Freud (crowd psychology), Carl Jung (collective unconscious), Everett Dean Martin (Behavior of Crowds) and Gustave Le Bon (the popular mind).
Swarm theory observed in non-human societies is a related concept and is being explored as it occurs in human society. Scottish journalist Charles Mackay identifies multiple facets of herd behaviour in his 1841 work, Extraordinary Popular Delusions and the Madness of Crowds.

"Benign" herding behaviors may occur frequently in everyday decisions based on learning from the information of others, as when a person on the street decides which of two restaurants to dine in. Suppose that both look appealing, but both are empty because it is early evening; so at random, this person chooses restaurant A. Soon a couple walks down the same street in search of a place to eat. They see that restaurant A has customers while B is empty, and choose A on the assumption that having customers makes it the better choice. Because other passersby do the same thing into the evening, restaurant A does more business that night than B. This phenomenon is also referred as an information cascade.

Crowds that gather on behalf of a grievance can involve herding behavior that turns violent, particularly when confronted by an opposing ethnic or racial group. The Los Angeles riots of 1992, New York Draft Riots, and Tulsa race massacre are notorious in U.S. history. The idea of a "group mind" or "mob behavior" was put forward by the French social psychologists Gabriel Tarde and Gustave Le Bon.

Sheeple (; a portmanteau of "sheep" and "people", also spelt sheople) is a derogatory term that highlights the passive herd behavior of people easily controlled by a governing power or market fads by connecting them to sheep, a herd animal that is "easily" led about. The term is used to describe those who voluntarily acquiesce to a suggestion without any significant critical analysis or research, in large part due to the majority of a population having a similar mindset. Word Spy defines it as "people who are meek, easily persuaded, and tend to follow the crowd (sheep + people)". Merriam-Webster defines the term as "people who are docile, compliant, or easily influenced: people likened to sheep". The word is pluralia tantum, which means it does not have a singular form.
While its origins are unclear, the word was used by W. R. Anderson in his column Round About Radio, published in London 1945, where he wrote:
The simple truth is that you can get away with anything, in government. That covers almost all the evils of the time. Once in, nobody, apparently, can turn you out. The People, as ever (I spell it "Sheeple"), will stand anything.
Another early use was from Ernest Rogers, whose 1949 book The Old Hokum Bucket contained a chapter entitled "We the Sheeple". The Wall Street Journal first reported the label in print in 1984; the reporter heard the word used by the proprietor of the American Opinion bookstore. In this usage, taxpayers were derided for their blind conformity as opposed to those who thought independently. The term was first popularized in the late 1980s and early 1990s by conspiracy theorist and broadcaster Bill Cooper on his radio program The Hour of the Time which was broadcast internationally via shortwave radio stations. The program gained a small, yet dedicated following, inspiring many individuals who would later broadcast their own radio programs critical of the United States government. This then led to its regular use on the radio program Coast to Coast AM by Art Bell throughout the 1990s and early 2000s. These combined factors significantly increased the popularity of the word and led to its widespread use.
The term can also be used for those who seem inordinately tolerant, or welcoming, of widespread policies. In a column entitled "A Nation of Sheeple", columnist Walter E. Williams writes, "Americans sheepishly accepted all sorts of Transportation Security Administration nonsense. In the name of security, we've allowed fingernail clippers, eyeglass screwdrivers, and toy soldiers to be taken from us prior to boarding a plane."

Currency crises tend to display herding behavior when foreign and domestic investors convert a government's currency into physical assets (like gold) or foreign currencies when they realize the government is unable to repay its debts. This is called a speculative attack and it will tend to cause moderate inflation in the short term. When consumers realize that the inflation of needed commodities is increasing, they will begin to stockpile and hoard goods, which will accelerate the rate of inflation even faster. This will ultimately crash the currency and likely lead to civil unrest.

Large stock market trends often begin and end with periods of frenzied buying (bubbles) or selling (crashes). Many observers cite these episodes as clear examples of herding behavior that is irrational and driven by emotion—greed in the bubbles, fear in the crashes. Individual investors join the crowd of others in a rush to get in or out of the market.
Some followers of the technical analysis school of investing see the herding behavior of investors as an example of extreme market sentiment. The academic study of behavioral finance has identified herding in the collective irrationality of investors, particularly the work of Nobel laureates Vernon L. Smith, Amos Tversky, Daniel Kahneman, and Robert Shiller.[a] Hey and Morone (2004) analyzed a model of herd behavior in a market context.
Some empirical works on methods for detecting and measuring the extent of herding include Christie and Huang (1995) and Chang, Cheng and Khorana (2000). These results refer to a market with a well-defined fundamental value. A notable incident of possible herding is the 2007 uranium bubble, which started with flooding of the Cigar Lake Mine in Saskatchewan, during the year 2006.

There are two strands of work in economic theory that consider why herding occurs and provide frameworks for examining its causes and consequences.
The first of these strands is that on herd behavior in a non-market context. The seminal references are Banerjee (1992) and Bikhchandani, Hirshleifer and Welch (1992), both of which showed that herd behavior may result from private information not publicly shared. More specifically, both of these papers showed that individuals, acting sequentially on the basis of private information and public knowledge about the behavior of others, may end up choosing the socially undesirable option. A large subsequent literature has examined the causes and consequences of such "herds" and information cascades.
The second strands concerns information aggregation in market contexts. A very early reference is the classic paper by Grossman and Stiglitz (1976) that showed that uninformed traders in a market context can become informed through the price in such a way that private information is aggregated correctly and efficiently. Subsequent work has shown that markets may systematically overweight public information; it has also studied the role of strategic trading as an obstacle to efficient information aggregation.

Herd behavior is often a useful tool in marketing and, if used properly, can lead to increases in sales and changes to the structure of society. Whilst it has been shown that financial incentives cause action in large numbers of people, herd mentality often wins out in a case of "Keeping up with the Joneses".

Social media can also be a powerful tool in perpetuating herd behaviour. Its immeasurable amount of user-generated content serves as a platform for opinion leaders to take the stage and influence purchase decisions, and recommendations from peers and evidence of positive online experience all serve to help consumers make purchasing decisions. Gunawan and Huarng's 2015 study concluded that social influence is essential in framing attitudes towards brands, which in turn leads to purchase intention. Influencers form norms which their peers are found to follow, and targeting extroverted personalities increases chances of purchase even further. This is because the stronger personalities tend to be more engaged on consumer platforms and thus spread word of mouth information more efficiently. Many brands have begun to realise the importance of brand ambassadors and influencers, and it is being shown more clearly that herd behaviour can be used to drive sales and profits exponentially in favour of any brand through examination of these instances.

Marketing can easily transcend beyond commercial roots, in that it can be used to encourage action to do with health, environmentalism and general society. Herd mentality often takes a front seat when it comes to social marketing, paving the way for campaigns such as Earth Day, and the variety of anti-smoking and anti-obesity campaigns seen in every country. Within cultures and communities, marketers must aim to influence opinion leaders who in turn influence each other, as it is the herd mentality of any group of people that ensures a social campaign's success. A campaign run by Som la Pera in Spain to combat teenage obesity found that campaigns run in schools are more effective due to influence of teachers and peers, and students' high visibility, and their interaction with one another. Opinion leaders in schools created the logo and branding for the campaign, built content for social media and led in-school presentations to engage audience interaction. It was thus concluded that the success of the campaign was rooted in the fact that its means of communication was the audience itself, giving the target audience a sense of ownership and empowerment. As mentioned previously, students exert a high level of influence over one another, and by encouraging stronger personalities to lead opinions, the organizers of the campaign were able to secure the attention of other students who identified with the reference group.
Herd behaviour not only applies to students in schools where they are highly visible, but also amongst communities where perceived action plays a strong role. Between 2003 and 2004, California State University carried out a study to measure household conservation of energy, and motivations for doing so. It was found that factors like saving the environment, saving money or social responsibility did not have as great an impact on each household as the perceived behaviour of their neighbours did. Although the financial incentives of saving money, closely followed by moral incentives of protecting the environment, are often thought of as being a community's greatest guiding compass, more households responded to the encouragement to save energy when they were told that 77% of their neighbours were using fans instead of air conditioning, proving that communities are more likely to engage in a behaviour if they think that everyone else is already taking part.
Herd behaviours shown in the two examples exemplify that it can be a powerful tool in social marketing, and if harnessed correctly, has the potential to achieve great change. It is clear that opinion leaders and their influence achieve huge reach among their reference groups and thus can be used as the loudest voices to encourage others in any collective direction.

Egocentric bias is the tendency to rely too heavily on one's own perspective and/or have a higher opinion of oneself than reality. It appears to be the result of the psychological need to satisfy one's ego and to be advantageous for memory consolidation. Research has shown that experiences, ideas, and beliefs are more easily recalled when they match one's own, causing an egocentric outlook. Michael Ross and Fiore Sicoly first identified this cognitive bias in their 1979 paper, "Egocentric Biases in Availability and Attribution". Egocentric bias is referred to by most psychologists as a general umbrella term under which other related phenomena fall.
The effects of egocentric bias can differ based on personal characteristics, such as age and the number of languages one speaks.   Thus far, there have been many studies focusing on specific implications of egocentric bias in different contexts.  Research on collaborative group tasks have emphasized that people view their own contributions differently than they view that of others.  Other areas of research have been aimed at studying how mental health patients display egocentric bias, and at the relationship between egocentric bias and voter distribution.  These types of studies surrounding egocentric bias usually involve written or verbal questionnaires, based on the subject's personal life or their decision in various hypothetical scenarios.

The term "egocentric bias" was first coined in 1980 by Anthony Greenwald, a psychologist at The Ohio State University.  He described it as a phenomenon in which people skew their beliefs so that what they recall from their memory or what they initially understood is different than what actually occurred.  He cites research by Rogers, Kuiper, and Kirker, who explain that the self-reference effect is the ability of people to recall information better if they think about how the information will affect them during the encoding process (recording memories in their brain).  Greenwald argues that the self-reference effect causes people to exaggerate their role in a situation.  Furthermore, information is better encoded, and thus people are more likely to suffer from egocentric bias, if they produce information actively rather than passively, such as by having a direct role in the outcome of a situation.
Egocentric bias occurs when people fail to consider situations from other people's perspectives.  Egocentric bias has influenced ethical judgements to the point where people not only believe that self-interested outcomes are preferential but are also the morally sound way to proceed.   People are more inclined to be aware of their own behaviors since they can use their thoughts and emotions to gain more information about themselves.  These thoughts and emotions can affect how people view themselves in relation to others in specific situations.  A common example arises when people are asked to explain how much credit should be given to each person in a collaborative project.  Daniel Schacter, a psychology professor at Harvard University, considers egocentric bias as one of the "seven sins" of memory and essentially reflects the prominent role played by the self when encoding and retrieving episodic memories. As such, people often feel that their contributions to a collaborative project are greater than those of other members, since people tend to focus more on how much they have done.
In social context, egocentric bias influences people to choose a social circle that is capable of maintaining one's positive traits. Studies show that one's choice of friend or social circle is likely to be dependent on the amount of positive feedback received.

In a 1993 study conducted in Japan, subjects were asked to write down fair or unfair behaviors that they themselves or others did. When writing about fair behavior, they tended to start with the word "I" rather than "others". Likewise, they began unfair behaviors with "others" rather than "I". This demonstrates that people tend to attribute successes and positive behaviors to themselves, while placing the burden of failures and negative behaviors on others. Furthermore, in this study there were gender differences detected; Japanese women, compared to men, remembered the behaviors of others more than their own, and were also more probable to characterize fair or unfair behavior to others compared to themselves.
Another study found that egocentric bias influences perceived fairness. Subjects felt that overpayment to themselves were more fair than overpayment to others; by contrast, they felt the underpayment to themselves were less fair than underpayment to others. Greenberg's studies showed that this egocentrism was eliminated when the subjects were put in a self-aware state, which was applied in his study with a mirror being placed in front of the subjects. When a person is not self-aware, they perceive that something can be fair to them but not necessarily fair to others. Therefore, fairness was something biased and subjective. When a person is self-aware, there is a uniform standard of fairness and there is no bias. When made self-aware, subjects rated overpayment and underpayment to both themselves and to others as equally unfair. It is believed that these results were obtained because self-awareness elevated subjects' concerns about perceived fairness in payment, thereby overriding egocentric tendencies.
The egocentric bias can also be clearly observed in young children, especially those who have not yet developed theory of mind, or the ability to understand concrete situations from the perspective of others. In one study by Wimmer and Perner, a child and a stuffed animal were presented with two differently colored boxes and both are shown that one contains an object of interest. The experimenter then removed the stuffed animal from the room and moved the object into the other box. When asked where the stuffed animal should search for the object, the children overwhelmingly tended to point to the box that they knew the object was in. Rather than thinking about the animal's perspective, the children displayed an egocentric bias in assuming that the animal would share their point of view, even though the animal had no way of knowing the same information as them.

The causes and motivations for egocentric bias were investigated in a 1983 journal entry by Brian Mullen of Murray State University. Inspired by the study by Ross et al. demonstrating the false consensus effect, Mullen's paper focused on the overestimation of consensus. Mullen analyzed the NBC television show "Play the Percentages" to determine whether egocentric bias was rooted in a perceptual  and unintentional distortion of reality versus a conscious, intentional motivation to appear normalized.  Subjects in this analysis were contestants from the show, 20–30 year old middle class married couple with equal gender distribution. At the start of each show, studio audiences were asked several trivia questions, and the percentage of correct answers was recorded for later use in the game. During each round of the game, opposing contestants estimated the percentage of correct answers. The contestant who had a closer estimate wins the percentage of correct answer as a score, and then if they answer said trivia question correctly, wins the remaining percentage for a maximum possible 100 points. The first couple to win 300 points received a cash prize, with the opportunity to win more prizes in bonus rounds. Thus, the show provided incentive for unbiased estimates of consensus. Statistical analysis of the collected data showed that the "egocentric bias of false consensus was observed in spite of the potent incentive for unbiased estimates of consensus." This analysis ultimately supports the hypothesis that egocentric bias is a result of unintentional perceptual distortion of reality rather than a conscious, intentional motivation to appear normalized.
From a psychological standpoint, memories appear to be stored in the brain in an egocentric manner: the role of oneself is magnified in one's experiences to make them more personally relevant and thereby easier to recall. Early childhood memories, therefore, may be more difficult to recall since one's sense of self is less developed, so old memories do not connect as strongly to oneself as newer ones. Moreover, egocentric bias may have evolved from hunter-gatherer times, in which communities were small and interdependent enough that individuals could assume that others around them had very similar outlooks. An egocentric view would have reduced cognitive load and increased communication efficiency.

A 2016 study published by Riva, Triscoli, Lamm, Carnaghi, and Silani found that egocentric bias tends to be experienced in a much greater degree by adolescents and older adults than by young and middle aged adults. They examined the emotional effect of visuo-tactile stimulation on pairs of participants from a population of 114 female of varying ages. The varying degree of egocentric bias with age was attributed to the developmental cycle of the right supramarginal gyrus (rSMG) of the parietal lobe, which finishes developing at the end of adolescence and decays early.

Recent studies of egocentric bias have been done in many different subgroups of people, such as bilingual people.  A study done by Paula Rubio-Fernández and Sam Glucksberg found that bilingual people are less prone to egocentric bias because they have grown to pay more attention to others' thoughts.  Thus, it is less difficult for them to differentiate between their own opinions and those of others.

Considered to be a facet of egocentric bias, the false-consensus effect states that people believe their thoughts, actions, and opinions are much more common than they are in reality. When people are asked to make an estimate of a population's statistic, they often only have data from themselves and tend to assume that others in the population are similar to them due to egocentric bias. In turn, people tend to overestimate the extent to which their opinion is shared by the rest of the population. Moreover, people tend to believe that those who differ in opinion must be part of a minority and that the majority actually agrees with them. Therefore, the false-consensus effect, or the tendency to deduce judgements from one's own opinions, is a direct result of egocentric bias.
A well known example of false-consensus effect is a study published by Ross, Greene and House in 1977. Students are asked to walk around a campus with a sandwich board that bearing the word "repent". People who agreed to do so (50%) estimated that most of their peers would also agree to do so (average estimation 63.5%). Conversely, those who refused to do the experiment reported that most of their peers would refuse as well.
People who exhibit the false consensus effect take egocentric bias a step further: they not only forgo thinking of other perspectives, but they believe that their viewpoints are those accepted by the majority of people.  Nevertheless, some psychologists do not distinguish between egocentric bias and the false consensus effect.  For example, in the paper published by Ross,  Greene, and House, the terms "false consensus" and "egocentric attribution bias" are used interchangeably.  In the second part of their study, they gave out a questionnaire which asked participants which option (out of two choices) they would choose in specified situations, and what percentage of the population would choose which option.  In all four scenarios that were given, subjects rated the option that they chose as the most probable.  Ross, Greene, and House conclude that their results support the false consensus hypothesis, and that "intuitive estimates of deviance and normalcy, and the host of social inferences and interpersonal responses that accompany such estimates, are systematically and egocentrically biased in accord with his own behavioral choices."

A related concept to egocentric bias is self-serving bias, in which one takes undue credit for achievements and blames failures on external forces. However, egocentric bias differs from self-serving bias in that egocentric bias is rooted in an erroneous assumption of other's perception of reality, while self-serving bias is an erroneous perception of one's own reality. For example, consider a student who earns a low grade in a class. Self-serving bias would result in the assumption that the student's low grade is a result of poor teaching, which would direct the fault of one's reality away from one's own actions.
Egocentric bias might also result in an overestimation of the number of students that received low grades in the class for the purpose to normalize these students' performance. However, similar to the false-consensus effect, the self-serving bias and the egocentric bias have also been used as interchangeable terms.
Both concepts may be the product of individualistic cultures that usually stress independence and personal achievement over group-oriented success. Cross-cultural studies have found a strong presence of the egocentric bias in the primarily individualistic American, South African, and Yugoslavian communities, but noted the opposite effect in the collectivistic Japanese, Nepali, and Indian societies. People from these cultures tend to demonstrate a bias toward modesty, in which success is attributed to external or group-related factors and failures are seen as the result of personal shortcomings.

Bayesian reasoning is a form of statistical inference that relies on Bayes' rule to make probability prediction based on given information.  In Bayesian updating, people use prior probabilities to make estimates, and then gradually change these probabilities as they gain more information.  Bayesian inference is often used by psychologists to determine whether subjects who exhibit the false-consensus effect have a rational thought process. To understand Bayes' rule, consider an example from an experiment by Kreuger and Clement: there is an urn with 100 chips, some blue and some red, and then subjects are told that the first chip drawn from the urn is blue.  Subjects are asked to estimate the probability that the urn contains predominantly blue chips. Using Bayes' rule, the probability that a blue chip is drawn given that the urn contains predominantly blue chips is equal to the probability of the urn being predominantly blue multiplied by the probability of the urn being predominantly blue given that a blue chip was drawn, all divided by the probability that the urn is predominantly blue.  Most participants overestimated the requested probability.  Data shows that subjects tend not to pay attention to sample size when making probability predictions.  For example, although it has statistically been proven by the law of large numbers that larger samples have less variability, people tend to claim that large and small samples have the same amount of variability.  Studies like the urn experiment above provide evidence that the false-consensus effect is not entirely rational, and that egocentric viewpoints tend to be predominant.

Egocentric bias can lead to the devaluation of peer contributions and the amplification of one's own work when in a collaborative setting. For example, when group members have been asked to report what percentage of the output they created, the total summed to greater than 100%. Usually, individuals are more easily able to recall their personal contributions and thus believe them to greater or more important. This applies to both positive and negative inputs: in a study of married couples, each spouse rated themselves as more responsible for helpful (cleaning) and detractive activities (causing arguments). Research has shown that feelings of sibling caregivers and their siblings depend on the contact between siblings and their feelings of closeness.  Each of these two groups believed that their siblings contributed less to the needs of their family than themselves, and were more resistant to increasing these types of contributions.  The closer that siblings were to each other, measured through observation and self reports, the smaller the extent of egocentric bias they felt in reporting each sibling's contribution.

An overly exaggerated or extremely low demonstration of egocentric bias could be an indicator of mental illness. Those with anxiety tend to view themselves as the center of all events around them, regardless of their nature or how unrelated they are to oneself.  On the other hand, people suffering from depression may have a lower tendency towards egocentricity, as evidenced by the fact that they tend to more realistically rate their contributions to group work, while non-depressed participants often overreport their additions.

The egocentric bias has also been shown to contribute to a citizen's decision to vote in elections. Firstly, people tend to view their personal choice between voting and abstinence as a reflection of those who support the same candidates and issues. Secondly, although each individual vote has very little power in large-scale elections, those who vote overestimate the significance of their ballot. Moreover, citizens demonstrate egocentric bias, in conjunction with the false-consensus effect, in their predictions of election outcomes. A study examining the 2008 American presidential election found that the more strongly people favor a certain candidate, the higher they estimate that candidate's likelihood of winning the election. For instance, those who strongly preferred Barack Obama predicted that he had a 65% chance of becoming the president, while those who preferred another candidate approximated that he only had a 40% chance of victory.

Pareidolia (; also US: ) is the tendency for perception to impose a meaningful interpretation on a nebulous stimulus, usually visual, so that one detects an object, pattern, or meaning where there is none. Pareidolia is a specific but common type of apophenia (the tendency to perceive meaningful connections between unrelated things or ideas).
Common examples include perceived images of animals, faces, or objects in cloud formations; seeing faces in inanimate objects; or lunar pareidolia like the Man in the Moon or the Moon rabbit. The concept of pareidolia may extend to include hidden messages in recorded music played in reverse or at higher- or lower-than-normal speeds, and hearing voices (mainly indistinct) or music in random noise, such as that produced by air conditioners or by fans. Face pareidolia has also been demonstrated in rhesus macaques.

The word derives from the Greek words pará (παρά, "beside, alongside, instead [of]") and the noun eídōlon (εἴδωλον, "image, form, shape").
Karl Ludwig Kahlbaum introduced the German term Pareidolie in his 1866 paper "Die Sinnesdelierien" ("On Delusion of the Senses"). When Kahlbaum's paper was reviewed the following year (1867) in The Journal of Mental Science, Volume 13, Pareidolie was translated into English as "pareidolia", and noted to be synonymous with the terms "...changing hallucination, partial hallucination, [and] perception of secondary images."

Pareidolia correlates with age and is frequent among patients with Parkinson's disease and dementia with Lewy bodies.

Pareidolia can cause people to interpret random images, or patterns of light and shadow, as faces. A 2009 magnetoencephalography study found that objects perceived as faces evoke an early (165 ms) activation of the fusiform face area at a time and location similar to that evoked by faces, whereas other common objects do not evoke such activation. This activation is similar to a slightly faster time (130 ms) that is seen for images of real faces. The authors suggest that face perception evoked by face-like objects is a relatively early process, and not a late cognitive reinterpretation phenomenon.
A functional magnetic resonance imaging (fMRI) study in 2011 similarly showed that repeated presentation of novel visual shapes that were interpreted as meaningful led to decreased fMRI responses for real objects. These results indicate that the interpretation of ambiguous stimuli depends upon processes similar to those elicited by known objects.
Pareidolia was found to affect brain function and brain waves. In a 2022 study, EEG records show that responses in the frontal and occipitotemporal cortexes begin prior to when one recognizes faces and later, when they are not recognized. By displaying these proactive brain waves, scientists can then have a basis for data rather than relying on self-reported sightings. 
These studies help to explain why people generally identify a few lines and a circle as a "face" so quickly and without hesitation. Cognitive processes are activated by the "face-like" object which alerts the observer to both the emotional state and identity of the subject, even before the conscious mind begins to process or even receive the information. A "stick figure face", despite its simplicity, can convey mood information, and be drawn to indicate emotions such as happiness or anger. This robust and subtle capability is hypothesized to be the result of natural selection favoring people most able to quickly identify the mental state, for example, of threatening people, thus providing the individual an opportunity to flee or attack preemptively. This ability, though highly specialized for the processing and recognition of human emotions, also functions to determine the demeanor of wildlife.

Pareidolia plays a significant role in creative cognition, enabling artists and viewers to perceive novel forms and meanings in ambiguous stimuli. Joanne Lee highlights that this phenomenon has been harnessed in artistic practices for centuries (Da Vinci for example). The phenomenon was  particularly important to surrealism, where artists like Salvador Dali, influenced by André Breton, embraced pareidolic ambiguity to challenge rationalist perceptions and provoke new ways of seeing.

A mimetolithic pattern is a pattern created on rocks that may come to mimic recognizable forms through the random processes of formation, weathering and erosion. Many examples exist, from the  Old Man of the Mountain (a face-like profile in a cliff) to Iztaccíhuatl, a range in Mexico whose name is Nahuatl for "White (like salt) woman", reflecting the four individual snow-capped peaks which depict the head, chest, knees and feet of a sleeping female when seen from east or west.
A well-known example is the Face on Mars, a rock formation on Mars that resembled a human face in certain satellite photos. Most mimetoliths are much larger than the subjects they resemble, such as a cliff profile that looks like a human face.
Picture jaspers exhibit combinations of patterns, such as banding from flow or depositional patterns (from water or wind), or dendritic or color variations, resulting in what appear to be miniature scenes on a cut section, which is then used for jewelry.
Chert nodules, concretions, or pebbles may in certain cases be mistakenly identified as skeletal remains, egg fossils, or other antiquities of organic origin by amateur enthusiasts.
In the late 1970s and early 1980s, Japanese researcher Chonosuke Okamura self-published a series of reports titled Original Report of the Okamura Fossil Laboratory, in which he described tiny inclusions in polished limestone from the Silurian period (425 mya) as being preserved fossil remains of tiny humans, gorillas, dogs, dragons, dinosaurs and other organisms, all of them only millimeters long, leading him to claim, "There have been no changes in the bodies of mankind since the Silurian period... except for a growth in stature from 3.5 mm to 1,700 mm." Okamura's research earned him an Ig Nobel Prize (a parody of the Nobel Prize) in biodiversity in 1996.
Some sources describe various mimetolithic features on Pluto, including a heart-shaped region.

Seeing shapes in cloud patterns is another example of this phenomenon.  Rogowitz and Voss (1990) showed a relationship between seeing shapes in cloud patterns and fractal dimension. They varied the fractal dimension of the boundary contour from 1.2 to 1.8, and  found that the lower the fractal dimension, the more likely people were to report seeing nameable shapes of animals, faces, and fantasy creatures. From above, pareidolia may be perceived in satellite imagery of tropical cyclones. Notably hurricanes Matthew and Milton gained much attention for resembling a human face or skull when viewed from the side.

A notable example of pareidolia occurred in 1877, when observers using telescopes to view the surface of Mars thought that they saw faint straight lines, which were then interpreted by some as canals. It was theorized that the canals were possibly created by sentient beings. This created a sensation. In the next few years better photographic techniques and stronger telescopes were developed and applied, which resulted in new images in which the faint lines disappeared, and the canal theory was debunked as an example of pareidolia.

Many cultures recognize pareidolic images in the disc of the full moon, including the human face known as the Man in the Moon in many Northern Hemisphere cultures and the Moon rabbit in East Asian and indigenous American cultures. Other cultures see a walking figure carrying a wide burden on their back, including in Germanic tradition, Haida mythology, and Latvian mythology.

The Rorschach inkblot test uses pareidolia in an attempt to gain insight into a person's mental state. The Rorschach is a projective test that elicits thoughts or feelings of respondents that are "projected" onto the ambiguous inkblot images.    Rorschach inkblots have low-fractal-dimension boundary contours, which may elicit general shape-naming behaviors, serving as vehicles for projected meanings.

Owing to the way designs are engraved and printed, occurrences of pareidolia have occasionally been reported in banknotes.
One example is the 1954 Canadian Landscape Canadian dollar banknote series, known among collectors as the "Devil's Head" variety of the initial print runs. The obverse of the notes features what appears to be an exaggerated grinning face, formed from patterns in the hair of Queen Elizabeth II. The phenomenon generated enough attention for revised designs to be issued in 1956, which removed the effect.

Renaissance authors have shown a particular interest in pareidolia. In William Shakespeare's play Hamlet, for example, Prince Hamlet points at the sky and "demonstrates" his supposed madness in this exchange with Polonius:
Nathaniel Hawthorne wrote a short story called "The Great Stone Face" in which a face seen in the side of a mountain (based on the real-life The Old Man of the Mountain) is revered by a village.

Renaissance artists often used pareidolia in paintings and drawings: Andrea Mantegna, Leonardo da Vinci, Giotto, Hans Holbein, Giuseppe Arcimboldo, and many more have shown images—often human faces—that due to pareidolia appear in objects or clouds.
In his notebooks, Leonardo da Vinci wrote of pareidolia as a device for painters, writing:
If you look at any walls spotted with various stains or with a mixture of different kinds of stones, if you are about to invent some scene you will be able to see in it a resemblance to various different landscapes adorned with mountains, rivers, rocks, trees, plains, wide valleys, and various groups of hills. You will also be able to see divers combats and figures in quick movement, and strange expressions of faces, and outlandish costumes, and an infinite number of things which you can then reduce into separate and well conceived forms.
Salem, a 1908 painting by Sydney Curnow Vosper, gained notoriety due to a rumour that it contained a hidden face, that of the devil. This led many commentators to visualize a demonic face depicted in the shawl of the main figure, despite the artist's denial that any faces had deliberately been painted into the shawl.
Surrealist artists such as Salvador Dalí would intentionally use pareidolia in their works, often in the form of a hidden face.

Two 13th-century edifices in Turkey display architectural use of shadows of stone carvings at the entrance. Outright pictures are avoided in Islam but tessellations and calligraphic pictures were allowed, so designed "accidental" silhouettes of carved stone tessellations became a creative escape. 
Niğde Alaaddin Mosque in Niğde, Turkey (1223), with its "mukarnas" art where the shadows of three-dimensional ornamentation with stone masonry around the entrance form a chiaroscuro drawing of a woman's face with a crown and long hair appearing at a specific time, at some specific days of the year.
Divriği Great Mosque and Hospital in Sivas, Turkey (1229), shows shadows of the three-dimensional ornaments of both entrances of the mosque part, to cast a giant shadow of a praying man that changes pose as the sun moves, as if to illustrate what the purpose of the building is. Another detail is the difference in the impressions of the clothing of the two shadow-men indicating two different styles, possibly to tell who is to enter through which door.

There have been many instances of perceptions of religious imagery and themes, especially the faces of religious figures, in ordinary phenomena. Many involve images of Jesus, the Virgin Mary, the word Allah, or other religious phenomena: in September 2007 in Singapore, for example, a callus on a tree resembled a monkey, leading believers to pay homage to the "Monkey god" (either Sun Wukong or Hanuman) in the monkey tree phenomenon.
Publicity surrounding sightings of religious figures and other surprising images in ordinary objects has spawned a market for such items on online auctions like eBay. One famous instance was a grilled cheese sandwich with the face of the Virgin Mary. This was parodied in the Glee episode "Grilled Cheesus".
During the September 11 attacks, television viewers supposedly saw the face of Satan in clouds of smoke billowing out of the World Trade Center after it was struck by the airplane. Another example of face recognition pareidolia originated in the fire at Notre Dame Cathedral, when a few observers claimed to see Jesus in the flames.
While attempting to validate the imprint of a crucified man on the Shroud of Turin as Jesus, a variety of objects have been described as being visible on the linen. These objects include a number of plant species, a coin with Roman numerals, and multiple insect species. In an experimental setting using a picture of plain linen cloth, participants who had been told that there could possibly be visible words in the cloth, collectively saw 2 religious words. Those told that the cloth was of some religious importance saw 12 religious words, and those who were also told that it was of religious importance, but also given suggestions of possible religious words, saw 37 religious words. The researchers posit that the reason the Shroud has been said to have so many different symbols and objects is because it was already deemed to have the imprint of Jesus prior to the search for symbols and other imprints in the cloth, and therefore it was simply pareidolia at work.

Pareidolia can occur in computer vision, specifically in image recognition programs, in which vague clues can spuriously detect images or features. In the case of an artificial neural network, higher-level features correspond to more recognizable features, and enhancing these features brings out what the computer sees. These examples of pareidolia reflect the training set of images that the network has "seen" previously.
Striking visuals can be produced in this way, notably in the DeepDream software, which falsely detects and then exaggerates features such as eyes and faces in any image. The features can be further exaggerated by creating a feedback loop where the output is used as the input for the network. (The adjacent image was created by iterating the loop 50 times.) Additionally the output can be modified such as slightly zooming in to create an animation of the images perspective flying through the surrealistic imagery.

In 1971 Konstantīns Raudive wrote Breakthrough, detailing what he believed was the discovery of electronic voice phenomena (EVP). EVP has been described as auditory pareidolia. Allegations of backmasking in popular music, in which a listener claims a message has been recorded backward onto a track meant to be played forward, have also been described as auditory pareidolia. In 1995, the psychologist Diana Deutsch invented an algorithm for producing phantom words and phrases with the sounds coming from two stereo loudspeakers, one to the listener's left and the other to his right, producing a phase offset in time between the speakers. After listening for a while, phantom words and phrases suddenly emerge, and these often appear to reflect what is on the listener's mind.

Medical educators sometimes teach medical students and resident physicians (doctors in training) to use pareidolia and patternicity to learn to recognize human anatomy on radiology imaging studies.
Examples include assessing radiographs (X-ray images) of the human vertebral spine. Patrick Foye, M.D., professor of physical medicine and rehabilitation at Rutgers University, New Jersey Medical School, has written that pareidolia is used to teach medical trainees to assess for spinal fractures and spinal malignancies (cancers). When viewing spinal radiographs, normal bony anatomic structures resemble the face of an owl. (The spinal pedicles resemble an owl's eyes and the spinous process resembles an owl's beak.) But when cancer erodes the bony spinal pedicle, the radiographic appearance changes such that now that eye of the owl seems missing or closed, which is called the "winking owl sign". Another common pattern is a "Scottie dog sign" on a spinal X-ray.
In 2021, Foye again published in the medical literature on this topic, in a medical journal article called "Baby Yoda: Pareidolia and Patternicity in Sacral MRI and CT Scans". Here, he introduced a novel way of visualizing the sacrum when viewing MRI magnetic resonance imaging and CT scans (computed tomography scans). He noted that in certain image slices the human sacral anatomy resembles the face of "Baby Yoda" (also called Grogu), a fictional character from the television show The Mandalorian. Sacral openings for exiting nerves (sacral foramina) resemble Baby Yoda's eyes, while the sacral canal resembles Baby Yoda's mouth.

In January 2017, an anonymous user placed an eBay auction of a Cheeto that looked like the gorilla Harambe. Bidding began at US$11.99, but the Cheeto was eventually sold for US$99,000.
Starting from 2021, an Internet meme emerged around the online game Among Us, where users presented everyday items such as dogs, statues, garbage cans, big toes, and pictures of the Boomerang Nebula that looked like the game's "crewmate" protagonists. In May 2021, an eBay user named Tav listed a Chicken McNugget shaped like a crewmate from Among Us for online auction. The Chicken McNugget was sold for US$99,997 to an anonymous buyer.

A shadow person (also known as a shadow figure, shadow being or black mass) is often attributed to pareidolia. It is the perception of a patch of shadow as a living, humanoid figure, particularly as interpreted by believers in the paranormal or supernatural as the presence of a spirit or other entity.
Pareidolia is also what some skeptics believe causes people to believe that they have seen ghosts.

